{
  "hash": "fbe5c7bc86a0f059e55efbdc1c4cff84",
  "result": {
    "markdown": "---\ntitle: \"Bigger Concepts\"\ndescription: |\n  Bigger concepts for MS797 coursework\ncategories: [machine learning]\n---\n\n\n\n\nThis page is for concepts that don't fit into the [glossary](../glossary/index.qmd).\n\n**This document is in process.**\n\n# Classification\n\nThe general umbrella categories for *classification*---where the outcome variable to be predicted is categorical, rather than continuous, like eye color rather than weight---are \"discriminative\" and \"generative\". Discriminative models take sets of data and separate them into classes, like finding the decision boundary between two or more classes, but can't predict the values of new data. Generative models (used in unsupervised learning) can generate new data, but are more susceptible to outliers (misclassified observations) than discriminative models, require more data to create, and are more computationally expensive.\n\nA generative model---somehow---tries to guess how the data was generated, so it can take a new bit of unseen data, assess the probability that each class would have produced it, and pick the class with the highest probability.\n\nA discriminative model can only look at existing data and create a decision boundary separating the existing data into classes.\n\n![Figure from https://stanford.edu/%7Eshervine/teaching/cs-229/cheatsheet-supervised-learning](disc-gen.png)\n\nNote that there is no decision boundary in the generative model, just levels of probability into which new data could be classified with varying levels of confidence.\n\nLogistic regression is discriminative. Linear discriminant analysis, quadratic discriminant analysis, and naive Bayes are generative, K-nearest neighbors appears to be neither.\n\n## Logistic Regression\n\nLogistic regression's output is a *probability* that a response variable belongs to a particular category.\n\nThe general form of a logistic model is:\n\n$$p(X) = \\text{PR}(Y = 1|X)$$ The right side of the equation reads \"The probability that Y equals 1, given X\" - or the chance that Y is a particular category for a specific value of X.\n\nA linear regression model would use this form:\n\n$$p(X) = \\beta_0 + \\beta_1X$$\n\nAnd the magic formula for this is the *logistic function*, in which we raise $e$ to this power.\n\n$$p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}$$\n\n(Why $e^{\\beta_0 + \\beta_1X}$ instead of, say, $2^{\\beta_0 + \\beta_1X}$ or $\\pi^{\\beta_0 + \\beta_1X}$ or $123456^{\\beta_0 + \\beta_1X}$?)\n\nDividing both sides by $1 - p(X)$ (somehow) yields:\n\n$$\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1X}$$\n\nwhich is also the *odds* that Y is 1 given X. If the probability of something is 20%, or 1/5, then the odds are 1/4:\n\n$$\\frac{1}{5} \\div \\left[ 1 - \\frac{1}{5} \\right] = \\frac{1}{5} \\div \\frac{4}{5} = \\frac{1}{5} \\times \\frac{5}{4} = 1/4 = 0.25$$\n\n### When to use logistic regression\n\nWhen the response variable is categorical, and has two possible classes, like \"Yes\" or \"No\", \"Up\" or \"Down\", etc. However, see Linear Discriminant Analysis below for examples of when *not* to use it.\n\nIn some cases, a categorical response variable could be analyzed with a linear regression by treating the categorical variable as numeric, like treating \"not infected\" as 0 and \"infected\" as 1, and using the rounded value as the predictor; however, beyond 2 possible responses it's unlikely that a continuous scale could be established between the possible values, and it gets messy.\n\n### How to do logistic regression\n\nUse `glm()` with the argument `family = binomial` to indicate logistic regression.\n\nUsing the `Boston` data set from the `ISLR2` package, the following will model the probability of a tract having a `crim` rate above the median, based on `zn`, using logistic regression:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = crim_above_med ~ zn, family = binomial, data = boston)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4319  -1.4319   0.4634   0.9427   1.8532  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.58076    0.10727   5.414 6.17e-08 ***\nzn          -0.09545    0.01349  -7.075 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 701.46  on 505  degrees of freedom\nResidual deviance: 559.53  on 504  degrees of freedom\nAIC: 563.53\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n:::\n\n\nThe `zn.fit` summary above will show the coefficients and p-values; the `$coef` aspect of the summary will show more detail.\n\n`predict()` can then be fed the fit to make predictions. The argument `type = \"response\"` will tell it to output probabilities \"of the form $P(Y = 1|X)$\" from the data that was used in the fit, also described as the training data. It returns a vector of probabilities for every data element.\n\n\n::: {.cell}\n::: {.cell-output-display}\n-----------------------------------------------------\n   1        2        3        4        5        6    \n-------- -------- -------- -------- -------- --------\n 0.2428   0.6412   0.6412   0.6412   0.6412   0.6412 \n-----------------------------------------------------\n:::\n:::\n\n\nThe responses can be converted into categories and turned into a confusion matrix to show how the regression did with the training data.\n\n\n::: {.cell}\n::: {.cell-output-display}\n---------------------\n &nbsp;    No    Yes \n--------- ----- -----\n **No**    119   15  \n\n **Yes**   134   238 \n---------------------\n:::\n:::\n\n\nThe overall success rate can be calculated with `mean()`, comparing each predicted value to the actual value:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7055336\n```\n:::\n:::\n\n\nThe model can then be run on test data that is held out from the main set, with the expectation that it won't do quite as well as the training data. The efficacy can then be assessed in the same way, and if it's not high, we can return to the model and try to find other forms that make better predictions - or perhaps conclude that we can't make a good prediction at all.\n\n## Linear Discriminant Analysis (LDA)\n\nThe simplest use case for LDA is when there are more than two possible response classes; logistic regression gives the probability that an observation is in a class A or B, but it can only handle two classes.\n\nLDA also gives better results when \"there is substantial separation between the two classes\", or where the sample size is small and the distribution of the predictors is approximately normal in each of the classes.\n\n<aside>How do we determine this?</aside>\n\n## Resampling methods\n\nResampling methods are used to refine and test models against training data by repeatedly testing against different subsets within the data.\n\n*Cross-validation* can be used to estimate the test error rate of a method (*model assessment*), and *bootstrap* is mostly to measure the accuracy of a parameter or of a method.\n\n### Cross-validation: validation set approach\n\nThis approach randomly splits the data into two sets: a *training* set and a *validation* or *test* set. Size ratios seem to be on the order of between 50-50 to 3:1 or 4:1 training-to-test data. The model is fit against the training data, and then `predict()` is used to predict the results for the validation data; the results of `predict` can then be compared against the actual classifications in the validation data to compute the error rate.\n\nThis process can be run repeatedly with different splits.\n\nThe general process is:\n\n1.  Randomly split the dataset into training and validation sets\n2.  Fit the model to the training data\n3.  Predict the outcomes of the model on the validation data\n4.  Convert the predictions into categorical variables that match the field being predicted (like Yes/No, etc)\n5.  The error rate is the proportion of correct predictions in the validation data.\n\nThe code below runs 10 iterations and accumulates the results into a results data frame.\n\n\n::: {.cell}\n\n:::\n\n\n### Cross-validation: leave-one-out (LOOCV)\n\nThis method is similar to the validation set approach, except that the validation set is only one element, and the training set is the remaining $n-1$ observations. It is run automatically $n$ times and returns an average of the test errors.\n\nThe process is simpler than the validation set approach:\n\n1.  Fit the model to the entire data set with `glm()` (omitting the `subset` argument)\n2.  Run `cv.glm` on the fit\n3.  The `delta` factor of the return data is the cross-validation error rate for the test data.\n\nCode from the book:\n\n\n::: {.cell}\n\n:::\n\n\n### Cross-validation: k-fold\n\nK-fold cross-validation is like leave-one-out cross-validation but with a larger test set; it splits the data up into $k$ segments and runs the cross-validation on each. It may give better results if run repeatedly and averaged, as with the validation set approach. It is faster than LOOCV, obviously, because it uses less data.\n\nThe process is the same as LOOCV, except a parameter such as `K = 10` is added to the `cv.glm()` call. 5 to 10 is a common range for values of $k$.\n\n\n```{=html}\n<!--\n### When to use it\n\n### How to do it\n\n### How to interpret it\n-->\n```\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}