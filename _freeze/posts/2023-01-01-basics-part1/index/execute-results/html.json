{
  "hash": "80e63616fabf632c326a98c7d0fb3b6e",
  "result": {
    "markdown": "---\ntitle: \"Verifying Basic Stats, Part 1\"\ndescription: |\n  You can never be too solid in your basic stats.\ndate: \"2023-01-01\"\ncategories: [statistics, machine learning, tufte]\nimage: tufte-anscombe-plots.png\ndraft: true\n---\n\n\n\n\n\n\n## On practice and data science\n\nA favorite quote on the subject of data science is:\n\n::: blockquote\nData is the sword of the 21st century, those who wield it well, the Samurai\\\n-- <cite>Jonathan Rosenberg</cite>\n:::\n\nAny samurai worth their salt will tell you that practice is the root of skill, and basics are the root of practice. You never stop practicing your basics; you cannot be too well-grounded in them.\n\nI find it easy to be distracted by the siren call of machine learning and its exciting methods for discovering structure in noise and predicting outcomes... the allure of advanced techniques for extracting meaning from mountains of data is undeniable, and the bleeding edge of knowledge is always an exciting place to be.\n\nI am sometimes prone to skipping ahead too fast, though. Blasting through basic stats in a semester or two of study is fine, and we all want to get on to the cool stuff, but I'm not always convinced that I, at least, have spent enough time in the trenches to have really solid intuition about my basics.\n\nIt says here that fundamental data errors, misinformation, and outright disinformation are going to become much bigger problems in the years ahead, as we cede more and more of our trust to social media (and unaccountable algorithm-driven policies), AI systems like [ChatGPT](https://openai.com/blog/chatgpt/) (which I remain a huge fan of, and also hugely terrified by), and generally authoratitive-presenting sources of any kind, whether public or private. It's bad enough that lots of people out there are careless with data and publish or share dumb mistakes and erroneous interpretations and conclusions; add to that the deliberate misinformation of [social media troll farms](https://www.theguardian.com/world/2022/may/01/troll-factory-spreading-russian-pro-war-lies-online-says-uk) skillfully and quickly polluting online information with outright lies and propaganda that spawn faster than fact-checkers can knock them down, and AI systems that [confidently deliver incorrect information](/posts/2022-12-02-sonnets/) in manners that can be utterly convincing, and it's clear that we're going to need to keep our data-samurai basics fresh and regularly practiced.\n\nIf we don't know how to quickly check basic statistical claims, we're going to be in trouble. I know we all know how to google this stuff and figure it out when we need to, but that's not good enough. And no, asking an AI system to do it for us isn't going to be the answer. Our data-fu must be sharp.\n\nSo let's practice.\n\n## Edward Tufte's outrageous claim\n\nRight on page 1 of \"[The Visual Display of Quantitative Information](https://www.edwardtufte.com/tufte/books_vdqi)\", the bible of data visualization, [Edward Tufte](https://www.edwardtufte.com) shows [Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet), a set of four small sets of data that (supposedly!) share nearly identical summary statistics:\n\n![[The Visual Display of Quantitative Information](https://www.edwardtufte.com/tufte/books_vdqi), p1](tufte-anscombe-data.png)\n\n<aside>(Careful observers will note that the first three `x` columns are identical, though the `y` columns vary, and that the fourth `x` column contains the same value for all but the 8th row.)</aside>\n\n...but that look quite different when plotted, to illustrate the point that visualizing data is a critical aspect of analysis:\n\n![[The Visual Display of Quantitative Information](https://www.edwardtufte.com/tufte/books_vdqi), p2](tufte-anscombe-plots.png)\n\nJust outrageous. Does he think we're chumps? We're supposed to just buy his claim that this is true? That all of those summary stats above are accurate, and that when plotted they look like this? Not today. We will investigate for ourselves.\n\n(Note: of course we know it's all true and that he's correct; this is basic stats, and he's on our side. But we're going to pretend we don't trust him, and roll up our sleeves and check anyway, using R and Python.)\n\n## The data\n\nJust to really flex on this, we're not going to use the built-in copies of the anscombe data set available in R's `datasets` library, or in Python's `Seaborn`. We'll build the dataframes ourselves in both languages.\n\n::: panel-tabset\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanscombe <- data.frame(\n  x1 = c(10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0),\n  y1 = c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68),\n  x2 = c(10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0),\n  y2 = c(9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74),\n  x3 = c(10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0),\n  y3 = c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73),\n  x4 = c(8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 19.0, 8.0, 8.0, 8.0),\n  y4 = c(6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89)\n)\n\n# view the data frame\nprint(anscombe)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   x1    y1 x2   y2 x3    y3 x4    y4\n1  10  8.04 10 9.14 10  7.46  8  6.58\n2   8  6.95  8 8.14  8  6.77  8  5.76\n3  13  7.58 13 8.74 13 12.74  8  7.71\n4   9  8.81  9 8.77  9  7.11  8  8.84\n5  11  8.33 11 9.26 11  7.81  8  8.47\n6  14  9.96 14 8.10 14  8.84  8  7.04\n7   6  7.24  6 6.13  6  6.08  8  5.25\n8   4  4.26  4 3.10  4  5.39 19 12.50\n9  12 10.84 12 9.13 12  8.15  8  5.56\n10  7  4.82  7 7.26  7  6.42  8  7.91\n11  5  5.68  5 4.74  5  5.73  8  6.89\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nanscombe = pd.DataFrame({\n    'x1': [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],\n    'y1': [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68],\n    'x2': [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],\n    'y2': [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74],\n    'x3': [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],\n    'y3': [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73],\n    'x4': [8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 19.0, 8.0, 8.0, 8.0],\n    'y4': [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n})\n\nprint(anscombe)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      x1     y1    x2    y2    x3     y3    x4     y4\n0   10.0   8.04  10.0  9.14  10.0   7.46   8.0   6.58\n1    8.0   6.95   8.0  8.14   8.0   6.77   8.0   5.76\n2   13.0   7.58  13.0  8.74  13.0  12.74   8.0   7.71\n3    9.0   8.81   9.0  8.77   9.0   7.11   8.0   8.84\n4   11.0   8.33  11.0  9.26  11.0   7.81   8.0   8.47\n5   14.0   9.96  14.0  8.10  14.0   8.84   8.0   7.04\n6    6.0   7.24   6.0  6.13   6.0   6.08   8.0   5.25\n7    4.0   4.26   4.0  3.10   4.0   5.39  19.0  12.50\n8   12.0  10.84  12.0  9.13  12.0   8.15   8.0   5.56\n9    7.0   4.82   7.0  7.26   7.0   6.42   8.0   7.91\n10   5.0   5.68   5.0  4.74   5.0   5.73   8.0   6.89\n```\n:::\n:::\n\n:::\n\nWe'll zoom in on the first of the four sets here, and extract the columns as just `x` and `y` into a smaller dataframe. We'll return to the full set later, but we only need one set to test the summary statistics above.\n\n::: panel-tabset\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset_1 <- anscombe |>\n  select(x = x1, y = y1)\nprint(set_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    x     y\n1  10  8.04\n2   8  6.95\n3  13  7.58\n4   9  8.81\n5  11  8.33\n6  14  9.96\n7   6  7.24\n8   4  4.26\n9  12 10.84\n10  7  4.82\n11  5  5.68\n```\n:::\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nset_1 = anscombe[['x1', 'y1']].rename(columns={'x1': 'x', 'y1': 'y'})\nprint(set_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       x      y\n0   10.0   8.04\n1    8.0   6.95\n2   13.0   7.58\n3    9.0   8.81\n4   11.0   8.33\n5   14.0   9.96\n6    6.0   7.24\n7    4.0   4.26\n8   12.0  10.84\n9    7.0   4.82\n10   5.0   5.68\n```\n:::\n:::\n\n:::\n\n`set_1` is now ready for basic analysis.\n\n## The claims\n\nThe claim is that all four of the data sets share the following summary statistics:\n\n| Statistic                           | Value          |\n|-------------------------------------|----------------|\n| $N$                                 | 11             |\n| mean of $X$'s                       | 9.0            |\n| mean of $Y$'s                       | 7.5            |\n| equation of regression line         | $Y = 3 + 0.5X$ |\n| standard error of estimate of slope | 0.118          |\n| $t$                                 | 4.24           |\n| sum of squares $X - \\bar{X}$        | 110.0          |\n| regression sum of squares           | 27.50          |\n| residual sum of squares of Y        | 13.75          |\n| correlation coefficient             | .82            |\n| $r^2$                               | .67            |\n\nLet's examine each of the claims in turn.\n\n### $N$ = 11, mean of $X$'s = 9, mean of $Y$'s = 7.5\n\nThe most basic of all summary statistics: counts and means. Yes, we're even going to check these.\n\n::: panel-tabset\n### R\n\nSimple enough: `nrow()` counts rows, and `mean()` calculates means.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset_1_n <- nrow(set_1)\nset_1_xbar <- mean(set_1[['x']])\nset_1_ybar <- mean(set_1[['y']])\n```\n:::\n\n\n<aside>Note: although `set_1$x` is a little easier to read, I prefer the double-bracket subsetting format `set_1[['x']]` in R, because it allows interpolation</aside>\n\n### Python\n\nIn Python, `len` counts rows, and the `mean()` method calculates means.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nset_1_n = len(set_1)\nset_1_xbar = set_1['x'].mean()\nset_1_ybar = set_1['y'].mean()\n```\n:::\n\n:::\n\n| Statistic             | \\(R\\) Value    | (Python) Value    |\n|-----------------------|----------------|-------------------|\n| $N$ (row count)       | 11    | 11    |\n| mean of x ($\\bar{x}$) | 9 | 9 |\n| mean of y ($\\bar{y}$) | 7.5009091 | 7.5009091 |\n\nThe mean of `y` is slightly higher than `7.5`, but that's a nit we can disregard; in nearly every real-world situation, there is no practical difference between `7.5` and `7.5009091`, and the purpose of the Anscombe data is to show the importance of visualization, not to achieve perfect mathematical precision.\n\n### equation of regression line: $Y = 3 + 0.5X$\n\nOK, so what is a regression line? In general terms, it's a line of best fit, a straight line that get as close as possible to all of the points in the data set. Looking at the scatterplot of the first set's points:\n\n::: panel-tabset\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset_1_baseplot <- ggplot(set_1, mapping = aes(x = x, y = y)) +\n  geom_point(color = \"darkgreen\", size = 3) +\n  scale_x_continuous(limits = c(0, 20)) +\n  scale_y_continuous(limits = c(0, 11)) +\n  theme_minimal() +\n  theme(axis.line = element_line(colour = \"darkblue\",\n                    linewidth = 1, linetype = \"solid\"))\n\nset_1_baseplot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n### Python\n\n(Not going too bananas to make the plot exactly match R; `seaborn` and `ggplot` are different animals)\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwith sns.axes_style(\"whitegrid\", {'axes.edgecolor': 'darkblue'}):\n    set_1_baseplot = sns.scatterplot(data=set_1, x='x', y='y', color='darkgreen', s=50)\n\n# set the axis limits; XXX capture output to \"foo\" to prevent weird echoing\nfoo = set_1_baseplot.set(xlim=(0, 20), ylim=(0.11))\n\nset_1_baseplot.spines['top'].set_linewidth(0)\nset_1_baseplot.spines['right'].set_linewidth(0)\nset_1_baseplot.spines['left'].set_linewidth(2)\nset_1_baseplot.spines['bottom'].set_linewidth(2)\n\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n:::\n\nA regression line is a straight line through the graph that best represents, to the extent that a straight line can, the overall shape of the data. It consists of an *intercept* (the value of X where the line crosses the X axis, where Y is 0), and a *slope* (the \"rise over the run\", or how much each unit of increase in X, as we move left to right, increases or decreases the value / height of Y). It is expressed with the equation\n\n$$y = \\beta_0 + \\beta_1 \\times x$$\n\nwhere $\\beta_0$ (\"beta zero\") is the intercept, and $\\beta_1$ (\"beta one\") is the slope, or the change in $y$ for every change of one unit in $x$. The regression line is the straight line through the points that best fits the overall shape of the data.\n\nSounds easy enough, but if we did it by eye, how would we do it? We could take some guesses:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the R code\"}\nset_1_baseplot +\n    geom_abline(slope = 0.5, intercept = 3, color = \"red\") +\n    geom_abline(slope = 0.58, intercept = 1.9, color = \"darkgreen\") +\n    geom_abline(slope = 0.66, intercept = 2.1, color = \"purple\") +\n    annotate(\"text\", x = 4, y = 9.75, label= \"Something like one of these,\\nmaybe?\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-3.png){width=672}\n:::\n:::\n\n\nFortunately, of course, we don't need to guess. Fitting a linear model to the data will give us the slope and the intercept using the [ordinary least squares (OLS)](https://en.wikipedia.org/wiki/Ordinary_least_squares) method, a fine default method for most cases.\n\n::: panel-tabset\n### R\n\nThe `lm` method in R is a simple way to get the fit for a linear model - we just have to remember that the intercept is the first coefficient, and the slope is the second.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(set_1$y ~ set_1$x)\nbeta_0 <- coef(fit)[1]\nbeta_1 <- coef(fit)[2]\n```\n:::\n\n\n### Python\n\nIn Python, the `polyfit` method of `numpy` will do what we need - we just have to remember that the slope is the first coefficient, and the intercept is the second.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfit = np.polyfit(set_1['x'], set_1['y'], 1)\nbeta_0 = fit[1]\nbeta_1 = fit[0]\n```\n:::\n\n:::\n\n| Statistic             | \\(R\\) Value | (Python) Value |\n|-----------------------|-------------|----------------|\n| intercept ($\\beta_0$) | 3.0000909  | 3.0000909  |\n| slope ($\\beta_1$)     | 0.5000909  | 0.5000909  |\n\nSo, once again, there's an inconsequential decimal offset, but it is small enough to disregard, and we can confirm that the equation of the regression line is indeed (basically) $Y = 3 + 0.5X$, and here's our plot:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the R code\"}\nset_1_baseplot +\n    geom_abline(slope = 0.5, intercept = 3, color = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nSo far so good; on we go.\n\n### standard error of estimate of slope = 0.118\n\n### $t$ = 4.24\n\n### sum of squares $X - \\bar{X}$ = 110.0\n\n### regression sum of squares = 27.50\n\n### residual sum of squares of Y = 13.75\n\n### correlation coefficient = .82\n\n### $r^2$ = .67\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}