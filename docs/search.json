[
  {
    "objectID": "academics/695M/bigger-concepts/index.html",
    "href": "academics/695M/bigger-concepts/index.html",
    "title": "Bigger Concepts",
    "section": "",
    "text": "This page is for concepts that don’t fit into the glossary.\nThis document is in process."
  },
  {
    "objectID": "academics/695M/bigger-concepts/index.html#logistic-regression",
    "href": "academics/695M/bigger-concepts/index.html#logistic-regression",
    "title": "Bigger Concepts",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression’s output is a probability that a response variable belongs to a particular category.\nThe general form of a logistic model is:\n\\[p(X) = \\text{PR}(Y = 1|X)\\] The right side of the equation reads “The probability that Y equals 1, given X” - or the chance that Y is a particular category for a specific value of X.\nA linear regression model would use this form:\n\\[p(X) = \\beta_0 + \\beta_1X\\]\nAnd the magic formula for this is the logistic function, in which we raise \\(e\\) to this power.\n\\[p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}\\]\n(Why \\(e^{\\beta_0 + \\beta_1X}\\) instead of, say, \\(2^{\\beta_0 + \\beta_1X}\\) or \\(\\pi^{\\beta_0 + \\beta_1X}\\) or \\(123456^{\\beta_0 + \\beta_1X}\\)?)\nDividing both sides by \\(1 - p(X)\\) (somehow) yields:\n\\[\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1X}\\]\nwhich is also the odds that Y is 1 given X. If the probability of something is 20%, or 1/5, then the odds are 1/4:\n\\[\\frac{1}{5} \\div \\left[ 1 - \\frac{1}{5} \\right] = \\frac{1}{5} \\div \\frac{4}{5} = \\frac{1}{5} \\times \\frac{5}{4} = 1/4 = 0.25\\]\n\nWhen to use logistic regression\nWhen the response variable is categorical, and has two possible classes, like “Yes” or “No”, “Up” or “Down”, etc. However, see Linear Discriminant Analysis below for examples of when not to use it.\nIn some cases, a categorical response variable could be analyzed with a linear regression by treating the categorical variable as numeric, like treating “not infected” as 0 and “infected” as 1, and using the rounded value as the predictor; however, beyond 2 possible responses it’s unlikely that a continuous scale could be established between the possible values, and it gets messy.\n\n\nHow to do logistic regression\nUse glm() with the argument family = binomial to indicate logistic regression.\nUsing the Boston data set from the ISLR2 package, the following will model the probability of a tract having a crim rate above the median, based on zn, using logistic regression:\n\n\n\nCall:\nglm(formula = crim_above_med ~ zn, family = binomial, data = boston)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4319  -1.4319   0.4634   0.9427   1.8532  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.58076    0.10727   5.414 6.17e-08 ***\nzn          -0.09545    0.01349  -7.075 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 701.46  on 505  degrees of freedom\nResidual deviance: 559.53  on 504  degrees of freedom\nAIC: 563.53\n\nNumber of Fisher Scoring iterations: 6\n\n\nThe zn.fit summary above will show the coefficients and p-values; the $coef aspect of the summary will show more detail.\npredict() can then be fed the fit to make predictions. The argument type = \"response\" will tell it to output probabilities “of the form \\(P(Y = 1|X)\\)” from the data that was used in the fit, also described as the training data. It returns a vector of probabilities for every data element.\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n0.2428\n0.6412\n0.6412\n0.6412\n0.6412\n0.6412\n\n\n\n\n\nThe responses can be converted into categories and turned into a confusion matrix to show how the regression did with the training data.\n\n\n\n\n\n\n\n\n\n\n \nNo\nYes\n\n\n\n\nNo\n119\n15\n\n\nYes\n134\n238\n\n\n\n\n\nThe overall success rate can be calculated with mean(), comparing each predicted value to the actual value:\n\n\n[1] 0.7055336\n\n\nThe model can then be run on test data that is held out from the main set, with the expectation that it won’t do quite as well as the training data. The efficacy can then be assessed in the same way, and if it’s not high, we can return to the model and try to find other forms that make better predictions - or perhaps conclude that we can’t make a good prediction at all."
  },
  {
    "objectID": "academics/695M/bigger-concepts/index.html#linear-discriminant-analysis-lda",
    "href": "academics/695M/bigger-concepts/index.html#linear-discriminant-analysis-lda",
    "title": "Bigger Concepts",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\nThe simplest use case for LDA is when there are more than two possible response classes; logistic regression gives the probability that an observation is in a class A or B, but it can only handle two classes.\nLDA also gives better results when “there is substantial separation between the two classes”, or where the sample size is small and the distribution of the predictors is approximately normal in each of the classes.\n\nHow do we determine this?"
  },
  {
    "objectID": "academics/695M/bigger-concepts/index.html#resampling-methods",
    "href": "academics/695M/bigger-concepts/index.html#resampling-methods",
    "title": "Bigger Concepts",
    "section": "Resampling methods",
    "text": "Resampling methods\nResampling methods are used to refine and test models against training data by repeatedly testing against different subsets within the data.\nCross-validation can be used to estimate the test error rate of a method (model assessment), and bootstrap is mostly to measure the accuracy of a parameter or of a method.\n\nCross-validation: validation set approach\nThis approach randomly splits the data into two sets: a training set and a validation or test set. Size ratios seem to be on the order of between 50-50 to 3:1 or 4:1 training-to-test data. The model is fit against the training data, and then predict() is used to predict the results for the validation data; the results of predict can then be compared against the actual classifications in the validation data to compute the error rate.\nThis process can be run repeatedly with different splits.\nThe general process is:\n\nRandomly split the dataset into training and validation sets\nFit the model to the training data\nPredict the outcomes of the model on the validation data\nConvert the predictions into categorical variables that match the field being predicted (like Yes/No, etc)\nThe error rate is the proportion of correct predictions in the validation data.\n\nThe code below runs 10 iterations and accumulates the results into a results data frame.\n\n\n\n\n\nCross-validation: leave-one-out (LOOCV)\nThis method is similar to the validation set approach, except that the validation set is only one element, and the training set is the remaining \\(n-1\\) observations. It is run automatically \\(n\\) times and returns an average of the test errors.\nThe process is simpler than the validation set approach:\n\nFit the model to the entire data set with glm() (omitting the subset argument)\nRun cv.glm on the fit\nThe delta factor of the return data is the cross-validation error rate for the test data.\n\nCode from the book:\n\n\n\n\n\nCross-validation: k-fold\nK-fold cross-validation is like leave-one-out cross-validation but with a larger test set; it splits the data up into \\(k\\) segments and runs the cross-validation on each. It may give better results if run repeatedly and averaged, as with the validation set approach. It is faster than LOOCV, obviously, because it uses less data.\nThe process is the same as LOOCV, except a parameter such as K = 10 is added to the cv.glm() call. 5 to 10 is a common range for values of \\(k\\)."
  },
  {
    "objectID": "academics/695M/index.html",
    "href": "academics/695M/index.html",
    "title": "695M - Machine Learning",
    "section": "",
    "text": "Date\n\nSpring 2022\n\nProfessor\n\nJustin Gross\n\n\nThis class was a deep (initial) dive into Machine Learning, with a focus on applications in the social sciences.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nUseful R tidbits: Machine Learning\n\n\n\n\n\n\n\nr\n\n\nmachine learning\n\n\n\n\nTasty R snacks that do useful things in Machine Learning\n\n\n\n\n\n\nFeb 3, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlossary\n\n\n\n\n\n\n\nmachine learning\n\n\nglossary\n\n\nmath\n\n\n\n\nGlossary for 695M - Machine Learning\n\n\n\n\n\n\nFeb 3, 2022\n\n\n19 min\n\n\n\n\n\n\n  \n\n\n\n\nBigger Concepts\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nBigger concepts for MS797 coursework\n\n\n\n\n\n\nFeb 3, 2022\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "academics/695M/useful-r-tidbits/index.html",
    "href": "academics/695M/useful-r-tidbits/index.html",
    "title": "Useful R tidbits: Machine Learning",
    "section": "",
    "text": "This document contains R snippets pertaining to work in the Machine Learning class. There is also one for the Political Networks Analysis class."
  },
  {
    "objectID": "academics/695M/useful-r-tidbits/index.html#chapter-3-linear-regression",
    "href": "academics/695M/useful-r-tidbits/index.html#chapter-3-linear-regression",
    "title": "Useful R tidbits: Machine Learning",
    "section": "Chapter 3: Linear Regression",
    "text": "Chapter 3: Linear Regression\n\n\n\nAttaching package: 'ISLR2'\n\n\nThe following object is masked from 'package:MASS':\n\n    Boston\n\n\n\nnames(): see the attributes of a model\nnames() is a nice, simple way to see the attributes of a linear model:\n\nattach(Boston)\nlm.fit <- lm(medv ~ lstat)\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n\n\n\ncoef(): see the model’s coefficients\ncoef() is a convenient way to get the coefficients attribute:\n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nNote: this is equivalent to lm.fit$coefficients, but slightly easier on the eyes.\n\n\nconfint(): see confidence intervals for a model\n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nNote: levels can be added with the level parameter (the default is 0.95):\n\nconfint(lm.fit, level = 0.90)\n\n                  5 %       95 %\n(Intercept) 33.626697 35.4809847\nlstat       -1.013877 -0.8862212\n\n\n\n\npredict(): see confidence OR prediction intervals for a model\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\nNote: prediction intervals are defined on p. 82; in brief, it is the interval in which we are \\(X\\)% certain that any future observation will fall.\nNote: the data.frame() call in the above parameter lists produce a nifty little data frame that is used by predict to identify the column to do calculations on, and the desired levels:\n\ndata.frame(lstat = (c(5, 10, 15)))\n\n  lstat\n1     5\n2    10\n3    15\n\n\nNote: the book uses the base R graphics:\n\nplot(lstat, medv)\nabline(lm.fit)\n\n\n\n\nThe tidyverse/ggplot version of this is:\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::select() masks MASS::select()\n\nggplot(Boston, mapping = aes(x = lstat, y = medv)) +\n  geom_point() +\n  geom_abline(intercept = coef(lm.fit)[1],\n              slope = coef(lm.fit)[2])\n\n\n\n\n…a bit more work, but more modern.\n\n\narranging plots in grids\nThe book uses base R, again, with par() and mfrow() to arrange plots:\n\npar(mfrow = c(2, 2))\nplot(lm.fit)\n\n\n\n\nggplot uses facet() to arrange plots from the same data set in grids.\nPlots from different data sets need additional packages to be combined; one option is the cowplot library and the plot_grid() function."
  },
  {
    "objectID": "academics/695M/useful-r-tidbits/index.html#multiple-linear-regression",
    "href": "academics/695M/useful-r-tidbits/index.html#multiple-linear-regression",
    "title": "Useful R tidbits: Machine Learning",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nIn order to fit a multiple linear regression model using least squares, we again use the lm() function. The syntax lm(y ~ x1 + x2 + x3) is used to fit a model with three predictors, x1, x2, and x3. The summary() function now outputs the regression coefficients for all the predictors.\n\nlm.fit <- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\nlm.fit <- lm(medv ~ ., data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1304  -2.7673  -0.5814   1.9414  26.2526 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\ncrim         -0.121389   0.033000  -3.678 0.000261 ***\nzn            0.046963   0.013879   3.384 0.000772 ***\nindus         0.013468   0.062145   0.217 0.828520    \nchas          2.839993   0.870007   3.264 0.001173 ** \nnox         -18.758022   3.851355  -4.870 1.50e-06 ***\nrm            3.658119   0.420246   8.705  < 2e-16 ***\nage           0.003611   0.013329   0.271 0.786595    \ndis          -1.490754   0.201623  -7.394 6.17e-13 ***\nrad           0.289405   0.066908   4.325 1.84e-05 ***\ntax          -0.012682   0.003801  -3.337 0.000912 ***\nptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\nlstat        -0.552019   0.050659 -10.897  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.798 on 493 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7278 \nF-statistic: 113.5 on 12 and 493 DF,  p-value: < 2.2e-16\n\n\nWe can access the individual components of a summary object by name (type ?summary.lm to see what is available). Hence summary(lm.fit)$r.sq gives us the \\(R^2\\), and summary(lm.fit)$sigma gives us the RSE. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages() function in R.\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.767486 2.298459 3.987181 1.071168 4.369093 1.912532 3.088232 3.954037 \n     rad      tax  ptratio    lstat \n7.445301 9.002158 1.797060 2.870777 \n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\nlm.fit1 <- lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1851  -2.7330  -0.6116   1.8555  26.3838 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  41.525128   4.919684   8.441 3.52e-16 ***\ncrim         -0.121426   0.032969  -3.683 0.000256 ***\nzn            0.046512   0.013766   3.379 0.000785 ***\nindus         0.013451   0.062086   0.217 0.828577    \nchas          2.852773   0.867912   3.287 0.001085 ** \nnox         -18.485070   3.713714  -4.978 8.91e-07 ***\nrm            3.681070   0.411230   8.951  < 2e-16 ***\ndis          -1.506777   0.192570  -7.825 3.12e-14 ***\nrad           0.287940   0.066627   4.322 1.87e-05 ***\ntax          -0.012653   0.003796  -3.333 0.000923 ***\nptratio      -0.934649   0.131653  -7.099 4.39e-12 ***\nlstat        -0.547409   0.047669 -11.483  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.794 on 494 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7284 \nF-statistic: 124.1 on 11 and 494 DF,  p-value: < 2.2e-16\n\n\nAlternatively, the update() function can be used.\n\nlm.fit1 <- update(lm.fit, ~ . - age)"
  },
  {
    "objectID": "academics/695M/useful-r-tidbits/index.html#interaction-terms",
    "href": "academics/695M/useful-r-tidbits/index.html#interaction-terms",
    "title": "Useful R tidbits: Machine Learning",
    "section": "Interaction Terms",
    "text": "Interaction Terms\nIt is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:black tells R to include an interaction term between lstat and black. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age.\n\nsummary(lm(medv ~ lstat * age, data = Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "academics/695M/useful-r-tidbits/index.html#non-linear-transformations-of-the-predictors",
    "href": "academics/695M/useful-r-tidbits/index.html#non-linear-transformations-of-the-predictors",
    "title": "Useful R tidbits: Machine Learning",
    "section": "Non-linear Transformations of the Predictors",
    "text": "Non-linear Transformations of the Predictors\nThe lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula object; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of medv onto lstat and lstat^2.\n\nlm.fit2 <- lm(medv ~ lstat + I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nlm.fit <- lm(medv ~ lstat)\nanova(lm.fit, lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere Model 1 represents the linear submodel containing only one predictor, lstat, while Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2. The anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. If we type\n\npar(mfrow = c(2, 2))\nplot(lm.fit2)\n\n\n\n\nthen we see that when the lstat^2 term is included in the model, there is little discernible pattern in the residuals.\nIn order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\n\nlm.fit5 <- lm(medv ~ poly(lstat, 5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16\n\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nBy default, the poly() function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the poly() function, the argument raw = TRUE must be used.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -76.488      5.028  -15.21   <2e-16 ***\nlog(rm)       54.055      2.739   19.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "academics/695M/useful-r-tidbits/index.html#qualitative-predictors",
    "href": "academics/695M/useful-r-tidbits/index.html#qualitative-predictors",
    "title": "Useful R tidbits: Machine Learning",
    "section": "Qualitative Predictors",
    "text": "Qualitative Predictors\nWe will now examine the Carseats data, which is part of the ISLR2 library. We will attempt to predict Sales (child car seat sales) in \\(400\\) locations based on a number of predictors.\n\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\nlm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, \n    data = Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\n\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nUse ?contrasts to learn about other contrasts, and how to set them.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location."
  },
  {
    "objectID": "academics/695M/glossary/index.html",
    "href": "academics/695M/glossary/index.html",
    "title": "Glossary",
    "section": "",
    "text": "Page numbers in parenthesis after terms, from ISLR 2nd edition. Non-page numbers indicate other sources; “biostats” references material from Biostatistics 690Z (Health Data Science: Statistical Modeling), fall 2021."
  },
  {
    "objectID": "academics/695M/glossary/index.html#chapter-2",
    "href": "academics/695M/glossary/index.html#chapter-2",
    "title": "Glossary",
    "section": "Chapter 2",
    "text": "Chapter 2\n\ninput variable (15)\n\nalso predictor, independent variable, feature; usually written \\(X_1, X_2\\), etc. The parameter or parameters we are testing to see if they are related to or affect the output.\n\noutput variable (15)\n\nalso response, dependent variable; usually written \\(Y\\). The outcome being measured.\n\nerror term (16)\n\n\\(\\epsilon\\) in the equation\n\\[Y = f(X) + \\epsilon\\] a random quantity of inaccuracy, independent of X and with mean 0.\n\nsystematic (16)\n\n\\(f\\) in the equation\n\\[Y = f(X) + \\epsilon\\] the function that describes the (systematic) information \\(X\\) provides about \\(Y\\). This plus the error term equals \\(Y\\).\n\nreducible error (18)\n\nThe amount of the error \\(\\epsilon\\) that could be eliminated by improving our estimator \\(\\hat{f}\\); the difference between \\(\\hat{f}\\) and \\(f\\). This book and course is mostly about ways to minimize the reducible error.\n\nirreducible error (18)\n\nThe amount of \\(\\epsilon\\) that could not be reduced even if \\(f\\) was a perfect estimator of \\(Y\\). Always greater than 0. Could be due to hidden variables in \\(\\epsilon\\), or random fluctuations in Y, like a measure of “[a] patient’s general feeling of well-being on that day”.\n\nexpected value (19)\n\naverage value of an expected measure.\n\ntraining data (21)\n\ndata used to develop the model for estimating \\(f\\).\n\nparametric methods (21)\n\nA model based on one or more input parameters, that yields a value for Y, as in:\n\\[f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\\]\n\\[Y \\approx \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\\]\n\\[\\text{income} \\approx \\beta_0 + \\beta_1 \\times \\text{education} + \\beta_2 \\times \\text{seniority}\\]\nThis creates a predictive, inflexible model which usually does not match the true \\(f\\), but which has advantages of simplicity and interpretability. It can be used to predict values for \\(Y\\) based on its parameters, or inputs. Linear and logistic regression are parametric.\n\nnon-parametric methods (23)\n\nmethods that do not attempt to estimate \\(f\\). More flexible and have the potential to very closely match observations, but with the risk of overfitting the data and increasing the variance of subsequent observations. They require much more data than parametric models, and may be difficult to interpret, K-Nearest Neighbor and Support Vector Machines are non-parametric.\n\nprediction (26)\n\nseeking to guess the value of an response variable \\(y_i\\) given a set of observations and a predictor \\(f\\).\n\ninference (26)\n\na model that seems to better understand the relationship between the response and the predictors.\n\nsupervised learning (26)\n\na category of model that allows us to guess a \\(y_i\\) response to a set of predictor measurements \\(x_i, i = 1, \\dots, n\\).\n\nunsupervised learning (26)\n\na category of model in which there are observations/measurements \\(x_i, i = 1, \\dots, n\\), but no associated response \\(y_i\\). Linear regression cannot be used because there is no response variable to predict.\n\ncluster analysis (27)\n\nin unsupervised learning, a statistical method for determining whether a set of observations can be divided into “relatively distinct groups,” looking for similarities within the groups. (Topic modeling may be an example of this.)\n\nquantitative variables (28)\n\nnumeric values; age, height, weight, quantity. Usually the response variable type for regression problems.\n\nqualitative variables (28)\n\nalso categorical: values from a discrete set. Eye color, name, yes/no. Usually the response variable type for classification problems.\n\nregression problems (28)\n\nproblems with quantitative response variables. Given predictors foo, bar, and baz, how big is the frob?\n\nclassification problems (28)\n\nproblems with qualitative response variables. Given predictors foo, bar, and baz, is the outcome likely to be a frob, a frib or a freeb?\n\nmean squared error (MSE) (29)\n\nthe average squared error for a set of observations: \\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{f}(x_i))^2\\] MSE is small if the predicted responses are close to the true responses, and larger as it becomes less accurate; computed from training data, and Gareth et al. suggest it should be called training MSE.\n\nvariance (34)\n\n*“the amount by which** \\(\\hat{f}\\) would change if we estimated it using a different training data set” More practically: the average* of squared differences from the mean, often expressed as \\(\\sigma^2\\), where \\(\\sigma\\) (or the square root of the variance) is the standard deviation per StatQuest: “the difference in fits between data sets” (like training and test)\n\nbias (35)\n\n“the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model”, as in the error from the (presumed) linearity of a regression against non-linear data whose complexity it does not capture. More flexible models increase variance and decrease bias. per StatQuest: “The inability for a machine learning method (like linear regression) to capture the true relationship is called bias” - a straight line trying to model a curved separation in classes will never get it right and always be biased\n\nbias (65)\n\nin an estimator, something that systematically misses the true parameter; for an unbiased estimator, \\(\\hat{\\mu} = \\mu\\) when averaged over (huge) numbers of observations\n\nbias-variance trade-off (36)\n\nThe tension in seeking the best model for the data between missing the true \\(f\\) with an overly simple (biased) model, vs. an overfitted model with too much variance from mapping too closely to test data.\n\nerror rate (37)\n\nIn classification, the proportion of classifications that are mistakes. \\[\\frac{1}{n}\\sum_{i=1}^{n}I(y_i \\neq \\hat{y}_i)\\] \\(I\\) is 1 if \\(y_i \\neq \\hat{y}_i\\) - if the guess for any given \\(y\\) is wrong. The error rate is the percentage of incorrect classifications. Also the training erorr rate.\n\nindicator variable (37)\n\n\\(I\\) in the error rate definition above; a logical variable indicating the presence or absence of a characteristic or trait (such as an accurate classification).\n\ntest error rate (37)\n\nlike the training error rate but applied to the test data. Uses \\(\\text{Ave}\\) instead of sum notation: \\[\\text{Ave}(I(y_0 \\neq \\hat{y}_0))\\] \\(\\hat{y}_0\\) is the predicted class label from the classifier.\n\nconditional probability (37)\n\nThe chance that \\(Y = j\\) given an observed \\(x_0\\), as in the Bayes classifier: \\[\\text{Pr}(Y = j|X = x_0)\\] In a two-class, yes/no classifier, we decide based on whether \\(\\text{Pr}(Y = j|X = x_0)\\) is \\(> 0.5\\), or not. Note that \\(Y\\) is the class, as in “ham”/“spam”, not a \\(y\\)-axis coordinate.\n\nBayes decision boundary (38)\n\na visual depiction of the line of 50% probability dividing (exactly two?) classes in a two-dimensional space\n\nBayes error rate (38)\n\nthe expected (average) probability of classification error over all values of X in a data set. \\[1 - E(\\underset{j}{maxPr}(Y = j|X))\\] The \\(\\underset{j}{maxPr}\\) whichever of the \\(j\\) classes has the highest probability for any given value of \\(X\\). Again, \\(Y\\) is not a y-axis coordinate of a two-dimensional space, it’s the class of the classification: “yes”/“no”, “ham”/“spam”, “infected”/“not infected”. Also: “The Bayes error rate is analogous to the irreducible error, discussed earlier.”\n\nK-nearest-neighbors (KNN) (39)\n\na classifier that assigns a class Y to an observation based on the population proportions of its nearest neighbors; a circular “neighborhood” on a two-dimensional plot. It looks at actual data points that have been classified, and asks what any given non-classified point would be classified as based on its nearest neighbors."
  },
  {
    "objectID": "academics/695M/glossary/index.html#chapter-3",
    "href": "academics/695M/glossary/index.html#chapter-3",
    "title": "Glossary",
    "section": "Chapter 3",
    "text": "Chapter 3\n\nSynergy effect / interaction effect (60)\n\nwhen two or more predictors affect each other as well as the outcome; when 50k each in TV or radio ads give different results than 100k in either one\n\nSimple linear regression\n\nthe simplest model, predicting \\(Y\\) from a single predictor \\(X\\). \\[Y \\approx \\beta_0 + \\beta_1X\\] \\(\\approx\\) = “is approximately modeled as”\n\nleast squares (61)\n\nthe most common measure of closeness of a regression line to its data points, the sum of squares of the distances between the points and the closest point on the line (directly above or below)\n\nresidual (61)\n\nthe difference between \\(y_i\\) and \\(\\hat{y}_i\\), also \\(e_i\\); the difference between the \\(i\\)th response variable and the \\(i\\)th response variable predicted by the model StatQuest: the difference between the observed and predicted value\n\nresidual sum of squares (RSS) (62)\n\nthe sum of the squared residuals for each point on the regression line \\[\\text{RSS} = e_1^2 + e_2^2 + \\dots + e_n^2\\] Formulas for \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) are on p. 62\n\nintercept (\\(\\beta_0\\)) (63)\n\nthe expected value of \\(Y\\) when \\(X = 0\\)\n\nslope (\\(\\beta_1\\)) (63)\n\nthe average increase in \\(Y\\) associated with a one-unit increase in \\(X\\)\n\nerror term (\\(\\epsilon\\)) (63)\n\nwhatever we missed with the model, due to the true model not being linear (it almost never is), measurement error, or other variables that cause variation in \\(Y\\)\n\npopulation regression line (63)\n\n“the best linear approximation to the true relationship between \\(X\\) and \\(Y\\)” \\[Y = \\beta_0 + \\beta_1X + \\epsilon\\] least squares line (63)\n\n\nthe regression line made of the least-squares estimates for \\(\\beta_0\\) and \\(\\beta_1\\)\n\nstandard error (SE) (65)\n\nthe average amount that an estimate \\(\\hat{\\mu}\\) (sample mean) differs from the actual value of \\(\\mu\\) (population mean) \\[\\text{Var}(\\hat{\\mu}) = \\text{SE}(\\hat{\\mu})^2 = \\frac{\\sigma^2}{n} (\\text{also} = \\frac{\\sigma}{\\sqrt{n}})\\] \\(\\sigma\\) is “the standard deviation of each of the realizations \\(y_i\\) of \\(Y\\). Since \\(\\sigma^2\\) is divided by \\(n\\), the standard error shrinks as observations increase. It represents the amount we would expect means of additional samples to”jump around” simply due to random chance and the limitations of the model’s accuracy.1\n\nresidual standard error (RSE) (66)\n\nthe estimate of \\(\\sigma\\) \\[\\text{RSE} = \\sqrt{RSS / (n-2)}\\]\n\nconfidence interval (66)\n\na range of values within which we have a measured probability (often 95%) of containing the true value of the parameter; a 95% confidence interval in linear regression takes the form \\[\\hat{\\beta_1} \\pm 2 \\cdot \\text{SE}(\\hat{\\beta_1})\\]\n\nt-statistic (67)\n\nthe number of standard deviations that \\(\\hat{\\beta_1}\\) is away from \\(0\\). \\[t = \\frac{{\\hat{\\beta_1}} - 0}{\\text{SE}(\\hat{\\beta_1})}\\] For there to be a relationship between \\(X\\) and \\(Y\\), \\(\\hat{\\beta_1}\\) has to be nonzero (i.e. have a slope). The standard error (SE) of \\(\\hat{\\beta_1}\\) (in the denominator above) measures its accuracy; if it is small, then \\(t\\) will be larger, and if it is large, then \\(t\\) will be smaller. \\(t\\) is around 2 for a p-value of 0.05 (actually about 1.96, as 2 standard deviations is 95.45% of a normal distribution), and around 2.75 for a p-value of 0.01.\n\np-value (67)\n\nthe probability of observing a value greater than \\(|t|\\) by chance.\n\nmodel sum of squares (MSS) (biostats)\n\nAlso sometimes ESS, “explained sum of squares”: the total variance in the response \\(Y\\) that can be accounted for by the model \\[\\text{MSS} = \\sum(\\hat{y_i} - \\bar{y})^2\\]\n\nresidual sum of squares (RSS) (biostats)\n\nthe total variance in the response \\(Y\\) that cannot be accounted for by the model \\[\\text{RSS} = \\sum(y_i - \\hat{y_i})^2\\] also \\[\\text{RSS} = e_i^2 + e_2^2 + \\dots + e_n^2\\] or \\[\\text{RSS} = (y_1 - \\hat{\\beta_0} - {\\hat{\\beta_1}x_1})^2 + (y_2 - \\hat{\\beta_0} - {\\hat{\\beta_1}x_2})^ + \\dots + (y_n - \\hat{\\beta_0} - {\\hat{\\beta_1}x_n})^2\\]\n\ntotal sum of squares (TSS) (70)\n\nthe total variance in the response \\(Y\\); the total variability of the response about its mean \\[\\text{TSS} = \\sum(y_i - \\bar{y})^2\\] compare with RSS, the amount of variability left unexplained after the regression. TSS - RSS is the amount of variability (or error) explained by the regression (MSS).\n\n\nNOTE: there is a nice visual here on stackexchange; if anybody knows how to tell Zotero to use a custom bibtex citation entry over the ones it generates, please let me know so I can integrate it better here :frown:\n\n\\(R^2\\) statistic (70)\n\nthe proportion of variance in \\(Y\\) explained by \\(X\\), a range from 0 to 1 \\[R^2 = \\frac{\\text{TSS - RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\\] \\(R^2\\) values close to 1 indicate a regression that explains a lot of the variability in the response, and a stronger model. A value close to 0 indicates that the regression doesn’t explain much of the variability.\n\ncorrelation (70)\n\na measure of the linearity of the relationship between \\(X\\) and \\(Y\\); values close to 0 indicate weak-to-no relationship, values near 1 or -1 indicate strong positive or negative correlation \\[\\text{Cor(X, Y)} = {\\frac {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{{\\sqrt {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}{\\sqrt {\\sum _{i=1}^{n}(y_{i}-{\\bar {y}})^{2}}}}}\\]\n\nstandard linear regression model (72)\n\nThe modal used for standard linear models, used to interpret the the effect on \\(Y\\) of a one-unit increase in any predictor \\(\\beta_j\\) while holding all other predictors constant \\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\epsilon\\]\n\nvariable selection (78)\n\nthe task of refining a model to include only the variables associated with the response\n\nnull model (79)\n\na model that conatins an intercept, but no predictors; used as a first stage in forward selection\n\nforward selection (79)\n\na variable selection method that starts with a null model, and then runs simple linear regressions on all predictors \\(p\\) and adding the one that results in the lowest RSS; repeated until some threshold is reached\n\nbackwards selection (79)\n\na variable selection method that starts with a model with all predictors, and removing the one with the lowest \\(p\\)-value until all remaining predictors are significant, whether by \\(p\\)-value or some other criterion\n\nmixed selection (79)\n\na hybrid approach starting with a null model, adding predictors one at a time that produce the best fit, and removing any that acquire a larger \\(p\\)-value in the process until all predictors are added or eliminated\n\ninteraction (81)\n\nwhen predictors affect each other, in addition to providing their own effect on the model\n\nconfidence interval (82)\n\na range with a percentage component in which there is that percentage chance that the true value of an estimated parameter lies; a 95% confidence interval is a range in which we can be 95% certain \\(f(X)\\) will be found\n\nprediction interval (82)\n\nsimilar to confidence interval, but a prediction range within which we are \\(X\\)% certain that any singular future observation will fall, rather than a statistic like an overall mean; a 95% prediction interval is a range in which we are confident that 95% of future observations will fall. Prediction ranges are substantially wider than confidence intervals.\n\nqualitative predictor / factor (83)\n\na categorical predictor with a fixed number of factors, like “yes” / “no” or “red” / “yellow” / “green”\n\ndummy variable (83)\n\na numeric representation of a factor to use in a model, as in representing “yes” / “no” factor variables as 1 / 0 in a regression\n\nbaseline (86)\n\nthe factor level where there is no dummy variable; a factor with 3 levels will use 2 dummy variables, with the factor’s absence signifying the 3rd value (usually 0)\n\nadditivity assumption (87)\n\nthe assumption that the association between a predictor \\(X\\) and the response \\(Y\\) does not depend on the value of other predictors; used by the standard linear regression model\n\nlinearity assumption (87)\n\nthe assumption, also used by the standard linear regression model, that unit changes in \\(X_j\\) result in the same change to Y regardless of its value\n\ninteraction term (88)\n\nthe product of two predictors in a multiple regression model, quantifying their effect on each other\n\nmain effect (89)\n\nisolated effects; the effect of a single predictor on the outcome\n\nhierarchical principle (89)\n\nthe principle that main effects should be left in a model even if they are statistically insignificant, if they are also part of an interaction that is significant\n\npolynomial regression (91)\n\nan extension of linear regression to accommodate non-linear relationships\n\nresidual plot (93)\n\na plot of the residuals or errors (\\(e_i = y_i - \\hat{y}_i\\)), used to check for non-linearity (a potential problem that would likely indicate something was missed in the model)\n\ntime series (94)\n\ndata consisting of observation made at discrete points in time\n\ntracking (95)\n\nwhen (residuals / variables?) tend to have similar values\n\nheteroscedasticity (96)\n\nnon-constant variances in errors; “unequal scatter”\n\nhomoscedasticity (extra)\n\nconstant variances in errors; follows the assumption of equal variance required by most methods\n\nweighted least squares (97)\n\nan extension to ordinary least squares used in circumstances of heteroscedasticity, to weight data points proportionally with the inverse variances\n\noutlier (97)\n\nan observation whose value is very far from its predicted value\n\nstudentized residual (98)\n\na residual divided by its estimated standard error; observations with student residuals higher than 3 (indicating 3 standard deviations) are likely outliers\n\nhigh leverage (98)\n\nobservations with an unusual \\(x_i\\) value, far from other / expected \\(x\\) values\n\nleverage statistic (99)\n\na quantification of a point’s leverage \\[h_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{i'=1}^{n}(x_{i`} - \\bar{x})^2}\\]\n\ncollinearity (99)\n\nwhen two or more predictor variables are closely related to each other\n\npower (101)\n\nthe probability of a test correctly detecting a nonzero coefficient (and correctly rejecting \\(H_0 : \\beta_j = 0\\))\n\nmulticollinearity (102)\n\nwhen collinearity exists between three or more predictors even when no pair of predictors is collinear (or correlated)\n\nvariance inflation factor (102)\n\n“the ratio of the variance of \\(\\hat{\\beta_j}\\) when fitting the full model divided by the variance of \\(\\hat{\\beta_j}\\) on its own”; smallest possible value of 1 indicates the absence of collinearity, 5-10 indicates a “problematic amount”. \\[\\text{VIF}(\\hat{\\beta_j}) = \\frac{1}{1 - {R_{X_j|X_-j}^2}}\\] “\\(R_{X_j|X_-j}^2\\) is the \\(R^2\\) from a regression of \\(X_j\\) onto all of the other predictors”\n\nK-nearest neighbors regression (105)\n\na mode of regression that seeks to classify observations by their proximity to classified neighbors\n\ncurse of dimensionality (107)\n\nwhen an observation has no nearby neighbors due to a high number of dimensions exponentially increasing the available space for other observations to be spread out in"
  },
  {
    "objectID": "academics/697E/697E-hw5/index.html",
    "href": "academics/697E/697E-hw5/index.html",
    "title": "Homework 5: Constraint and Brokerage in the Florentine Wedding dataset",
    "section": "",
    "text": "data(\"florentine\", package = \"ergm\")\nntwk_stat <- flomarriage\n\ndata(flo)\nntwk_ig <- graph_from_adjacency_matrix(flo)\nBuilding on last week’s look at basic network centrality attributes in the Florentine Wedding dataset, we’re going to look at two new measurements of centrality: brokerage and constraints.\n(Note: these two concepts begin to break the “use either one” conclusion when choosing statnet or igraph; brokerage is a function of statnet, and constraints are a function of igraph.)"
  },
  {
    "objectID": "academics/697E/697E-hw5/index.html#burts-constraint",
    "href": "academics/697E/697E-hw5/index.html#burts-constraint",
    "title": "Homework 5: Constraint and Brokerage in the Florentine Wedding dataset",
    "section": "Burt’s constraint",
    "text": "Burt’s constraint\nBurt’s constraint can be thought of as a measure of a node’s dependency on other nodes for connections or information. If a node has a small number of connections, or the connections are largely redundant (i.e. a small group of friends without many other connections), that node has a higher constraint than one that has connections to nodes that are less redundant to each other. It provides more pathways, or opportunities, for information or power to flow.\nIt should come as no surprise, given all of the various ways that the Medici are the dominant node in this dataset, that the Medici family has the lowest overall constraint.\nAs a reminder, here is the original network graph of the Florence wedding data with no scaling applied:\n\nset.seed(1235)\nplot(\n  ntwk_ig,\n  layout = layout_components(ntwk_ig),\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence interfamily marriages, 1282-1500\"\n)\n\n\n\n\nAnd here we scale the nodes by their Burt’s constraint measure:\n\nntwk_ig_b <- ntwk_ig\nntwk_ig_b.nodes <- ntwk_ig.nodes\nntwk_ig_b.nodes$burt <- ifelse(is.na(ntwk_ig_b.nodes$burt), 1, ntwk_ig_b.nodes$burt)\n\nntwk_ig_b.nodes$burt_inv <- 5 * (1 - ntwk_ig_b.nodes$burt)\nV(ntwk_ig_b)$size <- ntwk_ig_b.nodes[,\"burt_inv\"] * 5\nV(ntwk_ig_b)$label.cex <- ntwk_ig_b.nodes[,\"burt_inv\"] / 4\nV(ntwk_ig_b)$label.cex <- pmax(V(ntwk_ig_b)$label.cex, 1)\n\n# Ensure minimum node size\nV(ntwk_ig_b)$size <- pmax(V(ntwk_ig_b)$size, 1)\n\nburts_labels <- sprintf(\"%s\\n%s\", V(ntwk_ig_b)$name,\n                            # Omit leading \"0\" can't be done in sprintf\n                            substr(as.character(round(\n                              ntwk_ig_b.nodes$burt, 3\n                            )), 2, 10))\nset.seed(1235)\nplot(\n  ntwk_ig_b,\n  layout = layout_components(ntwk_ig_b),\n  vertex.label = burts_labels,\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence families by Burt's constraint\"\n)\n\n\n\n\nOnce again, we see the clear dominance of the Medici family in this measure. Note that all of the pendant nodes (Lamberteschi, Ginori, Pazzi and Acciaiuoli) have maximal constraints of 1; they are completely constrained by their single connections, by definition. The Pucci family’s constraint is undefined, as it is a disconnected node.\nThe Medici have the lowest constraints of any family, being well-connected to other well-connected nodes with some spread among them. It is interesting to note that the Guadagni family is not far behind, though, at 0.25 to 0.21 for the Medici. They are connected to 3 well-connected nodes besides the Lamberteschi, and the Bischeri node in particular gives them indirect access to the Strozzi and Peruzzi nodes, besides the Albizzi and Tornabouni nodes that they share with the Medici. It can be seen from this that the Bischeri node is a strength for the Guadagni family in much the same way that the Barbadori node is a strength of the Medici."
  },
  {
    "objectID": "academics/697E/697E-hw5/index.html#brokerage",
    "href": "academics/697E/697E-hw5/index.html#brokerage",
    "title": "Homework 5: Constraint and Brokerage in the Florentine Wedding dataset",
    "section": "Brokerage",
    "text": "Brokerage\nWith Gould-Fernandez Brokerage, we begin to look at the ways in which nodes mediate connections between other groups of nodes. This requires a directed network, since the direction of flow of access is a crucial aspect of the core concept. There are five roles recognized by Gould-Fernandez Brokerage:\n\nCoordinator\n\nmediates contact between two individuals from his or her own group\n\nItinerant broker\n\nmediates contact between two individuals from a single group to which he or she does not belong\n\nGatekeeper\n\nmediates an incoming contact from an out-group member to an in-group member\n\nRepresentative\n\nmediates an outgoing contact from an in-group member to an out-group member\n\nLiaison\n\nmediates contact between two individuals from different groups, neither of which is the group to which he or she belongs\n\n\nUnfortunately for the present moment, however, we cannot apply this concept to the Florence wedding data set, because at present there are no subgroups or categories between which to identify brokers! We will need to add data to the set to find a basis for this sort of analysis, and this is where we begin to transition to a research project.\nI am interested in the patronage system in Renaissance Florence, particularly as it applied to artists and scientists, as a measure of influence of the families. We may be able to return to brokerage concepts once we map in data related to this concept."
  },
  {
    "objectID": "academics/697E/697E-hw3/index.html",
    "href": "academics/697E/697E-hw3/index.html",
    "title": "Homework 3: The Florentine Wedding dataset",
    "section": "",
    "text": "data(flo)\nntwk.ig <- graph_from_adjacency_matrix(flo)\n\nntwk.vcount <- vcount(ntwk.ig)\nntwk.ecount <- ecount(ntwk.ig)\nntwk.is_bipartite <- is_bipartite(ntwk.ig)\nntwk.is_directed <- is_directed(ntwk.ig)\nntwk.is_weighted <- is_weighted(ntwk.ig)\nntwk.vertex_attr_names <- vertex_attr_names(ntwk.ig)\nntwk.edge_attr_names <- edge_attr_names(ntwk.ig)\nntwk.names <- V(ntwk.ig)$name\nntwk.dyad_census <- igraph::dyad.census(ntwk.ig)\nntwk.triad_census <- igraph::triad_census(ntwk.ig)\nntwk.global_clustering_coef <- transitivity(ntwk.ig, type = \"global\")\nntwk.local_avg_clustering_coef <- transitivity(ntwk.ig, type = \"average\")\nntwk.avg_path_length <- average.path.length(ntwk.ig, directed = T)\nntwk.component_names <- names(igraph::components(ntwk.ig))\nntwk.components_no <- igraph::components(ntwk.ig)$no\nntwk.components_csize <- igraph::components(ntwk.ig)$csize\n\n# distances(ntwk.ig,\"Acciaiuoli\",\"Strozzi\", weights=NA)\n\nntwk.graph_density <- graph.density(ntwk.ig)\nntwk.graph_density_noloops <- graph.density(ntwk.ig, loops = TRUE)\n\nntwk.avg_degree <- igraph::degree(ntwk.ig)\n\n# Get the degree info and make the data frame\n\nntwk.nodes <- data.frame(\n  name = V(ntwk.ig)$name, \n  degree = igraph::degree(ntwk.ig),\n  indegrees = igraph::degree(ntwk.ig, mode = \"in\", loops = FALSE),\n  outdegrees = igraph::degree(ntwk.ig, mode = \"out\", loops = FALSE)\n  )\n# ntwk.nodes[ntwk.nodes$name %in% c(\"Medici\", \"Strozzi\"),]\n\n# Get degree centralization\n\nntwk.indegree_centralization <- \n  centr_degree(ntwk.ig, loops = FALSE, mode = \"in\")$centralization\nntwk.outdegree_centralization <- \n  centr_degree(ntwk.ig, loops = FALSE, mode = \"out\")$centralization\nntwk.total_centralization <- \n  centr_degree(ntwk.ig, loops = FALSE, mode = \"total\")$centralization\n\n\nFor this assignment, I am looking at the Florentine Wedding dataset by J. F. Padgett, representing intermarriages between prominent families in Florence between 1282 and 1500. This is an area of interest to me as I have some background in fine art, and the Medici family in particular was known for its patronage of some of history’s greatest artists throughout the Renaissance, including Boticelli, Leonardo da Vinci, and Michelangelo. No less than Galileo Galilei himself tutored several generations of Medici children.\nThe great families of this era were frequently fierce rivals, with delicately balanced power structures measured in various spheres of influence; intermarriages were one way that alliances were forged between families in the ceaseless effort to gain political or financial advantage over their competitors.\nThe Medici family was the most prominent in Florence throughout the renaissance. Two other prominent families, the Strozzi and Guadagni, also exercised considerable influence (they were all banking families), and did not intermarry with the Medici, though they were not far removed. This essay looks at the marriage network between the various families as one measure of social power and centrality in Renaissance Florence.\nThe flo dataset in the R network library is an adjacency matrix of 16 families of this era, including the aforementioned. It contains the 16 nodes of the families, and 20 unweighted, directed edges between them. The Pucci family is an isolate, making the network disconnected; the remaining nodes form a single component.\nThe families in the dataset, with their total intermarriage count (degree), marriages into each family (indegrees) and marriages out to other families (outdegrees) are:\n\nntwk.nodes[,c(\"degree\", \"indegrees\", \"outdegrees\")]\n\n             degree indegrees outdegrees\nAcciaiuoli        2         1          1\nAlbizzi           6         3          3\nBarbadori         4         2          2\nBischeri          6         3          3\nCastellani        6         3          3\nGinori            2         1          1\nGuadagni          8         4          4\nLamberteschi      2         1          1\nMedici           12         6          6\nPazzi             2         1          1\nPeruzzi           6         3          3\nPucci             0         0          0\nRidolfi           6         3          3\nSalviati          4         2          2\nStrozzi           8         4          4\nTornabuoni        6         3          3\n\n\nA visualization shows each marriage as a line connecting two families:\n\nV(ntwk.ig)$size <- ntwk.nodes[,\"degree\"] * 5\nV(ntwk.ig)$label.cex <- ntwk.nodes[,\"degree\"] / 4\nV(ntwk.ig)$label.cex <- pmax(V(ntwk.ig)$label.cex, 1)\n\nplot(\n  ntwk.ig,\n  layout = layout_in_circle(ntwk.ig),\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence interfamily marriages, 1282-1500\"\n)\n\n\n\n\nEach node is sized according to its number of connections; it is easy to see from this diagram that the Medici family had the most marriages within the Florence family network, with six. The Strozzi and Guadagni families were next at four apiece.\nA component layout offers a different view that makes distances and paths easier to read:\n\nset.seed(1235)\nV(ntwk.ig)$size <- ntwk.nodes[,\"degree\"] * 3\nplot(\n  ntwk.ig,\n  layout = layout_components(ntwk.ig),\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence interfamily marriages, 1282-1500\"\n)\n\n\n\n\nFrom this layout, we can see that the Medici, Strozzi and Guadagni never directly intermarried, but were also only separated from each other by one node. With an overall average path length of approximately 2.5, this suggests some calculus and deliberation: not directly joining with their main rivals, but remaining only one degree removed.\nThe overall network density is \\(\\approx\\) 0.16; 20 connections by marriage out of a possible total of 120. Four of the 16 nodes are pendants. We might expect the network density to increase if we looked at more families than the 16 listed here; it is very unlikely that only 16% of families experienced any marriages at all! This would need to be weighed against the prominence of the additional families according to the criteria with which this initial set was selected.\nIt would be interesting to me to look deeper into a bipartite network of connections between the families and whom they patronized, especially the artists and scientists of the era. This would require more data than is present in this set, but could be very revealing about other ways in which the families interacted, competing for social and political influence in the dynamic period of the Renaissance."
  },
  {
    "objectID": "academics/697E/697E-hw4/index.html",
    "href": "academics/697E/697E-hw4/index.html",
    "title": "Homework 4: Centrality Analysis of the Florentine Wedding dataset",
    "section": "",
    "text": "data(flo)\nntwk.ig <- graph_from_adjacency_matrix(flo)\n\nntwk.vcount <- vcount(ntwk.ig)\nntwk.ecount <- ecount(ntwk.ig)\nntwk.is_bipartite <- is_bipartite(ntwk.ig)\nntwk.is_directed <- is_directed(ntwk.ig)\nntwk.is_weighted <- is_weighted(ntwk.ig)\nntwk.vertex_attr_names <- vertex_attr_names(ntwk.ig)\nntwk.edge_attr_names <- edge_attr_names(ntwk.ig)\nntwk.names <- V(ntwk.ig)$name\nntwk.dyad_census <- igraph::dyad.census(ntwk.ig)\nntwk.triad_census <- igraph::triad_census(ntwk.ig)\nntwk.global_clustering_coef <- transitivity(ntwk.ig, type = \"global\")\nntwk.local_avg_clustering_coef <- transitivity(ntwk.ig, type = \"average\")\nntwk.avg_path_length <- average.path.length(ntwk.ig, directed = T)\nntwk.component_names <- names(igraph::components(ntwk.ig))\nntwk.components_no <- igraph::components(ntwk.ig)$no\nntwk.components_csize <- igraph::components(ntwk.ig)$csize\n\n# distances(ntwk.ig,\"Acciaiuoli\",\"Strozzi\", weights=NA)\n\nntwk.graph_density <- graph.density(ntwk.ig)\nntwk.graph_density_noloops <- graph.density(ntwk.ig, loops = TRUE)\n\nntwk.avg_degree <- igraph::degree(ntwk.ig)\n\n# Get the degree info and make the data frame\n\nntwk.nodes <- data.frame(\n  name = V(ntwk.ig)$name, \n  degree = igraph::degree(ntwk.ig),\n  indegrees = igraph::degree(ntwk.ig, mode = \"in\", loops = FALSE),\n  outdegrees = igraph::degree(ntwk.ig, mode = \"out\", loops = FALSE)\n  )\n# ntwk.nodes[ntwk.nodes$name %in% c(\"Medici\", \"Strozzi\"),]\n\n# Get degree centralization\n\nntwk.indegree_centralization <- \n  centr_degree(ntwk.ig, loops = FALSE, mode = \"in\")$centralization\nntwk.outdegree_centralization <- \n  centr_degree(ntwk.ig, loops = FALSE, mode = \"out\")$centralization\nntwk.total_centralization <- \n  centr_degree(ntwk.ig, loops = FALSE, mode = \"total\")$centralization\nThis week, we’re going to look at various measures of degree centrality in the Florentine Wedding dataset.\nLast week, we looked at some basic network plots that showed the overall structure of the network, sizing the nodes by their individual degrees. One of the primary features of this data set is all of the ways in which it can be shown how dominant the Medici family was in medieval Florence."
  },
  {
    "objectID": "academics/697E/697E-hw4/index.html#closeness-centrality",
    "href": "academics/697E/697E-hw4/index.html#closeness-centrality",
    "title": "Homework 4: Centrality Analysis of the Florentine Wedding dataset",
    "section": "Closeness Centrality",
    "text": "Closeness Centrality\nCloseness centrality is a measure of how close a node is, via the shortest path, to every other node in the network. It is more revealing in hierarchies, where dominant nodes that are closer to the center of the graph will have, on average, shorter paths to the rest of the node than the nodes at the periphery.\nThe closeness centrality measures of the nodes in the Florentine data may come as a bit of a surprise:\n\ncloseness_centrality <- closeness(ntwk.ig, mode = \"all\")\n# One node is not connected, and has NaN as its centrality; replace it \n# with 0 just for the graph below.\ncloseness_centrality[is.nan(closeness_centrality)] <- 0\n# Calculate node size by 1000 x closeness\nV(ntwk.ig)$size <- closeness_centrality * 1000\n# Ensure minimum node size\nV(ntwk.ig)$size <- pmax(V(ntwk.ig)$size, 1)\n# Base text size on node size\nV(ntwk.ig)$label.cex <- V(ntwk.ig)$size / 15\nV(ntwk.ig)$label.cex <- pmax(V(ntwk.ig)$label.cex, 1)\nset.seed(1235)\ncloseness_labels <- sprintf(\"%s\\n%s\", ntwk.names,\n                            # Omit leading \"0\" can't be done in sprintf\n                            substr(as.character(round(\n                              closeness_centrality, 3\n                            )), 2, 10))\nplot.igraph(\n  ntwk.ig,\n  layout = layout_components(ntwk.ig),\n  vertex.label = closeness_labels,\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence nodes by closeness centrality\"\n)\n\n\n\n\nAlthough careful examination shows that the Medici family is still the dominant node, with the highest closeness centrality score, there is very little variation among all but the disconnected Pucci node. The chief reason for this is that the network itself is very small, and tightly connected. As we saw last week, the average path length for this network is 2.486; the Florentine families were all in close proximity to each other, as might be expected given that one of its primary constraints is geographical, but it is interesting to see how the power of the Medici family did not derive primarily from its closeness centrality, or its position in a hierarchy."
  },
  {
    "objectID": "academics/697E/697E-hw4/index.html#betweenness-centrality",
    "href": "academics/697E/697E-hw4/index.html#betweenness-centrality",
    "title": "Homework 4: Centrality Analysis of the Florentine Wedding dataset",
    "section": "Betweenness Centrality",
    "text": "Betweenness Centrality\nAnother useful measure is betweenness centrality, which is a count of the number of shortest paths between nodes that pass through another node. It can be seen as a measure of importance or control, in that it reflects how well a node is positioned along the optimal path between two other nodes.\n\nbetweenness_centrality <- betweenness(ntwk.ig)\n# Calculate node size by 2 x betweenness\nV(ntwk.ig)$size <- betweenness_centrality / 2\n# Ensure minimum node size\nV(ntwk.ig)$size <- pmax(V(ntwk.ig)$size, 10)\n# Base text size on node size\nV(ntwk.ig)$label.cex <- V(ntwk.ig)$size / 15\nV(ntwk.ig)$label.cex <- pmax(V(ntwk.ig)$label.cex, 1)\n\ncolors <- case_when(\n  betweenness_centrality == 0 ~ \"#FFFF33\",\n  betweenness_centrality < 20 ~ \"#FF8033\",\n  TRUE ~ \"#FF0033\"\n)\nset.seed(1235)\nplot(\n  ntwk.ig,\n  vertex.color = colors,\n  layout = layout_components(ntwk.ig),\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence nodes by betweenness centrality\"\n)\n\n\n\n\nThis chart uses heatmap colors to indicate the strength of betweenness centrality. The yellow nodes - the network’s pendants and isolate - are yellow, which indicates a betweenness measure of 0. The orange nodes are nodes with 20 or less optimal paths running through them, and the red nodes have more than 20. In fact, the scale had to be forced in this graph to make the smaller-weighted betweenness nodes visible at all compared to the overwhelming dominance of the Medici node, with a betweenness score more than twice that of its nearest rival:\n\nsort(round(betweenness_centrality, 2), decreasing = TRUE)\n\n      Medici     Guadagni      Albizzi     Salviati      Ridolfi     Bischeri \n       95.00        46.33        38.67        26.00        20.67        19.00 \n     Strozzi    Barbadori   Tornabuoni   Castellani      Peruzzi   Acciaiuoli \n       18.67        17.00        16.67        10.00         4.00         0.00 \n      Ginori Lamberteschi        Pazzi        Pucci \n        0.00         0.00         0.00         0.00 \n\n\n95 of the 312 optimal paths in the Florence network ran through the Medici. It is easy to imagine many different ways that such centrality could manifest as various forms of power."
  },
  {
    "objectID": "academics/697E/697E-hw4/index.html#eigenvector-and-bonacich-power",
    "href": "academics/697E/697E-hw4/index.html#eigenvector-and-bonacich-power",
    "title": "Homework 4: Centrality Analysis of the Florentine Wedding dataset",
    "section": "Eigenvector and Bonacich Power",
    "text": "Eigenvector and Bonacich Power\nEigenvector centrality is a measure of not just how many alters a node is connected to, but of how well-connected those alters are. A node with a high eigenvector centrality score is connected to significant numbers of other highly central nodes. For this chart, we can return to standard-area scaling (except for slightly inflating the smallest to keep them visible), and we see that once again, the Medici are in a dominant position, well-connected to the other well-connected Florence families:\n\neigen_info <- centr_eigen(ntwk.ig, directed = T)\n\n# Calculate node size by 1000 x closeness\nV(ntwk.ig)$size <- eigen_info$vector * 30\n# Ensure minimum node size\nV(ntwk.ig)$size <- pmax(V(ntwk.ig)$size, 5)\n# Base text size on node size\nV(ntwk.ig)$label.cex <- V(ntwk.ig)$size / 15\nV(ntwk.ig)$label.cex <- pmax(V(ntwk.ig)$label.cex, 1)\n\n# colors <- case_when(\n#   betweenness_centrality == 0 ~ \"#FFFF33\",\n#   betweenness_centrality < 20 ~ \"#FF8033\",\n#   TRUE ~ \"#FF0033\"\n# )\nset.seed(1235)\neigen_labels <- sprintf(\"%s\\n%.2f\", ntwk.names, eigen_info$vector)\n\nplot(\n  ntwk.ig,\n  # vertex.color = colors,\n  layout = layout_components(ntwk.ig),\n  vertex.label = eigen_labels,\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence nodes by eigenvector centrality\"\n)\n\n\n\n\nIt is interesting to look at the Strozzi family in this diagram, which has the second-highest eigenvector centrality score behind the Medici. Recall that in terms of simple degree, the Strozzi and Guadagni families were equal, and in terms of betweenness centrality, the Guadagini family had a significantly higher score than the Strozzi, showing that more optimal / shortest paths between nodes passed through the Guadagini. This could be interpreted as a measure of positional influence and power. However, here we see that the Strozzi’s eigenvector centrality is higher than the Guadagini, and it is plain to see why; the Strozzi are connected to four other families with relatively high eigenvector centrality (the Bischeri, Peruzzi and Castellani), where the Guadagni family is somewhat penalized by its connection to the Lamberteschi pendant node that has no other connections. So, although the Guadagni family was more centrally positioned in terms of betweenness, the Strozzi family’s fewer optimal paths were nonetheless connected to more central families.\nFinally, it is worth examining the Bonacich power connections, which is an interesting measure that in some way inverts the eigenvector concept, proposing that being well-connected to other well-connected nodes does not mean that a node has the ability to influence those nodes, as they are also well-connected, and so therefore there is advantage in being connected to weaker nodes. When viewed purely in terms of potential power dynamics, this has some interesting merit.\n\nbon_info <- power_centrality(ntwk.ig)\n\n# H/T: range01 <- function(x){(x-min(x))/(max(x)-min(x))}\nrange01 <- function(x){(x-min(x))/(max(x)-min(x))}\nscaled_bon_info <- range01(bon_info)\n\n# Calculate node size by 1000 x closeness\nV(ntwk.ig)$size <- scaled_bon_info * 30\n# Ensure minimum node size\nV(ntwk.ig)$size <- pmax(V(ntwk.ig)$size, 5)\n# Base text size on node size\nV(ntwk.ig)$label.cex <- V(ntwk.ig)$size / 15\nV(ntwk.ig)$label.cex <- pmax(V(ntwk.ig)$label.cex, 1)\n\n# colors <- case_when(\n#   betweenness_centrality == 0 ~ \"#FFFF33\",\n#   betweenness_centrality < 20 ~ \"#FF8033\",\n#   TRUE ~ \"#FF0033\"\n# )\nset.seed(1235)\neigen_labels <- sprintf(\"%s\\n%.2f\", ntwk.names, bon_info)\n\nplot(\n  ntwk.ig,\n  # vertex.color = colors,\n  layout = layout_components(ntwk.ig),\n  vertex.label = eigen_labels,\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence nodes by Bonacich centrality\"\n)\n\n\n\n\nThis offers a very different view of the power dynamics in Florence. The Medici are greatly reduced, because their direct connections are all powerful families themselves, and so although they are well-connected by other measures, they may not be in a position of power over those other families. More influential families according to Bonacich would include the Ridolfi and Tornabuoni families, who suddenly become dominant from this perspective. The Bonacich calculation is complex and includes notions of centrality and power, and is not purely a measure of adjacency; see how small the Albizzi’s Bonacich score is, even though one of its three connections is to the pendant Ginori family, which is nonetheless in a slightly stronger position according to Bonacich. A fuller investigation of this dynamic would require some careful research."
  },
  {
    "objectID": "academics/697E/index.html",
    "href": "academics/697E/index.html",
    "title": "697E - Political Networks Analysis",
    "section": "",
    "text": "Date\n\nSpring 2022\n\nProfessor\n\nMeredith Rolfe\n\n\nThis class applied network theory to political and social networks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful R tidbits: Political Networks Analysis\n\n\n\n\n\n\n\nr\n\n\nsocial networks\n\n\n\n\nTasty R snacks that do useful things in Political Networks Analysis\n\n\n\n\n\n\nFeb 8, 2022\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 6: Clustering and Blockmodels in the Florentine Wedding dataset\n\n\n\n\n\n\n\nr\n\n\nsocial networks\n\n\n\n\nYet more ways to look at the Florence data.\n\n\n\n\n\n\nMar 26, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 5: Constraint and Brokerage in the Florentine Wedding dataset\n\n\n\n\n\n\n\nr\n\n\nsocial networks\n\n\n\n\nExamining additional centrality measures in the Florentine network\n\n\n\n\n\n\nFeb 16, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 4: Centrality Analysis of the Florentine Wedding dataset\n\n\n\n\n\n\n\nr\n\n\nsocial networks\n\n\n\n\nExamining centrality measures in the Florentine network\n\n\n\n\n\n\nFeb 10, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 3: The Florentine Wedding dataset\n\n\n\n\n\n\n\nr\n\n\nsocial networks\n\n\n\n\nA first-look network analysis\n\n\n\n\n\n\nFeb 9, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nGlossary\n\n\n\n\n\n\n\nnetworks\n\n\nglossary\n\n\n\n\nGlossary for 697E coursework\n\n\n\n\n\n\nJan 29, 2022\n\n\n10 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "academics/697E/useful-r-tidbits/index.html",
    "href": "academics/697E/useful-r-tidbits/index.html",
    "title": "Useful R tidbits: Political Networks Analysis",
    "section": "",
    "text": "This document contains R snippets pertaining to work in the Political Networks Analysis class. There is also one for the Machine Learning class."
  },
  {
    "objectID": "academics/697E/useful-r-tidbits/index.html#prelude-finding-data-to-play-with",
    "href": "academics/697E/useful-r-tidbits/index.html#prelude-finding-data-to-play-with",
    "title": "Useful R tidbits: Political Networks Analysis",
    "section": "Prelude: finding data to play with",
    "text": "Prelude: finding data to play with\nSince in the class tutorials, the network objects are pre-loaded in the web-based runtime environments, we don’t actually see how to load some of the data sets that the exercises are based on, for running outside of this environment.\nThere are gazillions of ways to get data into R, but for the purposes of playing with networks, there are some packages with pre-existing network data that can be easily loaded. Two of these are igraphdata, which contains igraph-formatted network datasets, and ergm, which contains statnet-compatible network datasets.\nThe datasets in each can be listed with the data command:\n> data(package = \"igraphdata\")\n\nData sets in package ‘igraphdata’:\n\nKoenigsberg              Bridges of Koenigsberg from Euler's times\nUKfaculty                Friendship network of a UK university faculty\nUSairports               US airport network, 2010 December\nenron                    Enron Email Network\nfoodwebs                 A collection of food webs\nimmuno                   Immunoglobulin interaction network\nkarate                   Zachary's karate club network\nkite                     Krackhardt's kite\nmacaque                  Visuotactile brain areas and connections\nrfid                     Hospital encounter network data\nyeast                    Yeast protein interaction network\n\n> data(package = \"ergm\")\n\nData sets in package ‘ergm’:\n\ncohab_MixMat (cohab)     Target statistics and model fit to a hypothetical\n                         50,000-node network population with 50,000 nodes\n                         based on egocent\ncohab_PopWts (cohab)     Target statistics and model fit to a hypothetical\n                         50,000-node network population with 50,000 nodes\n                         based on egocent\ncohab_TargetStats (cohab)\n                         Target statistics and model fit to a hypothetical\n                         50,000-node network population with 50,000 nodes\n                         based on egocent\necoli1 (ecoli)           Two versions of an E. Coli network dataset\necoli2 (ecoli)           Two versions of an E. Coli network dataset\nfaux.desert.high         Faux desert High School as a network object\nfaux.dixon.high          Faux dixon High School as a network object\nfaux.magnolia.high       Goodreau's Faux Magnolia High School as a network\n                         object\nfaux.mesa.high           Goodreau's Faux Mesa High School as a network\n                         object\nflobusiness (florentine)\n                         Florentine Family Marriage and Business Ties Data\n                         as a \"network\" object\nflomarriage (florentine)\n                         Florentine Family Marriage and Business Ties Data\n                         as a \"network\" object\ng4                       Goodreau's four node network as a \"network\"\n                         object\nkapferer                 Kapferer's tailor shop data\nkapferer2 (kapferer)     Kapferer's tailor shop data\nmolecule                 Synthetic network with 20 nodes and 28 edges\nsamplike (sampson)       Cumulative network of positive affection within a\n                         monastery as a \"network\" object\nsamplk1 (samplk)         Longitudinal networks of positive affection\n                         within a monastery as a \"network\" object\nsamplk2 (samplk)         Longitudinal networks of positive affection\n                         within a monastery as a \"network\" object\nsamplk3 (samplk)         Longitudinal networks of positive affection\n                         within a monastery as a \"network\" object\nTo load the statnet “florentine” data from the ergm package:\n> data(\"florentine\", package = \"ergm\")\nThis creates the objects flobusiness and flomarriage, both from this dataset, ready to go in statnet. Note that another version of this dataset is found in the network package flo:\ndata(\"flo\", package = \"network\")\nThis loads the very simple data into a matrix, but it is not yet a network object. To convert it to an igraph object:\nlibrary(network)\nlibrary(igraph)\n\ndata(flo)\nntwk.ig <- graph_from_adjacency_matrix(flo)"
  },
  {
    "objectID": "academics/697E/useful-r-tidbits/index.html#week-1-tutorial-cheatsheet",
    "href": "academics/697E/useful-r-tidbits/index.html#week-1-tutorial-cheatsheet",
    "title": "Useful R tidbits: Political Networks Analysis",
    "section": "Week 1 tutorial cheatsheet",
    "text": "Week 1 tutorial cheatsheet\n\n\n\n\nigraph\nstatnet\n\n\n\n\nCount of vertices\nvcount()\n\n\n\nCount of edges\necount()\n\n\n\nBoth\n\nprint()\n\n\nBipartite or single mode?\nis_bipartite()\nprint()\n\n\nEdges directed or undirected?\nis_directed()\nprint()\n\n\nWeighted? (or binary)\nis_weighted()\nprint()\n\n\n\n\nvertex attribute\n\nvector of additional information about nodes in a network\n\nedge attribute\n\nvector of additional information about the edges in a network\n\n\nQ: how to get missing edge count in igraph?\n\n\n\n\n\n\n\n\n\nigraph\nstatnet\n\n\n\n\ndisplay vertex attributes\nvertex_attr_names()\nlist.vertex.attributes()\n\n\ndisplay edge attributes\nedge_attr_names()\nlist.edge.attributes()\n\n\n\nIn igraph, attributes are accessed via $, using the V and E functions, as in:\nV(karate.ig)$name\nE(karate.ig)$weight\nIn statnet, they are accessed via the %v% and %e% mechanisms, as in:\nkarate.stat %v% \"vertex.names\"\nkarate.stat %e% \"weight\"\n\nGetting the basic network descriptors\nFirst look at any network is to examine the network size, type (un/directed, un/weighted, bipartite) and available attributes of vertices and edges.\n\nigraph\nBasic descriptors first:\nvcount(climpref.ig)\n[1] 34\n\necount(climpref.ig)\n[1] 531\n\nis_bipartite(climpref.ig)\n[1] FALSE\n\nis_directed(climpref.ig)\n[1] FALSE\n\nis_weighted(climpref.ig)\n[1] TRUE\nListing vertex and edge attributes:\n> vertex_attr_names(climpref.ig)\n[1] \"name\"                 \"Climate.council\"      \"Klimaallianz\"        \n[4] \"Stiftung.Klimarappen\" \"type3\"                \"type5\"               \n[7] \"dm\"                  \n\n> edge_attr_names(climpref.ig)\n[1] \"weight\"\nAccessing vertex and edge attributes:\n> V(climpref.ig)$name %>% head()\n[1] \"AA\" \"AB\" \"AC\" \"AD\" \"AE\" \"AF\"\n\nE(climpref.ig)$weight %>% head()\n[1] 0.69 0.08 0.04 0.18 0.42 0.21\n\n\nstatnet\nThe basic descriptors in statnet are all shown by print():\n> print(flobusiness)\n Network attributes:\n  vertices = 16 \n  directed = FALSE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 15 \n    missing edges= 0 \n    non-missing edges= 15 \n\n Vertex attribute names: \n    priorates totalties vertex.names wealth \n\nNo edge attributes\nIt appears, though it’s thinly documented, that these attributes are programmatically accessible through the $gal attribute, as in is_directed <- flobusiness$gal$directed:\n> flobusiness$gal\n$n\n[1] 16\n\n$mnext\n[1] 16\n\n$directed\n[1] FALSE\n\n$hyper\n[1] FALSE\n\n$loops\n[1] FALSE\n\n$multiple\n[1] FALSE\n\n$bipartite\n[1] FALSE"
  },
  {
    "objectID": "academics/697E/useful-r-tidbits/index.html#week-2-cheatsheet",
    "href": "academics/697E/useful-r-tidbits/index.html#week-2-cheatsheet",
    "title": "Useful R tidbits: Political Networks Analysis",
    "section": "Week 2 cheatsheet",
    "text": "Week 2 cheatsheet\nA dyad census will count the reciprocal (mut), asymmetric (asym) and absent (null) dyads, based on directed graphs. In igraph:\nigraph::dyad.census(trade2007.ig)\n$mut\n[1] 11444\n\n$asym\n[1] 3148\n\n$null\n[1] 2244\nIn statnet:\nsna::dyad.census()\n [1]   6225  19035  40611   6442   7044  10097  55355  44966   9200   1876\n[11] 146537  25578  17167  30894 225908 374449\nTriad census is similar:\nigraph::triad_census(trade2007.ig)\nsna::triad.census(gotbook.stat, mode=\"graph\")  # undirected\n      \nsna::triad.census(trade2007.stat)              # directed\n      003   012   102 021D 021U 021C  111D  111U 030T 030C   201  120D\n[1,] 6225 13655 26489 4225 4657 4821 34812 23635 4374  537 97088 15073\n     120U  120C    210    300\n[1,] 7947 14249 136169 283855\nNote that the statnet version gives us information about the types of triads as column names in the matrix it returns. The igraph version also breaks them into the 16 categories, but returns them in a fixed order not detailed in its return value (described in its help documentation).\nThe total number of possible triads in a 298 vertex network is (298 x 297 x 296) / (3 x 2 x 1) - the 3 countdown comes from “triad”. Quads would be (298 x 297 x 296 x 295) / (4 x 3 x 2 x 1).\n\nTransitivity\nTransitivity is the percentage of potential connected triads - how many are complete. Basic way in igraph is transitivity(). The statnet version is gtrans(), but it only works in directed networks. print() will say whether the network is directed. (Note: in the tutorial, we see that the climate network IS directed, but it returns a different result than igraph: 0.627 vs 0.724. Not clear why. “it is calculating a transitivity score based on an understanding of network structure rooted in hierarchy”)\nLocal transitivity is the local clustering coefficient - how many nodes of an ego are connected to each other. Have to unpack this, but the magic is:\ntransitivity(gotbook.ig, type=\"local\", \n  vids=V(gotbook.ig)[\n  c(\"Petyr Baelish\",\"Jon Snow\", \"Tyrion Lannister\")]) \nThe global clustering coefficient in igraph is\ntransitivity(trade2007.ig, type=\"global\")\n[1] 0.8837142\nThe local coefficient is:\ntransitivity(trade2007.ig, type=\"average\")\n[1] 0.8862707\nNetwork transitivity in statnet is gtrans():\ngtrans(trade2007.stat)\n[1] 0.9993143\n\n\n\n\n\n\n\n\n\nigraph\nstatnet\n\n\n\n\nglobal clustering coefficient\ntransitivity(trade2007.ig,  type=\"global\")\ngtrans(trade2007.stat) (directed only)\n\n\nlocal clustering coefficient\ntransitivity(trade2007.ig,  type=\"local\")\n???\n\n\naverage local clustering coefficient\ntransitivity(trade2007.ig,  type=\"average\")\n???"
  },
  {
    "objectID": "academics/697E/useful-r-tidbits/index.html#degree",
    "href": "academics/697E/useful-r-tidbits/index.html#degree",
    "title": "Useful R tidbits: Political Networks Analysis",
    "section": "Degree",
    "text": "Degree\nigraph::degree() and statnet::degree(), and once again they give different results; igraph includes loops, statnet doesn’t. Force igraph to ignore them with loops = FALSE. “Note that setting diag=TRUE in sna::degree does not guarantee equivalence as statnet only single counts the loops in a directed network, while igraph double-counts the loops.”\nigraph shows the node names, statnet doesn’t.\nGetting the degree of a particular set of nodes in igraph:\n> igraph::degree(trade2007.ig, v = V(trade2007.ig)[c(\"China\", \"Canada\", \"United Kingdom\", \"Denmark\")])\n         China         Canada United Kingdom        Denmark \n           364            364            364            362 \nThis can also be done by index, as in:\n> igraph::degree(flo_ig, v = 1:3)\nAcciaiuoli    Albizzi  Barbadori \n         2          6          4 \nStatnet degrees aren’t named:\n> sna::degree(flo_stat)\n [1]  2  6  4  6  6  2  8  2 12  2  6  0  6  4  8  6\n> which(sna::degree(flo_stat) == 0)\n[1] 12\n\nIndegree and outdegree calculations\n\n\n\n\n\n\n\n\n\nigraph\nstatnet\n\n\n\n\nindegree\nigraph::degree(climate.ig,mode=\"in\", loops = FALSE)\nsna::degree(climate.stat, cmode=\"indegree\")\n\n\noutdegree\nigraph::degree(climate.ig,mode=\"out\", loops = FALSE)\nsna::degree(climate.stat, cmode=\"outdegree\")\n\n\n\nCode from the tutorial to create data.frames with degree statistics:\n#igraph:\n\ntrade2007.nodes <- data.frame(name = V(trade2007.ig)$name,\n    totdegree = igraph::degree(trade2007.ig, loops = FALSE),\n    indegree = igraph::degree(trade2007.ig, mode = \"in\", loops = FALSE),\n    outdegree = igraph::degree(trade2007.ig, mode = \"out\", loops = FALSE))\n\n#statnet version:\n\ntrade2007.nodes <- data.frame(name = trade2007.stat%v%\"vertex.names\",\n    totdegree = sna::degree(trade2007.stat),\n    indegree = sna::degree(trade2007.stat, cmode = \"indegree\"),\n    outdegree = sna::degree(trade2007.stat, cmode = \"outdegree\"))"
  },
  {
    "objectID": "academics/697E/useful-r-tidbits/index.html#components",
    "href": "academics/697E/useful-r-tidbits/index.html#components",
    "title": "Useful R tidbits: Political Networks Analysis",
    "section": "Components",
    "text": "Components\nShortest path length between 2 nodes: igraph distances() does this.\ndistances(gotbook.ig,\"Petyr Baelish\",\"Robb Stark\")\n# Calculate distance using unweighted edges\ndistances(gotbook.ig,\"Petyr Baelish\",\"Robb Stark\", weights=NA)\n# list shortest paths between 2 nodes\nall_shortest_paths(gotbook.ig,\"Bronn\",\"Varys\", weights=NA)$res\n#find average shortest path for network\naverage.path.length(gotbook.ig,directed=F)\nComponent structure and membership\nNote: a graph is fully connected if its number of components is 1. igraph returns this as the no parameter of igraph::components(); it appears that statnet has no parallel function, but if the number of isolates is 0, the graph is connected (sna::isolates()).\n# What element are returned by components\nnames(igraph::components(gotbook.ig))\n\n# Number of components\nigraph::components(gotbook.ig)$no \n\n# Size of each component\nigraph::components(gotbook.ig)$csize \n\n# retrieve the index of isolate nodes\n# (nodes with component count of 1 from \"components\" above)\nisolates(gotbook.stat)\n\n# There is no direct command in igraph, but we can do this:\n# Create a list of the degree of each node in the network\ndeg_counts <- igraph::degree(gotbook.ig, loops = FALSE)\n\n# filter and count the nodes with 0 degrees (or any other quantity of interest)\nlength(deg_counts[deg_counts == 0])\n\n# subset vertex.names attribute to get names of isolates\nas.vector(gotbook.stat %v% 'vertex.names')[c(isolates(gotbook.stat))] %>%\n  head()\n##   [1] \"Aegon Frey (Jinglebell)\"         \"Alebelly\"                       \n##   [3] \"Alfyn\"                           \"Allar Deem\"                     \n##   [5] \"Antario Jast\"                    \"Balman Byrch\""
  },
  {
    "objectID": "academics/697E/useful-r-tidbits/index.html#graph-density",
    "href": "academics/697E/useful-r-tidbits/index.html#graph-density",
    "title": "Useful R tidbits: Political Networks Analysis",
    "section": "Graph density",
    "text": "Graph density\nNote: network.density() (statnet) ignores edge values “at present”.\n#get network density: igraph\ngraph.density(climate.ig)\n\n## [1] 0.4117647\n\n#get network density: statnet\nnetwork.density(climate.stat)\n\n## [1] 0.399654\nAdding loops = TRUE to graph.density() appears to fix the problem and gets the two packages to agree:\n#get network density without loops: igraph\ngraph.density(climate.ig, loops=TRUE)\n\n## [1] 0.399654\nSO, it’s safest to always do either: - graph.density(climate.ig, loops=TRUE) (igraph), OR - network.density(climate.stat) (statnet)\nIn statnet, we can get network density with loops (nodes connecting to themselves) omitted:\n#get network density without loops: statnet\ngden(climate.stat, diag=FALSE)\n\n## [1] 0.3921569"
  },
  {
    "objectID": "academics/697E/useful-r-tidbits/index.html#network-degree-centralization",
    "href": "academics/697E/useful-r-tidbits/index.html#network-degree-centralization",
    "title": "Useful R tidbits: Political Networks Analysis",
    "section": "Network degree centralization",
    "text": "Network degree centralization\nIn statnet, call centralization() with the degree function and appropriate parameters for degree in the cmode argument:\ncentralization(climate.stat, degree, cmode=\"indegree\")\ncentralization(climate.stat, degree, cmode=\"outdegree\")\ncentralization(climate.stat, degree, cmode=\"freeman\") # default\nCould also call it with other sna functions like betweenness, closeness\nThe igraph version uses centr_degree() and returns an object with several components, of which centralization is one:\ncentr_degree(climate.ig, loops = FALSE, mode = \"in\")$centralization\ncentr_degree(climate.ig, loops = FALSE, mode = \"out\")$centralization\n\nEigenvector centralization\nstatnet uses evcent() to calculate the eigenvalue centrality score for each node in the network:\nevcent(imf2014.stat, ignore.eval=TRUE))\nEigenvector centrality index for the network:\ncentralization(imf2014.stat, evcent)\nIn igraph, a set of eigenvector-related information is created with centr_eigen():\n# If the network is directed, specify \"directed - T\" - will not auto-detect\neigen_info <- centr_eigen(imf2014.ig, directed = T)\n# Centrality score for node 3:\neigen_info[3]$vector\n# Eigenveector centrality index for the network:\neigen_info$centralization\nThe scores calculated by igraph and statnet are different. We aren’t sure why. It appears that igraph counts incoming ties to calculate eigenvector centrality, and statnet recommends using Bonachic power instead for directed networks.\n\n\nBonacich Power Centrality\nigraph:\npower_centrality(imf2014.ig)\nstatnet:\nbonpow(imf2014.stat)\nAgain, there appear to be some inconsistency between igraph and statnet in the calculations, with statnet apparently not incorporating weights and failing on singular matrices.\n\n\nDerived and Reflected Centrality\nThere are no library routines for these calculations. Convert the data to a matrix first:\nmat2014 <- as.matrix(as_adjacency_matrix(imf2014.ig, attr=\"weight\"))\n\nTo calculate the proportion of centrality that is received, we first square the adjacency matrix. The diagonal of the adjacency matrix is equal to the the square of node degree. We then divide this diagonal (sqared degree) by total sqaured indegree (calculated by rowSums) to get the proportion of received centrality.\n\n# square the adjacency matrix\nmat2014sq <- t(mat2014) %*% mat2014\n\n# Calculate the proportion of reflected centrality.\nimf2014.nodes$rc <- diag(mat2014sq) / rowSums(mat2014sq)\n\n# freplace missing values with 0\nimf2014.nodes$rc <- ifelse(is.nan(imf2014.nodes$rc), 0, imf2014.nodes$rc)\n\n# Calculate received eigenvalue centrality\nimf2014.nodes$eigen.rc <- imf2014.nodes$eigen * imf2014.nodes$rc\n\nIf total centraltiy is 1, then derived centrality is simply 1 - the proportion of eigenvector centrality due to received centrality.\n\n# Calculate the proportion of derived centrality.\nimf2014.nodes$dc <- 1 - diag(mat2014sq) / rowSums(mat2014sq)\n\n# replace missing values with 0\nimf2014.nodes$dc <- ifelse(is.nan(imf2014.nodes$dc), 1, imf2014.nodes$dc)\n\n# Calculate received eigenvalue centrality\nimf2014.nodes$eigen.dc <- imf2014.nodes$eigen * imf2014.nodes$dc"
  },
  {
    "objectID": "academics/697E/useful-r-tidbits/index.html#week-5-big-block-of-basic-code",
    "href": "academics/697E/useful-r-tidbits/index.html#week-5-big-block-of-basic-code",
    "title": "Useful R tidbits: Political Networks Analysis",
    "section": "Week 5: Big Block of Basic Code",
    "text": "Week 5: Big Block of Basic Code\nBig Blocks of Basic Code to get a bunch of measures of a network:\n# Get the basic stuff we can do all at once with igraph\ncliminfl.nodes <- data.frame(\n    name      = V(climinfl.ig)$name,\n    totdegree = igraph::degree(climinfl.ig, loops=FALSE),\n    indegree  = igraph::degree(climinfl.ig, mode=\"in\", loops=FALSE),\n    outdegree = igraph::degree(climinfl.ig, mode=\"out\", loops=FALSE),\n    eigen     = centr_eigen(climinfl.ig, directed = T)$vector,\n    bonanich  = power_centrality(climinfl.ig),\n    centr_clo = igraph::closeness(climinfl.ig),\n    centr_btw = igraph::betweenness(climinfl.ig, directed = FALSE),\n    # igraph only\n    burt      = constraint(climinfl.ig)\n)\n# Network-level measures:\n#   closeness centralization\ncliminfl.centr_clo = centr_clo(climpref.ig)$centralization\n#   betweenness centralization\ncliminfl.centr_btw = centr_betw(climpref.ig, directed = FALSE)$centralization\n\n# statnet version\ncliminfl.nodes <- data.frame(\n    name      = climinfl.stat %v% \"vertex.names\",\n    totdegree = sna::degree(climinfl.stat),\n    indegree  = sna::degree(climinfl.stat, cmode = \"indegree\"),\n    outdegree = sna::degree(climinfl.stat, cmode = \"outdegree\"),\n    eigen     = sna::evcent(climinfl.stat, ignore.eval = TRUE),\n    bonanich  = sna::bonpow(climinfl.stat),\n    centr_clo = sna::closeness(climinfl.stat, gmode = \"graph\",\n                               cmode = \"suminvundir\"),\n    centr_btw = sna::betweenness(climinfl.stat, gmode = \"graph\")\n)\n# Network-level measures:\n#   closeness centralization\ncliminfl.centr_clo = centralization(climinfl.stat, sna::closeness, mode = \"graph\")\n#   betweenness centralization\ncliminfl.centr_btw = centralization(climinfl.stat, sna::betweenness, mode = \"graph\")\n\n# Statnet-only: Gould-Fernandez Brokerage\n# replace ATTR with the vector of the desired attribute, such as\n# `climinfl.nodes$orgtype5`\ntemp <- data.frame(brokerage(climinfl.stat, cl = ATTR)$z.nli)\ncliminfl.nodes <- climinfl.nodes %>%\n  mutate(broker.tot = temp$t,\n         broker.coord = temp$w_I,\n         broker.itin = temp$w_O,\n         broker.rep = temp$b_IO,\n         broker.gate = temp$b_OI,\n         broker.lia = temp$b_O)\n\n# Calculated measures not specific to igraph or statnet:\n# Build the derived and reflected centrality (dc/rc) measures\n# \"To calculate the proportion of centrality that is received, we first\n# square the adjacency matrix. The diagonal of the adjacency matrix is\n# equal to the the square of node degree. We then divide this diagonal\n# (squared degree) by total squared indegree (calculated by rowSums) to get\n# the proportion of received centrality.\"\n\nmat_climinfl <- as.matrix(as_adjacency_matrix(climinfl.ig))  # not \" attr='weight'\"\nmat_climinfl_sq <- t(mat_climinfl) %*% mat_climinfl\n# alternately:\nmat_climinfl <- as.matrix.network(climinfl.stat, attr = \"weight\")\ndiag(mat_climinfl) <- 0\nmat_climinfl_sq <- mat_climinfl %*% mat_climinfl\n\ncliminfl.nodes$rc <- diag(mat_climinfl_sq) / rowSums(mat_climinfl_sq)\n# replace missing values with 0\ncliminfl.nodes$rc <- ifelse(is.nan(climinfl.nodes$rc), 0, climinfl.nodes$rc)\ncliminfl.nodes$dc <- 1 - climinfl.nodes$rc\n\n# Build the derived and reflected eigenvector measures\ncliminfl.nodes$eigen.rc <- climinfl.nodes$eigen * climinfl.nodes$rc\ncliminfl.nodes$eigen.dc <- climinfl.nodes$eigen * climinfl.nodes$dc\n\nCloseness centrailty\nFrom text: “The closeness centrality of a node is defined as the sum of the geodesic distances between that node and all other nodes in a network.”\nBoth are called in the above blocks, with sna::closeness() or igraph::closeness() on the respective network object.\nFrom igraph::closeness help: “Closeness centrality measures how many steps is required to access every other vertex from a given vertex.”\nigraph and statnet have very different implementations, with options that have to be carefully set.\n\nigraph\nigraph uses inverse closeness.\nFor directed networks, use mode=(\"in\", \"out\", \"all\", \"total\"), describing the path type; in is paths to a vertex, out is paths from a vertex. Undirected networks ignore this parameter. It will use the “weight” edge attribute automatically if it’s there, or can be overriden with something else.\n\n\nstatnet\nMust specify gmode (type of graph) as graph (undirected) or digraph (directed, default). cmode (type of closeness centrality being measured) is one of: directed, undirected (both standard closeness), suminvdir (directed case) and suminvundir (undirected case), and gil-schmidt for that. The suminv options correspond to igraph’s default inversion, though they’re still calculated slightly differently, so they’re generally preferred.\nstatnet ignores the edge weights by default; ignore.eval = FALSE to use them, according to the documentation, but the results appear not to use them.\nsna::closeness(climpref.stat, gmode=\"graph\", cmode=\"suminvundir\", ignore.eval=FALSE))\n\n\n\nCloseness centralization\nCloseness centralization is the network-level measure of the closeness centrality node-level measure.\ncliminfl.centr_clo = centr_clo(climpref.ig)$centralization\ncliminfl.centr_clo = centralization(climinfl.stat, FUN = \"closeness\", mode = \"graph\")\n\n\nBetweenness centrality\nBetweenness centrality is the node-level measure of the number of geodesics (shortest path between two nodes) on which a node sits. A high betweenness centrality measure means a node is on many shortest-paths, suggesting a measure of influence or power.\nigraph::betweenness(climpref.ig, directed = FALSE)\nsna::betweenness(climpref.ig, gmode = \"graph\")\nThe igraph version directed argument, for whether or not direction should be considered, defaults to TRUE; it might be wondered why it doesn’t read the directedness of the graph as a default, but oh well.\nThe statnet version would use gmode of digraph for a directed network, and cmode for a variant undirected form; see ?sna::betweenness for more. Statnet appears to use weights by default; weights = NA disables weights in igraph.\n\n\nBetweenness centralization\nThe network-level measure of betweenness centralization represents Freeman centralization, at least according to the statnet documentation.\nThis is a “a measure of how central its most central node is in relation to how central all the other nodes are”.\ncliminfl.centr_btw = centr_betw(climpref.ig, directed = FALSE)$centralization\ncliminfl.centr_btw = centralization(climpref.stat, FUN = \"betweenness\", mode = \"graph\")\n\n\n(Burt’s) network constraint\nigraph-only function measuring a node’s connection redundancy from 0 (none) to 1. Uses weights.\nconstraint(climinfl.ig)\n\n\nGould-Fernandez Brokerage\n(Statnet-only)\nFrom ?brokerage(): “Gould and Fernandez (following Marsden and others) describe brokerage as the role played by a social actor who mediates contact between two alters.”\nRequires a directed network with a vertex attribute used for grouping. Returns a structure with a lot of information; tutorial refers mainly to znli containing the following roles:\n\n\n\n\n\n\n\n\n\nprefix\nRole\nAction\nPath\n\n\n\n\nw_I\nCoordinator\nmediates contact between two individuals from his or her own group.\nA -> A -> A\n\n\nw_O\nItinerant broker\nmediates contact between two individuals from a single group to which he or she does not belong.\nA -> B -> A\n\n\nb_{OI}\nGatekeeper\nmediates an incoming contact from an out-group member to an in-group member.\nA -> B -> B\n\n\nb_{IO}\nRepresentative\nmediates an outgoing contact from an in-group member to an out-group member.\nA -> A -> B\n\n\nb_O\nLiaison\nmediates contact between two individuals from different groups, neither of which is the group to which he or she belongs.\nA -> B -> C\n\n\nt\nTotal\nTotal (cumulative) brokerage role occupancy\n(Any two)\n\n\n\nbrokerage(climinfl.stat, cl = climinfl.nodes$orgtype5)"
  },
  {
    "objectID": "academics/697E/useful-r-tidbits/index.html#week-6",
    "href": "academics/697E/useful-r-tidbits/index.html#week-6",
    "title": "Useful R tidbits: Political Networks Analysis",
    "section": "Week 6",
    "text": "Week 6\n(All statnet unless otherwise specified)\nCalculating structural equivalence (“SE clusters”): look for nodes that have the same pattern of ties with the same neighbors, like siblings of a parent. Unexplained distance functions include “hamming”, “correlation”, “gamma”. The sedist() function ignores edge values (weights?).\nflomarr.se <- equiv.clust(flomarr.stat, equiv.fun = \"sedist\", method = \"hamming\", mode = \"graph\")\nThe output of equiv.clust() can be plotted as a “Cluster Dendogram”:\nflomarr.se <- equiv.clust(flomarriage, equiv.fun = \"sedist\", method = \"hamming\", mode = \"graph\")\nplot(flomarr.se,labels = flomarr.se$glabels)\nAnnoyingly, the different cluster.methods “single, average, or ward.D” are not explained in the tutorial. The default is “complete”.\nrect.hclust() (visually) “cuts” the diagram at a given height, making separate clusters:\nplot(flomarr.se, labels = flomarr.se$glabels)\nrect.hclust(flomarr.se$cluster, h = 9)\nflomarr.se <- equiv.clust(flomarriage, equiv.fun = \"sedist\", cluster.method = \"single\",\n                          method = \"hamming\", mode = \"graph\")\nplot(flomarr.se,labels = flomarr.se$glabels)"
  },
  {
    "objectID": "academics/697E/glossary/index.html",
    "href": "academics/697E/glossary/index.html",
    "title": "Glossary",
    "section": "",
    "text": "A social network is a set of actors (or points, or nodes, or agents) that may have relationships (or edges, or ties) with one another. Networks can have few or many actors, and one or more kinds of relations between pairs of actors.\n– (Hanneman, Robert A. and Riddle, Mark 2005, chap. 2)"
  },
  {
    "objectID": "academics/697E/glossary/index.html#week-1---introduction",
    "href": "academics/697E/glossary/index.html#week-1---introduction",
    "title": "Glossary",
    "section": "Week 1 - Introduction",
    "text": "Week 1 - Introduction\n\nClass lecture 1a: Nodes, Edges and Network Samples\n(Note: some terms overlap somewhat or have context-dependent synonyms.)\n\nnode / vertex\n\na junction in a network where two or more lines (edges) intersect; a dot connecting lines.\n\ntie / edge / link / relation\n\nlines connecting nodes, which indicate some sort of connection or relationship.\n\nnode / ego / actor\n\n\\(i\\) - the node being discussed or focused on; also note “Actors are described by their relations, not by their attributes.”1\n\nalter\n\n\\(j\\) - the node that \\(i\\) connects to\n\nnetwork population\n\n\\(n\\) - the size of the population or count of nodes\n\n\nWith set notation, we define \\(i\\) as a set of \\(n\\) elements: \\[i \\in 1, 2, 3 \\dots n\\]\nand \\(j\\) similarly as a set of \\(n\\) elements, except that \\(j\\) cannot equal \\(i\\) (a node cannot connect to itself): \\[j \\in 1, 2, 3 \\dots n, i \\ne j\\]\n\ninteraction threshold\n\na measure to determine whether two entities have a sufficient connection to be considered to have a link between them\n\nsnowball sample\n\nan entity group formed by starting with “a focal actor or set of actors”2 and “rolling outwards” to its connections until all nodes or actors (in a limited set) are located, or a decision to stop is made\n\negocentric name generator\n\na mode of building an entity group (an egocentric network) defined by connection to a single central node; like a snowball sample that doesn’t expand past the first set of connections\n\n\n\n\nClass lecture 1b: Ties and Adjacency Matrices\n\nadjacency matrix\n\nan \\(n \\times n\\) matrix depicting connections between \\(n\\) nodes as 0 or 1, where 1 is a connection (or vertex/edge) and 0 is the absence of one:\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\nA\n-\n0\n1\n0\n\n\nB\n0\n-\n1\n0\n\n\nC\n1\n1\n-\n0\n\n\nD\n0\n0\n0\n-\n\n\n\n\ndirected tie\n\na relationship where \\(W_{i,j} \\ne W_{j,i}\\), as might be used to represent a transfer of resources from one node to another; graphed with arrows\n\nsymmetric tie\n\na relationship where \\(W_{i,j} = W_{j,i}\\), with no direction; graphed with lines\n\nbinary tie\n\na tie where \\(W_{i,j}\\) is 0 or 1, as in the example above, indicating the absence or presence of a tie (also dichotomous)\n\nvalued tie\n\na tie where \\(W_{i,j}\\) is a value \\(v\\), indicating a weight or magnitude of the connection; may be graphed with line attributes such as weight, color, etc\n\n\nTies are symmetric or directed, and binary or valued.\n\n\nClass lecture 1c: Edgelists\n\nedgelist\n\na table indicating edges in a network, with at least “from” and “to” columns, and possibly additional columns for attributes or values\n\n\n\n\n\nFrom\nTo\nValue\n\n\n\n\nA\nB\n5\n\n\nA\nE\n2\n\n\nB\nA\n-1\n\n\nB\nC\n4\n\n\nB\nD\n2\n\n\nC\nA\n-4\n\n\n\nEdgelists are more efficient in sparse networks, as they only list actual connections rather than being a matrix of all possible connections\n\n\nHanneman, Robert A. and Riddle, Mark (2005), chapter 1 (Social network data):\n\nbinary measures of relations\n\nundirected relations, 0 or 1 for the absence or presence of a connection\n\nmultiple-category nominal measures of relations\n\ndirected relations with categories (like “friend, lover, business relationship, kin, or no relationship”)\n\ngrouped ordinal measures of relations\n\nmeasures that reflect a level of intensity or degree; often turned into binary measures by means of a threshold or cut-off\n\nfull-rank ordinal measures of relations\n\nan ordering from 1 to \\(n\\) of an actor’s relations (uncommon in social networks)\n\ninterval measures of relations\n\ncontinuous measures that express the strength of connections by comparison with others, to be able to say “this tie is twice as strong as that tie”; the “most advanced” level of measurement\n\n\n\n\nLazer (2011)\n\nhomophily\n\nthe idea that individuals who are similar to one another are more likely to form ties\n\nwhole network data\n\nrelational information about a whole set (or subset) of data, with all of the actors’ relations to each other considered\n\negocentric data\n\nrelational information about a set of nodes connected to one particular node and not to each other\n\ndiameter\n\nthe maximum degree of separation between any two nodes in the network\n\none-mode data\n\nties among one set (or category) of agent, such as nations in the context of trade\n\ntwo-mode data / bipartite data\n\nties between different sets (or categories) of agents, such as ties between nations and international organizations; a network split into two parts, each of whose nodes only connect to nodes in the opposite part, not to nodes within its own part; “affiliation network” in Wasserman and Faust (1994)\n\n\n\n\nBorgatti et al. (2009)\n\nsociometry\n\n“a technique for eliciting and graphically representing individuals’ subjective feelings toward one another”\n\nstrength of weak ties (SWT) theory\n\nthe theory that one is likelier to hear new information from people they aren’t closely connected to in a network (c.f. homiphily)\n\ncentrality\n\na family of positional properties of nodes in a network\n\nFreeman’s betweenness\n\na type of centrality where a node is frequently along the shortest path between pairs of nodes, giving control over flow or power\n\nopportunity-based antecedents\n\n“the likelihood that two nodes will come into contact” - when considering the formation of ties\n\nbenefit-based antecedents\n\n“some kind of utility maximization or discomfort minimization that leads to tie formation”\n\nnode homogeneity\n\na category of node outcomes referring to the similarity of nodes\n\nperformance\n\na category of node outcomes referring to some good (such as strong performance)"
  },
  {
    "objectID": "academics/697E/glossary/index.html#week-2---network-structure",
    "href": "academics/697E/glossary/index.html#week-2---network-structure",
    "title": "Glossary",
    "section": "Week 2 - Network Structure",
    "text": "Week 2 - Network Structure\n\nClass lecture 2a: Network Structure - Walks, Paths, and Distance\nConnections between nodes:\n\nadjacent\n\na direct connection between nodes; not necessarily bilateral in directed networks. A leading to B (“A adj B”) does not mean that B leads to A (“B adj A)\n\nreachable\n\nwhether a node is reachable from another node, regardless of distance\n\ndistance\n\nthe number of ties (steps, edges) that must be traversed to reach a target node\n\nwalk\n\nsequence (not path, see below) that connects two nodes, consisting of the nodes and edges\n\ntrail\n\na walk that can only go through each edge / tie once, but can hit the same node more than once\n\npath\n\na trail that only hits each node and edge once; the start and end node may be the same\n\ngeodesic distance\n\nthe shortest path between two nodes; by definition, not a trail or walk because repeated segments wouldn’t be the shortest; with binary data, the number of edges between the nodes; with weighted data, might be a sum or some other calculation of “effort”\n\n\n\n\nClass lecture 2b: Graph Substructures and Components\nNetwork substructures\n\ndyad, triad, clique\n\ntwo, three, or four-or-more connected nodes\n\ncomplete graph\n\na network where every node is directly connected to every other node\n\nconnected graph\n\na network where every node is indirectly connected to every other node\n\nunconnected graph\n\na network where at least one node is unreachable\n\ncomponent\n\nthe set of all points that constitutes a connected subgraph within a network\n\nmain component\n\nthe largest component within a network\n\nminor component\n\na smaller one, possibly one of many\n\npendant\n\na node with only one link or edge to a network, “dangling”\n\nisolate\n\nan unconnected node\n\n\n\n\nClass lecture 2c: Dyad and Triad Census\n\nmutual dyad\n\na dyad where both nodes connect to each other, as in an undirected network\n\nasymmetric dyad\n\na dyad (in a directed network) where one node connects to another, but non-reciprocally (one way only)\n\nnull\n\na dyad of two unconnected nodes\n\n(empty / one edge / two path / triangle) triad\n\na triad with zero, one, two or three edges between three nodes (all four possible permutations)\n\nbalance theory\n\nthe theory that two nodes connected to a common node will also develop connections to each other\n\nglobal transitivity index\n\nthe proportion of triads in a network that are complete (with 3 connections between them)\n\n\nThere is a vocabulary for triads in directed networks describing the 16 possible permutations of ties (or the absence thereof) among 3 nodes, counting the number of mutual, asymmetric and null dyads, with direction indicators, like 003 or 120D - see the slide at 3:45\n\nvacuously transitive triad\n\na triad where (to be continued…) (5 of 16 possibilities)\n\nintransitive triad\n\na triad where (to be continued…) (7 of 16 possibilities)\n\ntransitive triad\n\na triad where (to be continued…) (4 of 16 possibilities)\n\n\n(See Alhazmi, Gokhale, and Doran (2015))\n\n\nClass lecture 2d: Transitivity and Clustering Coefficient\n\nlocal transitivity / local clustering coefficient\n\nthe likelihood that the neighbors of a node are also connected to each other; the number of connections that do exist over the number of connections that could exist\n\n\n\nIn the example above, there are 8 nodes that Homer (center) could connect to; among those 8 nodes, there are \\(7 + 6 + 5 + 4 + 3 + 2 + 1 = 28\\) possible undirected connections (not connecting to Homer), and 9 of those 28 do exist, for a local clustering coefficient of \\(9/28 \\approx 0.32\\).\n\naverage clustering coefficient\n\nthe average of the local clustering coefficient of all nodes in the network"
  },
  {
    "objectID": "academics/697E/glossary/index.html#week-3---network-degree",
    "href": "academics/697E/glossary/index.html#week-3---network-degree",
    "title": "Glossary",
    "section": "Week 3 - Network Degree",
    "text": "Week 3 - Network Degree\n\nClass lecture 3a: Degree\n\n(vertex) degree\n\nthe number of links that a node has; the number of nodes it’s connected to\n\ndegree distribution\n\na distribution showing the number of nodes of a network that have each degree level\n\nindegree\n\nthe number of links that a node receives in a directed network\n\noutdegree\n\nthe number of links that a node sends in a directed network\n\n\n\n\nClass lecture 3b: Centrality vs. Centralization\n\ncentrality\n\na measure of the prominence of one node relative to others; can be variously defined\n\n(degree) centrality\n\nproportional the the number of other nodes to which a node is linked\n\ncentralization\n\na property of a graph or network, referring to its overall cohesion; comparing most central point to all other points; ratio of the actual sum of differences to the maximum possible sum of differences\n\n\n\n\nClass lecture 3c: Network Density\n\nnetwork density\n\nnumber of ties as a proportion of the maximum possible number of ties; varies from 0 to 1, calculation will vary by whether network is undirected or directed (twice as many potential connections)"
  },
  {
    "objectID": "academics/697E/glossary/index.html#week-4---status-eigenvectors",
    "href": "academics/697E/glossary/index.html#week-4---status-eigenvectors",
    "title": "Glossary",
    "section": "Week 4 - Status & Eigenvectors",
    "text": "Week 4 - Status & Eigenvectors\n\nClass lecture 4a: Status and Hierarchy\n\ncloseness centrality\n\nthe sum of geodesic distances (shortest paths) to all other points in the graph. Divide by \\(n-1\\), then invert. A measure of how close a node is to all of the other nodes in the network.\n\n\n\n\nClass lecture 4b: Status and Prestige\n\nprestige\n\nsignal of the quality of a node, of a node’s visibility within the network\n\neigenvector centrality\n\n“takes into account the centrality of other nodes to which a node is connected. That is, being connected to other central nodes increases centrality.” Takes into account the centrality of the nodes a node is connected to, considering the importance of the connected nodes and not just their quantity or path length. A node with high eigenvector centrality is connected to significant numbers of other highly central nodes. \\[\\lambda \\text{c} = \\text{Wc}\\]\n\nBonacich Power\n\n“A closely related concept, but includes a weighting factor that emphasizes global vs. local structure (and negative connections)” In contrast to eigenvector centrality, penalizes nodes connected to other well-connected nodes, under the theory that being connected to strong or powerful nodes means a node has less influence and power than it would if it were connected to fewer or weaker nodes. \\[\\text{c}(\\alpha, \\beta) = \\alpha(\\text{I} - \\beta\\text{W})^{-1}\\text{W1}\\]\n\n\n\n\nClass lecture 4c: Hubs and Bridges\n\nbridge\n\na node with few ties to central actors\n\nhub\n\na node with many ties to peripheral actors\n\nreflected centrality\n\nthe degree of centrality that a node receives from a connected node that is due to that node’s centrality. If A has influential friend B, then A’s eigenvector centrality will be higher; reflected centrality is the portion of A’s centrality that comes from B’s centrality.\n\nderived centrality\n\nthe remainder of the eigenvector centrality that A receives from B in the example above, that is not due to B’s centrality but just from B being a connected node\n\n\nReflected and derived centrality can be represented in the following matrix by Mizruchi et al:\n\n\n\n\nHigh reflected centrality\nLow reflected centrality\n\n\n\n\nHigh derived centrality\nCosmopolitans\nPure bridges\n\n\nLow derived centrality\nPure hubs\nPeripherals\n\n\n\nThe four prototypes are:\n\ncosmopolitans\n\nwell-connected nodes that are connected to other well-connected nodes; the “cool kids” table\n\npure hubs\n\nwell-connected nodes that are connected to nodes that are not well-connected; the cool kid who talks to uncool kids\n\npure bridge\n\na node connected to few but high-centrality nodes; a friend of the cool kid with few other friends\n\nperipheral\n\na node with few and low-centrality connections; me and my D&D nerd friends in high school\n\n\n\n\nClass lecture 4d: Status and Core/Periphery Networks\n\nbetweenness centrality\n\na count of the number of shortest paths between nodes that pass through another node"
  },
  {
    "objectID": "academics/697E/697E-hw6/index.html",
    "href": "academics/697E/697E-hw6/index.html",
    "title": "Homework 6: Clustering and Blockmodels in the Florentine Wedding dataset",
    "section": "",
    "text": "This week we will look at structural equivalence and blockmodels in the Florentine wedding data set. Structural equivalence is a way to look at subsections of a network that identifies nodes with similar connections to other nodes, such as parents of the same child, or children of the same parent.\nWe begin, as usual, by loading the Florence data in both the igraph and statnet packages.\n\n# Load the statnet version\ndata(\"florentine\", package = \"ergm\")\nflo_stat <- flomarriage\n\n# Load the igraph version\ndata(flo)\n# If the matrix is symmetric, it is undirected.\n# https://mathworld.wolfram.com/UndirectedGraph.html\n# We know the Florence marriage network is undirected, but code is always better.\nflo_ig <- graph_from_adjacency_matrix(flo, mode = ifelse(isSymmetric(flo), \"undirected\", \"directed\"))\n\nThe structural equivalence object is created with the default parameters, except for forcing the undirected mode. Printing the resulting object shows its creation parameters.\n\nflo_stat.se <- sna::equiv.clust(flo_stat, mode = \"graph\")\nprint(flo_stat.se)\n\nPosition Clustering:\n\n    Equivalence function: sedist \n    Equivalence metric: hamming \n    Cluster method: complete \n    Graph order: 16 \n\n\nThe first thing to do is get a visual look at the default clustering dendrogram.\n\nplot(flo_stat.se, labels = flo_stat.se$glabels)\n\n\n\n\nWhat does this show us? Well, as with nearly every other observation we’ve looked at in this data set, one of the most obvious observations is how the Medici are once again separate from the other nodes (families) in the set. The fact that they are alone in the top left shows that they are dissimilar to all other nodes in the network.\nIf we look at the lower left corner of the graph, cam see that the Peruzzi and Strozzi families are structurally similar. Let’s look again at the basic network diagram, highlighting these families:\n\nset.seed(1235)\n\noranges <- colorRampPalette(c(\"gold\", \"skyblue\"))\ncol <- oranges(5)\n\nvcol <- rep(\"gray80\", vcount(flo_ig))\nvcol[which(V(flo_ig)$name == \"Peruzzi\")] <- col[1]\nvcol[which(V(flo_ig)$name == \"Strozzi\")] <- col[1]\n\nplot(\n  flo_ig,\n  layout = layout_components(flo_ig),\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.color = vcol,\n  vertex.label.font = 2,\n  main = \"Florence interfamily marriages, 1282-1500\"\n)\n\n\n\n\nThey are not identical - there are no structurally identical nodes in this network - but they are similar. They both connect to the Bischeri and Castellani families.1 The Strozzi also connect to the Ridolfi. This difference is represented by the height of the connector lines in the cluster dendrogram above.\nThree other pairs of nodes show the same level of similarity:\n\nset.seed(1235)\n\nvcol <- rep(\"gray80\", vcount(flo_ig))\nvcol[which(V(flo_ig)$name == \"Peruzzi\")] <- col[1]\nvcol[which(V(flo_ig)$name == \"Strozzi\")] <- col[1]\nvcol[which(V(flo_ig)$name == \"Acciaiuoli\")] <- col[2]\nvcol[which(V(flo_ig)$name == \"Barbadori\")] <- col[2]\nvcol[which(V(flo_ig)$name == \"Ginori\")] <- col[3]\nvcol[which(V(flo_ig)$name == \"Pucci\")] <- col[3]\nvcol[which(V(flo_ig)$name == \"Pazzi\")] <- col[4]\nvcol[which(V(flo_ig)$name == \"Salviati\")] <- col[4]\n\nplot(\n  flo_ig,\n  layout = layout_components(flo_ig),\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.color = vcol,\n  vertex.label.font = 2,\n  main = \"Florence interfamily marriages, 1282-1500\"\n)\n\n\n\n\nThis seems believable, overall; the least intuitive impression on this graph is that the Pucci family, which is the network’s lone isolate, is similar to the Ginori family on the other side of the graph, which connects only to the Albizzi family; nodes with similar structural equivalence are usually direct neighbors, since they are structurally equivalent due to their shared connections to other nodes.\nWhere this begins to be a bit less convincing is in the triad of the Tornabuoni, Albizzi and Lamberteschi families:\n\nset.seed(1235)\n\nvcol <- rep(\"gray80\", vcount(flo_ig))\nvcol[which(V(flo_ig)$name == \"Tornabuoni\")] <- col[5]\nvcol[which(V(flo_ig)$name == \"Albizzi\")] <- col[5]\nvcol[which(V(flo_ig)$name == \"Lamberteschi\")] <- col[5]\n\nplot(\n  flo_ig,\n  layout = layout_components(flo_ig),\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.color = vcol,\n  vertex.label.font = 2,\n  main = \"Florence interfamily marriages, 1282-1500\"\n)\n\n\n\n\nThe Tornabuoni and Albizzi seem reasonably similar, sharing 2 other connections with each other, but the Lamberteschi share only one with either of them (the Guadagni). It seems like a bit of a stretch to suggest that they are of the same level of similarity to each other, but it could be argued that the Lamberteschi are more similar to the Tornabuoni and Albizzi than to any other nodes in the network.\nFinally, it’s instructive to look at the three nodes with no structurally similar peers: the Medici, Guadagni and Ridolfi:\n\nset.seed(1235)\n\nvcol <- rep(\"gray80\", vcount(flo_ig))\nvcol[which(V(flo_ig)$name == \"Medici\")] <- col[1]\nvcol[which(V(flo_ig)$name == \"Guadagni\")] <- col[3]\nvcol[which(V(flo_ig)$name == \"Ridolfi\")] <- col[5]\n\nplot(\n  flo_ig,\n  layout = layout_components(flo_ig),\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.color = vcol,\n  vertex.label.font = 2,\n  main = \"Florence interfamily marriages, 1282-1500\"\n)\n\n\n\n\nThese nodes are plainly quite different from any other nodes in the network, in terms of their connections.\nIf we wanted another way to describe the diagram - already quite complex despite the data set’s relatively small size - one way we might seek to do that is by breaking the diagram into a manageable number of sub-clusters that are similar to each other. And one way to achieve that is to use rectangular clustering based on height, describing the similarity threshold we feel is appropriate. For example, if we set the height to 8, we get:\n\nplot(flo_stat.se, labels = flo_stat.se$glabels)\nrect.hclust(flo_stat.se$cluster, h = 8)\n\n\n\n\nThis results in four clusters containing data (and two empty clusters). If we plot the results, we can see the clusters whose components the algorithm considers more similar to each other than to other nodes:\n\nvcol <- rep(\"gray80\", vcount(flo_ig))\nvcol[which(V(flo_ig)$name == \"Peruzzi\")] <- col[1]\nvcol[which(V(flo_ig)$name == \"Strozzi\")] <- col[1]\nvcol[which(V(flo_ig)$name == \"Bischeri\")] <- col[1]\nvcol[which(V(flo_ig)$name == \"Castellani\")] <- col[1]\nvcol[which(V(flo_ig)$name == \"Ridolfi\")] <- col[2]\nvcol[which(V(flo_ig)$name == \"Acciaiuoli\")] <- col[3]\nvcol[which(V(flo_ig)$name == \"Barbadori\")] <- col[3]\nvcol[which(V(flo_ig)$name == \"Albizzi\")] <- col[3]\nvcol[which(V(flo_ig)$name == \"Tornabuoni\")] <- col[3]\nvcol[which(V(flo_ig)$name == \"Lamberteschi\")] <- col[3]\nvcol[which(V(flo_ig)$name == \"Ginori\")] <- col[4]\nvcol[which(V(flo_ig)$name == \"Pucci\")] <- col[4]\nvcol[which(V(flo_ig)$name == \"Pazzi\")] <- col[4]\nvcol[which(V(flo_ig)$name == \"Salviati\")] <- col[4]\n\nplot(\n  flo_ig,\n  layout = layout_components(flo_ig),\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.color = vcol,\n  vertex.label.font = 2,\n  main = \"Florence interfamily marriages, 1282-1500\"\n)\n\n\n\n\nOn the whole, this appears to be a pretty reasonable partitioning of the network into clusters of structurally similar nodes. As we observed earlier, the Medici and Guadagni families are distinct enough not to have any structurally equivalent peers in the network, and the Ridolfi, while itself also lacking structurally similar peers, is still more similar to the other clusters than it is to the Medici or Guadagni.\n\nBlockmodel partitions\nFinally, a brief look at blockmodel partitions, which are another way to visualize the clusters described above. Keeping the same height parameter of 8 from the above diagram, we can generate a blockmodel partition from the data:\n\nflo_stat.bm <- blockmodel(flo_stat, flo_stat.se, h = 8, mode = \"graph\")\nflo_stat.bm\n\n\nNetwork Blockmodel:\n\nBlock membership:\n\n  Acciaiuoli      Albizzi    Barbadori     Bischeri   Castellani       Ginori \n           1            1            1            2            2            1 \n    Guadagni Lamberteschi       Medici        Pazzi      Peruzzi        Pucci \n           3            1            4            1            2            1 \n     Ridolfi     Salviati      Strozzi   Tornabuoni \n           1            1            2            1 \n\nReduced form blockmodel:\n\n     Acciaiuoli Albizzi Barbadori Bischeri Castellani Ginori Guadagni Lamberteschi Medici Pazzi Peruzzi Pucci Ridolfi Salviati Strozzi Tornabuoni \n           Block 1   Block 2 Block 3 Block 4\nBlock 1 0.06666667 0.0500000    0.30     0.6\nBlock 2 0.05000000 0.8333333    0.25     0.0\nBlock 3 0.30000000 0.2500000     NaN     0.0\nBlock 4 0.60000000 0.0000000    0.00     NaN\n\n\n\n# Code from the tutorial\nplot.block<-function(x=blk_mod, main=NULL, cex.lab=1){\n  plot.sociomatrix(x$blocked.data, labels=list(x$plabels,x$plabels),\n                   main=main, drawlines = FALSE, cex.lab=cex.lab)\n  for (j in 2:length(x$plabels)) if (x$block.membership[j] !=\n                                     x$block.membership[j-1]) \n    abline(v = j - 0.5, h = j - 0.5, lty = 3, xpd=FALSE)\n}\n\nplot.block(flo_stat.bm, cex.lab=.5)\n\n\n\n\nTHe four clusters are indicated by the dotted horizontal lines in the diagram; each black box is a connection between the nodes along the left and top axes. The more filled-in a partition is, the more similar the clustered elements are to each other; this is also indicated numerically in the chart above with measures closer to 1.\nThe most obvious observations to make here are:\n\nThe cluster with the highest inner similarity rating is 2, with a numeric rate of 0.83 (on a scale of 0 to 1). This can be interpreted to mean that the Peruzzi, Strozzi, Bischeri and Castallani familes mostly intermarried among each other. (Cousins!)\nThe grouping returned by rect.hclust with the same h parameter of 8 does not match the grouping returned by blockmodel with the same value. It gets the first three groups correct, but all of the remaining nodes are packed together into the fourth group. I suspect this may be a bug in the blockmodel code, related to the fact that there were 4 clusters in the rect.hclust code that had data, and 2 without, but blockmodel considered the empty ones as valid, and then stopped at 4. This will need to be investigated.\n\n\n\n\n\n\nFootnotes\n\n\n(Note that their connections to each other are not part of this calculation; it is only assessing nodes they have in common with each other.↩︎"
  },
  {
    "objectID": "posts/2022-01-29_distill-and-github-pages-setup/index.html",
    "href": "posts/2022-01-29_distill-and-github-pages-setup/index.html",
    "title": "Distill and Github Pages Setup",
    "section": "",
    "text": "Note\n\n\n\nNOTE: Since this article was written, we have migrated from Distill to Quarto. We’ll leave this here in case it’s still helpful to anyone, but it’s not how the site is currently built.\nThis document contains a walkthrough of setting up a new Distill blog using RStudio and GitHub pages. It’s the first in a series of articles for social scientists showing how to create, publish and update a website using these packages. This is a relatively easy way to set up a lightweight website that’s well-suited for blogging and writing articles, especially about and using R. Once it’s built, you maintain it using RStudio just like your other projects.\nAlthough it’s intended to be complete, and not overly technical where avoidable, it does assume a modest level of comfort using basic tools like RStudio and GitHub. If you’re on Mac or Linux, you probably have git built in already, but you might need to install it if you’re on Windows. That’s outside the scope of this article, but you can find information about it on the Git for Windows download page.\nThe very high-level overview of the process here is:\nOnce you’ve done that, you can begin writing content and publishing blogs, and spend time fine-tuning and tweaking your site. (You can also do that before setting up GitHub Pages if you want to wait to unveil it until it has more content and polish - as you prefer.)\nAssuming you have git and RStudio installed, and a GitHub account created, you’re ready to proceed!"
  },
  {
    "objectID": "posts/2022-01-29_distill-and-github-pages-setup/index.html#create-new-github-repository",
    "href": "posts/2022-01-29_distill-and-github-pages-setup/index.html#create-new-github-repository",
    "title": "Distill and Github Pages Setup",
    "section": "Create new GitHub repository",
    "text": "Create new GitHub repository\nThe first step is to create a new repository for the site. Log into GitHub and create a new repo:\n\nCall it something sensible like “distill-blog”; make it public (unless you don’t want to) and add a .gitignore file using the R template (just to start things off). Also, if you haven’t already, change your github repository settings to use “main” as your default branch name instead of “master”.\nWe’ll come back in a bit to set up the Pages part, but for now we just want to get the project built and populated. We now have an empty Git repository sitting on GitHub, waiting to be used for something."
  },
  {
    "objectID": "posts/2022-01-29_distill-and-github-pages-setup/index.html#create-a-new-rstudio-project-using-the-repository",
    "href": "posts/2022-01-29_distill-and-github-pages-setup/index.html#create-a-new-rstudio-project-using-the-repository",
    "title": "Distill and Github Pages Setup",
    "section": "Create a new RStudio project using the repository",
    "text": "Create a new RStudio project using the repository\nNow we’ll use our new, empty GitHub repository for our Distill blog. From RStudio, choose “File -> New Project” and choose “Version Control”:\n\nChoose “Git”:\n\nCopy in the repository URL from your project (found as shown below):\n\nPaste it into the Repository URL field in the wizard, choose where you want to store it on your computer, and I recommend checking Open in new session for a clean slate. Click “Create Project” to create it.\n\nThe empty project will open in a new RStudio window. RStudio has added 3 new files (actually three directories) to the skeleton created by Github, which initially only contains the skeleton .gitignore file:\n\nRStudio adds the .git directory, the .Rproj.user directory, and the distill-blog.Rproj RStudio project file (it will be named with the “Project directory name” field from the final screen of the New Project Wizard).\n\nThe .gitignore file created by Github tells git to ignore the .Rproj.user directory (which contains local settings used by RStudio and which you don’t need to (and shouldn’t) share or publish), and the .git directory is part of every repository and contains the repository settings.\nWe want to keep github and RStudio in sync, generally speaking. More precisely, we want to keep the repository in sync between GitHub and RStudio. The local git repository doesn’t yet know about the project file distill-blog.Rproj, since RStudio added that after cloning the repository from GitHub, so a good first task is to add it to the local repository, and push it back up to GitHub to get it in sync, and make sure that that core pipeline is working, since we’ll be using it constantly.\nAlthough there is a “Git” pane in the Environments panel, which shows that distill-blog.Rproj is ready for staging:\n\n…I am an old-school curmudgeon and prefer to use the terminal for git commands, since not everything I normally do with git has a representation in this visual UI. In RStudio I have even remapped the shortcut control-backtick to the terminal for quick switching:\n\nWithin the terminal, typing git status will show that git doesn’t yet know about distill-blog.Rproj:\n\nThis isn’t going to be a git tutorial, so we’re just going to add the file to the repo and push it back to github. If you get stuck in this process, pause here until you’re able to troubleshoot and continue.\nThe git commands to update the repository with are:\ngit add .\ngit commit -m \"Add RStudio project skeleton\"\ngit push\n(Note: the first command, git add ., says to add all files in the current directory to the repository. We could have explicitly typed git add distill-blog.Rproj, but (a) git add . is much faster, and (b) much easier in the steps below when we may not even know all of the files that need adding, and explicitly enumerating them could be troublesome and error-prone. Sometimes you want to be careful and specific about which files you’re staging, but that’s beyond the scope of this discussion, and we’re keeping it simple here.)\n[slinbergdistill-blog] (main)$ git add . \n[slinbergdistill-blog] (main +)$ git commit -m \"Add RStudio project skeleton\"\n[main 6c67d2d] Add RStudio project skeleton\n 1 file changed, 13 insertions(+)\n create mode 100644 distill-blog.Rproj\n[slinbergdistill-blog] (main)$ git push \nEnter passphrase for key '/Users/slinberg/.ssh/sfl_id_rsa': \nEnumerating objects: 4, done.\nCounting objects: 100% (4/4), done.\nDelta compression using up to 8 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 444 bytes | 444.00 KiB/s, done.\nTotal 3 (delta 0), reused 0 (delta 0), pack-reused 0\nTo github.com:stevelinberg/distill-blog.git\n   54aae63..6c67d2d  main -> main \nAfter git push uploads the changes back to the GitHub repository, now if we flip back to GitHub and refresh the page, we’ll see that the project file has been added:\n\nAt this point, we have an empty RStudio project committed to a GitHub repository, but we don’t yet actually have any content. Now it’s time to create the Distill blog skeleton and add it to the repository."
  },
  {
    "objectID": "posts/2022-01-29_distill-and-github-pages-setup/index.html#add-the-distill-skeleton-to-the-repository",
    "href": "posts/2022-01-29_distill-and-github-pages-setup/index.html#add-the-distill-skeleton-to-the-repository",
    "title": "Distill and Github Pages Setup",
    "section": "Add the Distill skeleton to the repository",
    "text": "Add the Distill skeleton to the repository\nWe will now manually (sort of) create the Distill skeleton. Although we could have created a new project of a Distill blog from the RStudio Create New Project wizard, it wouldn’t have had the GitHub repository, and we would have needed to add that and do the github setup separately. Six of one, half-dozen of the other, but instead we created the GitHub repository first instead, and we’ll add in Distill ourselves now. The end result should be the same.\n(Note: these commands are entered in the RStudio Console, which looks a lot like the Terminal, but speaks a different language. They’re both areas where you type commands, but the difference is that the terminal is a shell that passes commands to the operating system, for doing things like creating directories, copying or deleting files, or issuing git commands, and the Console is an R-language construct that processes R commands. If you type an R command in the Terminal, or a shell command in the Console, they will both be confused and spit out an error message, but it won’t do any damage beyond that. A glance at the tab titles will quickly orient you.)\nIn the console, install the distill library if you haven’t yet (reinstalling it won’t do any harm):\n> install.packages(\"distill\")\ntrying URL 'https://cran.rstudio.com/bin/macosx/contrib/4.1/distill_1.3.tgz'\nContent type 'application/x-gzip' length 454710 bytes (444 KB)\n==================================================\ndownloaded 444 KB\n\n\nThe downloaded binary packages are in\n    /var/folders/jw/rmjpk2xj7z7_8txfqrjz0jhr0000gn/T//RtmpjDcuEY/downloaded_packages\nThen load the library, which you should also do as your first step (after opening the project) any time you work on it in the future:\n> library(distill)\nWith the distill library loaded, we’re ready to create the site. One important point is that we’re going to create the site in the root of our project, rather than inside a subdirectory. There’s no need to bury it one level down, since it won’t be a subcomponent of another project. The first argument in the command below, \".\", means to work in the current directory, which is set by default as the directory containing the distill-blog.Rproj file. The argument is the blog title (which can be changed later if needed, but if you can avoid needing to do that it saves a few braincells), and the final argument causes it to be built with support for GitHub Pages, which is where our blog will ultimately end up.\nHere’s the command I ran for this blog:\n> create_blog(\".\", \"Steve Linberg\", gh_pages = TRUE)\nThis takes a few seconds to run and creates a lot of new files. I like to go ahead and commit the project at this point as well, giving us a rollback point where we can wipe out changes we made and start over with a clean install if we get in trouble down the road. Switch back to the terminal, add all files in the directory, commit and push:\ngit add .\ngit commit -m \"Install Distill skeleton\"\ngit push\nIf we pop back over to GitHub and reload the repository view, we’ll see a lot of new files added:"
  },
  {
    "objectID": "posts/2022-01-29_distill-and-github-pages-setup/index.html#build-the-site",
    "href": "posts/2022-01-29_distill-and-github-pages-setup/index.html#build-the-site",
    "title": "Distill and Github Pages Setup",
    "section": "Build the site",
    "text": "Build the site\nBefore we do any further customizing, we can build the site and get it deploying on GitHub Pages. At this point, there’s nothing to see because the site hasn’t been built yet; doing the first build will construct the first blog post and the basic site skeleton.\nWhen you create the Distill site in this way, RStudio doesn’t yet know that there is one, and so the Build tab may be missing in the Environment pane:\n\nIf so, quit RStudio and re-open the project file, and it will appear:\n\nClick the Build tab and then “Build Website”:\n\nIt will grind for a few moments and do the first build, and present you with a glorious preview of your new blog in the Viewer panel:\n\nLet’s commit this stage too, since a few new files were just created. Switch to the Terminal tab (ctrl + backtick) and issue these git commands:\ngit add .\ngit commit -m \"First build of site skeleton\"\ngit push\nBreathtaking in its awesomeness, we are now ready to share it with the world!\nOK, you might prefer to do some work on it before you unveil it, and that’s fine. When you’re ready, continue below. (Read the RStudio page Creating a Blog to learn the details of how Distill sites work, and how to add and customize content; like anything worth learning, it takes a bit of time, but the payoff will be your mighty blog powers.) In this case, we’re going to go ahead and pretend we’re done and test the deployment to make sure it all works."
  },
  {
    "objectID": "posts/2022-01-29_distill-and-github-pages-setup/index.html#configuring-github-pages",
    "href": "posts/2022-01-29_distill-and-github-pages-setup/index.html#configuring-github-pages",
    "title": "Distill and Github Pages Setup",
    "section": "Configuring GitHub Pages",
    "text": "Configuring GitHub Pages\nNext, we need to go back to GitHub and get GitHub Pages working, so we can publish our site.\nFrom your GutHub project page, as shown below:\n\nClick “Settings”\nOn the left, click “Pages”\nUnder “Source”, set the branch to “main”\nSet the folder to “/docs”\nClick “Save”.\n\n(Note: ignore the “Choose a theme” button; it doesn’t apply to Distill projects.)\n\nGitHub will show a success message (hopefully):\n\nNote the “Custom domain” section at the bottom; we’ll return to that if/when we want to move the site to a custom domain. As it is, GitHub generates a URL for the site based on your GitHub username and the repo name. In this case, the address is https://stevelinberg.github.io/distill-blog/. Click the link, and you should see your site:\n\nThis means it’s live!\nYou could just go from here, if you’re content with the URL GitHub chose for you. Your basic workflow will be:\n\nWork on content in RStudio\nCommit changes with git as you go\nBuild the site when you’re ready (or as often as you like)\nPush the changes to GitHub, which publishes them to your site.\n\nIf you want to brand a little more strongly and put the site on your own domain, continue with “Using a Custom Domain”, the next article in this series, coming soon."
  },
  {
    "objectID": "posts/2022-04-02-quickie-for-importing-tasks-to-omnifocus-3/index.html",
    "href": "posts/2022-04-02-quickie-for-importing-tasks-to-omnifocus-3/index.html",
    "title": "Quickie for importing tasks to OmniFocus 3",
    "section": "",
    "text": "I’ve been an OmniFocus user since version 1.0.\nWell, sort of. I used it a ton in version 1.0 and then eventually stopped. Then I used it again a ton when version 2 came out, and then eventually stopped. It worked well when I really forced myself to use it, but eventually I’d run out of gas, and it always bugged me a bit that I couldn’t get a handle on why. I read David Allen’s “Getting Things Done”, which it was inspired by, and it all made sense, but it still just had a lot of friction to keep using.\nOne of the problems, I now realize, was the notion of contexts - each to-do item could be assigned to one, and only one, context, like “online” or “supermarket” or “home”, describing the setting where a task could be done. The problem was that there were many tasks that could be done in multiple contexts, but no way to mark them as such. One user I talked with on one of the forums explained that the way to think about it was that I myself could be in multiple simultaneous contexts, like “home” and “online”, and that tasks that matched either of those would appear. It sort of made sense, but it still never felt natural and I eventually dropped it again.\nI checked it out again in version 3, and the problem has been solved: contexts are gone, replaced by the much more versatile and familiar concept of tags. Tags can be freely defined, and any item can have as many tags as you want. So now I’m using it again and finding it much easier to get along with.\n(I also really appreciate that there is the option to just pay for it straight up, though they also offer the odious subscription-based pricing that everybody except developers hates. There is upgrade pricing from prior versions that makes it more affordable. They also offer educational pricing, so between my educational pricing and upgrade discount, OF3 went from $99 to about $38. Sold.)\nIt’s full of really nice little interface touches that are almost invisible but that make life very pleasant. One of them is the ability to import tasks in TaskPaper format, which is very straightforward. I needed to add a list of 24 assignments (sigh) to my inbox, one per day for the next 24 days. I had them typed out, and with the help of a few emacs macros added due dates and tags, using the following format:\n- task description @tags(tag1, tag2) @due(YYYY-MM-DD HH:MM)\nSo my full list was:\n- (ch 3) Simple linear regression @tags(Machine Learning, Homework) @due(2022-04-02 23:59)\n- (ch 3) Multiple linear regression @tags(Machine Learning, Homework) @due(2022-04-03 23:59)\n- (ch 4) Logistic regression @tags(Machine Learning, Homework) @due(2022-04-04 23:59)\n- (ch 4) Multiple logistic regression @tags(Machine Learning, Homework) @due(2022-04-05 23:59)\n- (ch 4) Linear discriminant analysis @tags(Machine Learning, Homework) @due(2022-04-06 23:59)\n- (ch 4) Quadratic discriminant analysis @tags(Machine Learning, Homework) @due(2022-04-07 23:59)\n- (ch 4) Naive Bayes @tags(Machine Learning, Homework) @due(2022-04-08 23:59)\n- (ch 4) K-Nearest Neighbors @tags(Machine Learning, Homework) @due(2022-04-09 23:59)\n- (ch 4) Poisson regression @tags(Machine Learning, Homework) @due(2022-04-10 23:59)\n- (ch 5) Cross-validation @tags(Machine Learning, Homework) @due(2022-04-11 23:59)\n- (ch 5) Bootstrap @tags(Machine Learning, Homework) @due(2022-04-12 23:59)\n- (ch 6) Best subset selection @tags(Machine Learning, Homework) @due(2022-04-13 23:59)\n- (ch 6) Stepwise selection @tags(Machine Learning, Homework) @due(2022-04-14 23:59)\n- (ch 6) Ridge regression @tags(Machine Learning, Homework) @due(2022-04-15 23:59)\n- (ch 6) Lasso @tags(Machine Learning, Homework) @due(2022-04-16 23:59)\n- (ch 6) Principal component regression @tags(Machine Learning, Homework) @due(2022-04-17 23:59)\n- (ch 8) Bagging @tags(Machine Learning, Homework) @due(2022-04-18 23:59)\n- (ch 8) Random forests @tags(Machine Learning, Homework) @due(2022-04-19 23:59)\n- (ch 8) Boosting @tags(Machine Learning, Homework) @due(2022-04-20 23:59)\n- (ch 8) Bayesian Additive Regression Trees @tags(Machine Learning, Homework) @due(2022-04-21 23:59)\n- (ch 9) Support Vector Machines @tags(Machine Learning, Homework) @due(2022-04-22 23:59)\n- (ch 12) Principal component analysis @tags(Machine Learning, Homework) @due(2022-04-23 23:59)\n- (ch 12) K-means clustering @tags(Machine Learning, Homework) @due(2022-04-24 23:59)\n- (ch 12) Hierarchical clustering @tags(Machine Learning, Homework) @due(2022-04-25 23:59)\nCopy the entire list, and then just paste into the inbox, and voilà:\n\n\n\nInbox\n\n\nNow the problem is I have to actually do all of those things. (But at least I can check off the top item for the new blog post.)\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{linberg2022,\n  author = {Steve Linberg},\n  title = {Quickie for Importing Tasks to {OmniFocus} 3},\n  date = {2022-04-02},\n  url = {https://slinberg.net/posts/2022-04-02-quickie-for-importing-tasks-to-omnifocus-3},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSteve Linberg. 2022. “Quickie for Importing Tasks to OmniFocus\n3.” April 2, 2022. https://slinberg.net/posts/2022-04-02-quickie-for-importing-tasks-to-omnifocus-3."
  },
  {
    "objectID": "posts/2022-12-02-sonnets/index.html",
    "href": "posts/2022-12-02-sonnets/index.html",
    "title": "AI Sonnets on Data Science",
    "section": "",
    "text": "I’ve been playing around with ChatGPT, which lets you interact with OpenAI’s GPT-3 language model in a chatbot style. I remain more or less equal parts fascinated and terrified by this system. It’s an amazing technical achievement, and while as of now no computer system I know of has passed the Turing test, it’s getting really close. The thing that scares me the most about the GPT-3 system is the certainty with which it says things that aren’t true… I tested it with the question of whether C is statically typed (it is), and this was its response:\nThis was “written” with such confidence that it made me question whether I’ve been wrong about C all these years, for a few minutes at least. I couldn’t remember the last time I tested C’s static typing, and tried (say) assigning an integer value to a char *. Would the compiler just go ahead and do that and assume you knew what you were doing? Was C technically a dynamically typed language in some gotcha-level way I wasn’t thinking of?\nAnswer: no, of course not. But the way the answer was phrased made me doubt myself."
  },
  {
    "objectID": "posts/2022-12-02-sonnets/index.html#a-sonnet-about-data-analytics",
    "href": "posts/2022-12-02-sonnets/index.html#a-sonnet-about-data-analytics",
    "title": "AI Sonnets on Data Science",
    "section": "A Sonnet About Data Analytics",
    "text": "A Sonnet About Data Analytics\nAnyway, after I got this out of my system I decided to just play with it. I asked it to write a sonnet about data analytics. This was its response:\n\nData analytics is a complex field,\nWhere numbers and trends are revealed,\nWith algorithms and tools,\nInsights are found and ruled,\nA treasure trove of knowledge sealed.\nThrough data analysis we can see,\nPatterns and trends in our history,\nFrom the past we can learn,\nAnd our future discern,\nA powerful tool for all to see.\nWith data we can make predictions,\nAnd take strategic decisions,\nFrom sales to marketing,\nData has a lot to bring,\nA valuable asset for all conditions.\nSo let us embrace data analytics,\nAnd all that it has to give us,\nA world of information,\nAt our fingertips and imagination,\nA powerful tool for all of us.\n\nI thought… that’s not too bad! I wanted to see another, so I said “do it again”. It responded:\n\nI’m sorry, but I’m not able to generate multiple versions of the same content. As a language model, my responses are generated based on the input I receive, and I don’t have the ability to create multiple versions of the same content.\n\nOK, fair enough. So you need to vary the input to get different output. I started adding adjectives.\n“write a sad sonnet about data analytics”"
  },
  {
    "objectID": "posts/2022-12-02-sonnets/index.html#a-sad-sonnet-about-data-analytics",
    "href": "posts/2022-12-02-sonnets/index.html#a-sad-sonnet-about-data-analytics",
    "title": "AI Sonnets on Data Science",
    "section": "A Sad Sonnet About Data Analytics",
    "text": "A Sad Sonnet About Data Analytics\n\nData analytics can be a cruel task,\nFilled with numbers that never ask,\nOur hearts may be breaking,\nBut the numbers keep shaking,\nLeaving us empty and devoid of any mask.\nWe pour our hearts into the data,\nBut it remains cold and uninviting,\nNo matter how much we try,\nIt never seems to satisfy,\nLeaving us feeling lost and solitary.\nThe insights we seek elude us still,\nAs the data remains chill,\nOur hopes and dreams shattered,\nBy the numbers that mattered,\nLeaving us alone and feeling ill. \nBut even in the darkest of days,\nWe must keep searching for the data’s ways,\nFor though it may break our hearts,\nIn the end it imparts,\nThe knowledge that sets us free from its daze.\n\n…I sat back heavily. My heart sank. This was utterly tragic. I had no idea data science could be so bleak.\nI had to go on.\n“write an angry sonnet about data analytics”"
  },
  {
    "objectID": "posts/2022-12-02-sonnets/index.html#an-angry-sonnet-about-data-analytics",
    "href": "posts/2022-12-02-sonnets/index.html#an-angry-sonnet-about-data-analytics",
    "title": "AI Sonnets on Data Science",
    "section": "An Angry Sonnet About Data Analytics",
    "text": "An Angry Sonnet About Data Analytics\n\nData analytics, a curse upon us all,\nA never-ending stream of numbers and code,\nWe toil and we labor,\nBut the data remains unyielding and cold,\nLeaving us frustrated and feeling alone.\nWith each passing day, our frustration grows,\nAs we struggle to make sense of the data,\nIt taunts and it teases,\nLeaving us feeling like fools,\nOur efforts met with nothing but woe.\nBut we will not be defeated by data,\nWe will not be broken by its cold embrace,\nWe will continue to fight,\nUntil we have unlocked its secrets,\nAnd claimed its insights as our own.\nFor though it may challenge us,\nData analytics is ours to tame,\nAnd we will not rest,\nUntil we have conquered its vast domain,\nAnd claimed its power as our own.\n\nThis one, I could definitely relate to.\n“write an obsequious sonnet about data analytics”"
  },
  {
    "objectID": "posts/2022-12-02-sonnets/index.html#an-obsequious-sonnet-about-data-analytics",
    "href": "posts/2022-12-02-sonnets/index.html#an-obsequious-sonnet-about-data-analytics",
    "title": "AI Sonnets on Data Science",
    "section": "An Obsequious Sonnet About Data Analytics",
    "text": "An Obsequious Sonnet About Data Analytics\n\nData analytics, a blessing to us all,\nA never-ending stream of numbers and code,\nWe humbly bow before it,\nGrateful for its insights and knowledge,\nAs it guides us on our path to success.\nWith each passing day, our gratitude grows,\nAs we learn more and more from the data,\nIt reveals and it teaches,\nLeading us to new heights,\nOur efforts rewarded with insights and growth.\nWe are but humble servants of data,\nEager to do its bidding and follow its lead,\nFor we know that in its hands,\nLie the keys to our future,\nAnd the power to unlock our greatest potential.\nSo let us embrace data analytics,\nAnd all that it has to offer us,\nFor in its power and wisdom,\nLies the path to our success,\nAnd the keys to a brighter future.\n\nI feel inspired.\n“write a narcissistic sonnet about data analytics”"
  },
  {
    "objectID": "posts/2022-12-02-sonnets/index.html#a-narcissistic-sonnet-about-data-analytics",
    "href": "posts/2022-12-02-sonnets/index.html#a-narcissistic-sonnet-about-data-analytics",
    "title": "AI Sonnets on Data Science",
    "section": "A Narcissistic Sonnet About Data Analytics",
    "text": "A Narcissistic Sonnet About Data Analytics\n\nData analytics, a reflection of me,\nA never-ending stream of numbers that see,\nInto the depths of my soul,\nAnd reveal the truth about me whole,\nA mirror that shows me all that I can be.\nWith each passing day, my admiration grows,\nAs I learn more and more from the data,\nIt shows me my strengths and my weaknesses,\nAnd helps me to improve and progress,\nA faithful companion on my journey.\nI am the master of data analytics,\nAnd it is mine to wield and command,\nWith its power and its knowledge,\nI will conquer and prevail,\nAnd rise above all others in the land.\nSo let us celebrate data analytics,\nAnd all that it does for me,\nFor in its numbers and its insights,\nLies the key to my greatness,\nAnd the path to my ultimate victory.\n\nI am the master of data analytics\nThe path to my ultimate victory.\nI can’t think of anything to add to that. That’s a wrap."
  },
  {
    "objectID": "posts/2022-10-31-hacked/index.html",
    "href": "posts/2022-10-31-hacked/index.html",
    "title": "Hacked! (or not)",
    "section": "",
    "text": "I thought this was pretty funny… some spam tonight that this website’s database has been hacked, and I better pay up some bitcoin or they’ll destr0y [my] site's reputati0n! Oh noes!\nIt takes some doing to hack the database of a site that doesn’t have a database, let me tell you. Very impressive work guys! They must have fprotted my tarball with lunix.\nUnfortunately my $5 in free bitcoin that I got a while back is only worth about $1.62 last time I looked. Guess they’re gonna keep my database on that 0ffsh0re server. So it goes."
  },
  {
    "objectID": "posts/2022-10-31-hacked/index.html#the-bad-news",
    "href": "posts/2022-10-31-hacked/index.html#the-bad-news",
    "title": "Hacked! (or not)",
    "section": "The bad news",
    "text": "The bad news\n\nYour Site Has Been CompromisedY0ur Site Has Been Hacked\nPLEASE FoRWARD THIS EMAIL T0 SoMEoNE IN Y0UR C0MPANY WHo iS ALL0WED To MAKE IMPORTANT DECISIoNS!\nWe have hacked y0ur website https://slinberg.net:443 and extracted your databases.\nHow did this happen?\nour team has f0und a vulnerability within your site that we were able t0 exploit. After finding the vulnerability we were able to get y0ur database credentials and extract your entire database and m0ve the information to an 0ffsh0re server.\nWhat does this mean?\nWe will systematically go through a series 0f steps 0f totally damaging your reputati0n. First your database will be leaked or s0ld to the highest bidder which they will use with whatever their intentions are. Next if there are e-mails f0und they will be e-mailed that their informati0n has been s0ld or leaked and your site https://slinberg.net:443 was at fault thusly damaging y0ur reputation and having angry customers/associates with whatever angry customers/associates do. Lastly any links that you have indexed in the search engines will be de-indexed based 0ff of blackhat techniques that we used in the past t0 de-index 0ur targets.\nHow d0 i st0p this?\nWe are willing to refrain from destr0ying y0ur site’s reputati0n for a small fee. The current fee is $3000 in bitcoins (0.15 BTC).\nPlease send the bitcoin to the f0llowing Bitcoin address (Copy and paste as it is case sensitive):\n(yeah we won’t copy and paste this part)\n0nce you have paid we will automatically get inf0rmed that it was your payment. Please note that you have t0 make payment within 3 days after opening this e-mail or the database leak, e-mails dispatched, and de-index 0f your site WiLL start!\nHow do i get Bitcoins?\nY0u can easily buy bitcoins via several websites or even offline from a Bitcoin-ATM.\nWhat if i d0n’t pay?\nif you decide not t0 pay, we will start the attack at the indicated date and uph0ld it until you do, there’s n0 counter measure t0 this, you will only end up wasting more m0ney trying to find a s0lution. We will completely destroy y0ur reputation am0ngst g00gle and your customers.\nThis is not a h0ax, d0 not reply t0 this email, don’t try to reas0n 0r negotiate, we will not read any replies. 0nce you have paid we will st0p what we were doing and y0u will never hear from us again!\nPlease n0te that Bitcoin is anonym0us and n0 0ne will find 0ut that y0u have complied. Finally don’t reply as this email is unm0nitored."
  },
  {
    "objectID": "posts/2022-11-18-mastodon/index.html",
    "href": "posts/2022-11-18-mastodon/index.html",
    "title": "Mastodon",
    "section": "",
    "text": "I’m less than thrilled with late 2022 Twitter. Not a fan of the new administration; let’s just leave it at that. I’m worried about whether the academic API will survive the present melée, and whether it will be overrun with awfulness with the slashing of content moderators, or turned over to AI moderation systems which I can assert are not ready for the full-time job.\nAnyway, one of the places Twitter refugees like me are flocking to is Mastodon. Danielle Navarro wrote up a nice introduction for data science folks like us, so I won’t go into the hows and whys here. The most important single point is that Mastodon is a federation of lots of servers, and you pick one (or more) to join, and then there are a few modes of viewing where you either see posts from just people you directly follow (“Home”), people on that server (“Local”), or everywhere (“Federated”, the Twitter-like “firehose” of everything all at once).\n“Home” and “Federated” will give you basically the same thing on whichever server(s) you join, so your choice of server only impacts the “Local” view. I set up a personal account on mastodon.social, the biggest single server, out of some instinct that I should be on the biggest one for some reason. You don’t actually need to do that - and there are so many people on that server that the “Local” view is more or less useless. It’s fine, but there’s no advantage to being on a big server.\nFosstodon is oriented at open-source software and data, so it seemed like a good place to set up my professional/academic account. Took a few hours to get my account approved; I then signed up for their Patreon to throw them a buck a month to help keep the lights on. Mastodon carries no ads, and isn’t owned by anybody. I love this, and would rather contribute a little to a product I use than to BE the product, which is the case in any “free,” ad-bearing social media.\nThe incomparable Julia Silge is on Mastodon (and also at Fosstodon); that’s good enough for me. So is Danielle Navarro, for that matter, and lots of other R and data luminaries. It’s been a long time since social media felt fun. Not missing the blue bird very much so far.\nPS. Validate your username! Mine is @stevelinberg@fosstodon.org; add a text link to your Quarto site’s front page, like I did in my Contact section, and be sure the rel=\"me\" attribute is part of the link. The markdown for this (including the groovy Mastodon icon: ) is easy enough:\n{{< fa brands mastodon title=\"mastodon\" >}}\n  [\\@stevelinberg\\@fosstodon.org](https://fosstodon.org/@stevelinberg){rel=\"me\"}\nDon’t forget to escape the @ symbols in the text part of the link. If you have header icons in your _quarto.yml as well, the entry looks like:\nwebsite:\n  navbar:\n    right:\n      - icon: mastodon\n        href: https://fosstodon.org/@stevelinberg\n        rel: me\nEasy peasy, lemon squeezy. (H/T to Mickaël CANOUIL for this tip about rel: me in yml!)\nThen go to your Mastodon’s account settings, and enter your website in one of the 4 metadata fields you get for links; the text on the right will give you the format for the link you need to verify your account with:\n\n\n\nsettings\n\n\nJust be sure that rel=\"me\" makes it in to your link, however you format it, and you’re golden.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{linberg2022,\n  author = {Steve Linberg},\n  title = {Mastodon},\n  date = {2022-11-18},\n  url = {https://slinberg.net/posts/2022-11-18-mastodon},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSteve Linberg. 2022. “Mastodon.” November 18, 2022. https://slinberg.net/posts/2022-11-18-mastodon."
  },
  {
    "objectID": "posts/2022-02-02-pythagorean-theorem/index.html",
    "href": "posts/2022-02-02-pythagorean-theorem/index.html",
    "title": "Pythagorean Theorem",
    "section": "",
    "text": "How did I make it to $adulthood * 2 without knowing that our old friend, the Pythagorean Theorem (\\(a^2 + b^2 = c^2\\)), works in any number of dimensions?\nThe Euclidean distance \\(d\\) between the origin (\\(0, 0, \\dots 0\\)) and a point defined by \\(n\\) coordinates \\(x\\) is just\n\\[d = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2}\\]\nAdd up all of the squares and take the square root. Dope-smack for me.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{linberg2022,\n  author = {Steve Linberg},\n  title = {Pythagorean {Theorem}},\n  date = {2022-02-02},\n  url = {https://slinberg.net/posts/2022-02-02-pythagorean-theorem},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSteve Linberg. 2022. “Pythagorean Theorem.” February 2,\n2022. https://slinberg.net/posts/2022-02-02-pythagorean-theorem."
  },
  {
    "objectID": "posts/2022-02-19-playfair-and-the-flatten-the-curve-graphic/index.html",
    "href": "posts/2022-02-19-playfair-and-the-flatten-the-curve-graphic/index.html",
    "title": "Playfair and the “Flatten the Curve” Graphic",
    "section": "",
    "text": "One of the good early tidbits from Edward Tufte’s “The Visual Display of Quantitative Information” is this quote from Scottish political economist William Playfair’s 1786 book “The Commercial and Political Atlas”:\n\nInformation, that is imperfectly acquired, is generally as imperfectly retained; and a man who has carefully investigated a printed table, finds, when done, that he has only a very faint and partial idea of what he has read; and that like a figure imprinted on sand, is soon totally erased and defaced.\n\nThis speaks to the general principle, widely accepted now but less so at that time, that tables of data can be hard to interpret and understand, and that knowledge gained from them is fleeting. Deep truths revealed by diligent statistical analysis are of limited value if they can’t make themselves readily understood, and if they don’t stick in the minds of viewers.\nOne of the great recent triumphs of visual design was the justly-famous “Flatten the Curve” graphic that illustrated the need to keep the rates of COVID-19 transmission low enough that hospitals could keep up and not be overwhelmed. It actually originated from the CDC in a 2007 publication with a title only a career bureaucrat could love: “Interim Pre-Pandemic Planning Guidance : Community Strategy for Pandemic Influenza Mitigation in the United States : Early, Targeted, Layered Use of Nonpharmaceutical Interventions” (Centers for Disease Control and Prevention (U.S.) 2007):\n\n\n\nThe CDC’s original “Flatten the Curve” graphic (2007)\n\n\nPonderous as the document’s title may be, this graphic nonetheless showed this nontrivial concept very well. The solid-colored curve was the bad scenario of an outbreak of a pandemic with no intervention, and the transparent curve was a view of squishing that outbreak down over time so that it would be more manageable.\nAs brilliant as the design was - far, far more effective than paragraphs of explanatory text with supporting tables of data - it was still a bit text-heavy, with two text axes and five chunks of (small) inline text that had to be read to fully understand its meaning, and similarly-colored curves showing the two different outcomes, so it wasn’t quite a glance-level chart that could be instantly understood. When COVID-19 hit in 2019, this graphic was popularized and went through many iterations of further simplification, to make its message as clear and impactful as possible. The New York Times published this version in 2020 (Roberts 2020):\n\n\n\nThe NYT’s simplified graphic (2020)\n\n\nThis is even more effective: the text and colors are bold and easy to read, the axis text is simplified, there are only three elements of text in the graph (each only three words), and a third item has been added: a horizontal line indicating the healthcare system’s capacity. This is about as simple and direct as it’s possible to make this information. It communicated a complex problem in a way that could be instantly understood, by almost anyone, regardless of education levels. It may have saved millions of lives, by being shared around the world and absorbed into the collective human understanding of what needed to happen to make the pandemic as manageable as possible.\n(Another very subtle and effective change was the rendering of both curves as slightly irregular in shape, as opposed to the mathematically perfect normal-distribution forms in the CDC’s original graphic; this would help to reinforce the notion that infection / hospitalization / death rates from the pandemic would not follow shapes that could be precisely predicted, though overall trends could still be forecast.)\nNot every complex social phenomenon can be rendered into such an effective message, but good data science should always strive for this level of simplicity and clarity when the purpose is to communicate something important as broadly and effectively as possible.\n\n\n\n\nReferences\n\nCenters for Disease Control and Prevention (U.S.). 2007. “Interim Pre-Pandemic Planning Guidance : Community Strategy for Pandemic Influenza Mitigation in the United States : Early, Targeted, Layered Use of Nonpharmaceutical Interventions.” https://stacks.cdc.gov/view/cdc/11425.\n\n\nRoberts, Siobhan. 2020. “Flattening the Coronavirus Curve.” The New York Times, March. https://www.nytimes.com/article/flatten-curve-coronavirus.html.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{linberg2022,\n  author = {Steve Linberg},\n  title = {Playfair and the “{Flatten} the {Curve}” {Graphic}},\n  date = {2022-02-19},\n  url = {https://slinberg.net/posts/2022-02-19-playfair-and-the-flatten-the-curve-graphic},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSteve Linberg. 2022. “Playfair and the ‘Flatten the\nCurve’ Graphic.” February 19, 2022. https://slinberg.net/posts/2022-02-19-playfair-and-the-flatten-the-curve-graphic."
  },
  {
    "objectID": "posts/2022-10-20_distill_to_quarto_part_2/index.html",
    "href": "posts/2022-10-20_distill_to_quarto_part_2/index.html",
    "title": "Porting to Quarto, part 2",
    "section": "",
    "text": "Here are a few more notes and tips on the transition to Quarto, for other folks who may be grappling with it as well. You might reference part one as well."
  },
  {
    "objectID": "posts/2022-10-20_distill_to_quarto_part_2/index.html#site-structure-and-categories",
    "href": "posts/2022-10-20_distill_to_quarto_part_2/index.html#site-structure-and-categories",
    "title": "Porting to Quarto, part 2",
    "section": "Site structure and categories",
    "text": "Site structure and categories\nThis site is (presently) a mix of 3 general categories of content:\n\nStatic, once-off pages like About or the front page\nTime-oriented narrative pages: the blog\nAcademic stuff that is sort of in between, having chronology but usually a bit more substantive than blog posts, which can be short and more stream-of-consciousness\n\nThere’s no real technical distinction between a blog post and a static page, except that Quarto includes functionality for grouping and listing blog posts by using various metadata added in posts’ YAML headers. One of the differences, though, is that blog posts can have categories; standard pages can’t. For this reason, I decided to treat the academic content as blog posts, to inherit the category functionality, but I wanted to keep them distinct from the blog section itself.\nI spent a lot of time trying to solve the following problem, and have just come to the conclusion that I can’t. This happened in Distill, and was part of my motivation for moving to Quarto, and it turns out it’s the same situation in Quarto:\n\n\n\n\n\n\nCategories cannot be shared across sections.\n\n\n\nI liked the idea of having categories on posts and pages on the site, as a non-hierarchical form of organization.1 Basically, I assumed categories were tags, metadata that could be arbitrarily attached to content in various forms and places, and then used to navigate the site in other ways.\nThat’s not quite how it works. In Quarto, categories are metadata attached to documents, and they are only accessible from listing pages, like the blog page or the academics page. There isn’t a way to search a site for categories, and categories don’t cross-pollenate across sections (i.e. categories defined in blog posts won’t be accessible in listings for academics, and vice versa).\nI spent a lot of time trying to find a workaround for this, and up until this morning I thought I was close. I made “academics” a subdirectory of “posts”, and did some gymnastics to keep the separate listing pages from combining the listings. I got all of that working, but in the end I noticed that the category clouds for the listing pages only consisted of the categories for the content it was showing, not all of the categories defined in the site.\nSo, I realized it’s just not going to work, and backed the academics page out of /posts and made it a parallel top-level structure. A lot of wasted time. Sigh.\nWe’ve come this far, though, so we’re committed now, and overall Quarto does feel cleaner and less reliant on hidden CLI voodoo than Distill. So we’ll just add a couple more notes and call it done."
  },
  {
    "objectID": "posts/2022-10-20_distill_to_quarto_part_2/index.html#suppressing-stuff-on-blog-pages",
    "href": "posts/2022-10-20_distill_to_quarto_part_2/index.html#suppressing-stuff-on-blog-pages",
    "title": "Porting to Quarto, part 2",
    "section": "Suppressing stuff on blog pages",
    "text": "Suppressing stuff on blog pages\nWhen you make a blog post, you get a lot of additional stuff by default that is usually good, but which you may not always want.\nIn my initial attempt to solve the above problem, I had a directory structure like this for the academics section:\nacademics/\nacademics/_metadata.yml\nacademics/index.qmd\nacademics/695M/\nacademics/697E/\nThe file index.qmd was the listing page for the academics section, but it was treated as a blog post because it was inside the academics directory being listed. This meant that it inherited blog post things like author name, post date, citation information, license information, etc. That was fine for posts, but I didn’t want it all on this particular page.\nTurning it all off took some doing, but the following fields in the YAML header did it:\n\n\n\n\n\n\n\n\nattribute\nvalue\neffect\n\n\n\n\ncitation\nfalse\nsuppress the BibTex citation block at the bottom of the page\n\n\nlicense\n\"\"\nsuppress the “Reuse” header and license information.\n\n\ntitle-block-style\nnone\nturn off some of the header block information, but we still also need:\n\n\ndate-format\n\"\"\nsupress the date header; also be sure NOT to have date in the header, or it will still be echoed\n\n\nauthor\n\"[hidden text]{style='display:none'}\"\nthe yuckiest hack of all; author can’t be blank, so the only thing we can do is put it in a <span> and use CSS to hide it.\n\n\n\nThe raw code is:\ncitation: false\nlicense: \"\"\ntitle-block-style: none\ndate-format: \"\"\nauthor: \"[hidden text]{style='display:none'}\"\nFor a visual reference, if we create a simple page like this:\n---\ntitle: \"A sample page\"\ndescription: |\n  a sample page description\n---\n\nI don't want any extra stuff on this page!\nIt will render like this:\n\nFor a blog post, maybe that’s fine, but for an index page or something else, we might only want the title and body, shown in green, and not the stuff in red:\n\nThe attributes listed above will remove this:\n\nLeaving us with this:\n\nAaaahhhhh."
  },
  {
    "objectID": "posts/2022-10-20_distill_to_quarto_part_2/index.html#redirecting-from-previous-urls",
    "href": "posts/2022-10-20_distill_to_quarto_part_2/index.html#redirecting-from-previous-urls",
    "title": "Porting to Quarto, part 2",
    "section": "Redirecting from previous URLs",
    "text": "Redirecting from previous URLs\nCool URLs don’t change.\nWe said that back in 1998. Did anybody listen? Not really. Do any of your 25-year-old URLs still work like that one does? I doubt it. My point.\nWhen you migrate your site, URLs are going to break. If you want bookmarks - or google results - to find their way to the pages that got moved around, instead of dropping your confused readers on your cute 404 page, you need to leave pointers from your old URLs to your new ones.\nThis can be a daunting project, depending on how many URLs you need to update, and there are several ways to do it. The best way involves having some control over the webserver that’s hosting your site, such as server-side rewrite rules or .htaccess files, because that doesn’t depend on the viewer’s browser to handle the issue that is really a server issue.\nHowever, if you’re hosting somewhere where you don’t have that kind of control, the next-best way is to use aliases in the YAML header. It won’t work in all cases, but it’s a lot better than nothing.\nSo, for example, due to differences in the way the Quarto and Distill versions of this site were built, the academics listing page was at this address:\nhttps://slinberg.net/academics_list.html\nAnd it is now:\nhttps://slinberg.net/academics.html\nHowever, if you follow the old link, you’ll see that it still brings you to the right place. This is done with this in the header:\naliases: \n  - \"academics_list.html\"\nWhat this actually does is tell Quarto to generate a file called academics_list.html, which contains a redirect to academics.html. This pollutes your directory a bit, and means you have to overlay the old directory structure into your current one, so it’s not ideal from a neatness perspective. However, it will work well enough in most cases.\nSo that’s a wrap on the migration process. Other Quarto goodness will follow as I uncover it."
  },
  {
    "objectID": "posts/2022-10-01_distill_to_quarto_part_1/index.html",
    "href": "posts/2022-10-01_distill_to_quarto_part_1/index.html",
    "title": "Porting slinberg.net from Distill to Quarto",
    "section": "",
    "text": "Quarto: all the cool kids are doing it.\nSetting aside the whys, this is a narrative of the hows - narrating the conversion of this site from Distill to Quarto as I go. This process was done with RStudio version 2022.07.11."
  },
  {
    "objectID": "posts/2022-10-01_distill_to_quarto_part_1/index.html#the-process-so-far",
    "href": "posts/2022-10-01_distill_to_quarto_part_1/index.html#the-process-so-far",
    "title": "Porting slinberg.net from Distill to Quarto",
    "section": "The process so far",
    "text": "The process so far\nUp to this point, the steps have been:\n\nUse RStudio to create a new “Quarto Website” project, which I inspiringly named quarto-blog, which will also be the Github repository name\nSeparately download the standalone Quarto CLI, for purposes yet to be determined\nRead a couple of the early Distill-to-Quarto narratives, such as\n\nKarat Sidhu - Converting a distill website to Quarto\nDanielle Navarro - Porting a distill blog to quarto\n\nSet a few of the basic options in the site’s _quarto.yml file (title, description, site-url, etc)\nCreated this as-of-this-typing-unsaved blog post as a new Quarto Document inside the project\nCheck RStudio’s Render on Save option in the file header, which quickly rebuilds the skeletal site on saves (of this file)"
  },
  {
    "objectID": "posts/2022-10-01_distill_to_quarto_part_1/index.html#creating-a-this-blog-post",
    "href": "posts/2022-10-01_distill_to_quarto_part_1/index.html#creating-a-this-blog-post",
    "title": "Porting slinberg.net from Distill to Quarto",
    "section": "Creating a (this) blog post",
    "text": "Creating a (this) blog post\nSo one immediate difference in terms of workflow is that the Quarto CLI doesn’t have - as of version 1.1.251, anyway - commands for creating blog post files. In Distill, you would say create_post(\"Foo\") in the RStudio console, and that would create a skeleton with some base infrastructure, some automatic date-fu, etc. It would be easy enough to build a quick utility for doing this, but I kind of like that a blog post appears to just be a file (OK, a directory with an index.qmd file) inside the top-level posts directory.\nSo I’m going to save this file in the path posts/2022-10-01_distill_to_quarto_part_1/index.qmd, and that’s all that’s needed to make it a blog post. Yes, I’m typing out the date and title by hand, which is slightly more tedious than Distill, but I also know there’s nothing special going on under the hood when I add this file.\nWhy name the folder …distill_to_quarto_part_1 instead of just distill_to_quarto? Because I’m pretty sure I read somewhere that there’s a context where blog post titles can be disambiguated from their date prefixes, and so I want the title to be unique enough to withstand that if I add another similarly-titled post in the future. Not feeling super confident about this decision so far, but there it is.\n(See That pesky first image in Appendix A below for the first snag: the first, or “listing”, image, and a pathname problem in Quarto.)"
  },
  {
    "objectID": "posts/2022-10-01_distill_to_quarto_part_1/index.html#getting-the-groovy-table-of-contents",
    "href": "posts/2022-10-01_distill_to_quarto_part_1/index.html#getting-the-groovy-table-of-contents",
    "title": "Porting slinberg.net from Distill to Quarto",
    "section": "Getting the groovy table of contents",
    "text": "Getting the groovy table of contents\nThe table of contents (to the left in desktop mode) automatically appears once you have headers in the document structure. Each document’s (or post’s) title is rendered as an h1 tag above the body, but you need to have at least one h2 tag for the table of contents to appear.\nTo make this work in the posts directory (or presumably any directory), it needs to be specified in the directory’s _metadata.yml file (so in this case, posts/_metadata.yml) like this:\n# Default for table of contents\ntoc: true\ntoc-title: Table of contents\ntoc-location: left\nI believe that was there by default; it could be moved over to the right if you wanted something else in the left margin, like a site directory or something.\n\nGithub links\nAt least in the default configuration, this is also where your github links configured for this specific page will appear. Unfortunately, that means they won’t appear if your document lacks h2 tags, and consequently the table of contents isn’t built. I’d prefer to have these links appear regardless, and separate from the table of contents; putting that on my mental list of things to figure out."
  },
  {
    "objectID": "posts/2022-10-01_distill_to_quarto_part_1/index.html#deleting-the-two-sample-posts",
    "href": "posts/2022-10-01_distill_to_quarto_part_1/index.html#deleting-the-two-sample-posts",
    "title": "Porting slinberg.net from Distill to Quarto",
    "section": "Deleting the two sample posts",
    "text": "Deleting the two sample posts\nNow that this document is saved, establishing the blog structure, let’s nuke the two sample posts the site was initialized with; this, happily, is just a matter of deleting the directories posts/post-with-code and welcome.\n\nJust deleting the directories re-renders the site, and they’re gone. Nice that there’s nothing else to that.\n(Doing this did appear to confuse RStudio the next time I saved this file, though, and caused a new render process to start, stranding the prior preview tab in my browser and opening a new one. Still a few early-launch kinks to work out here, unsurprisingly…)\nPS. WHOA! that image is way too big. Let’s scale it down. Change the … do we still call this snippet rmarkdown? … from this:\n![](delete-posts.png)\nto this:\n![](delete-posts.png){width=50%}\n\nThat’s better. That was a screengrab from a high-res display, so downscaling it by 50% was the right approach."
  },
  {
    "objectID": "posts/2022-10-01_distill_to_quarto_part_1/index.html#create-the-missing-repo-and-push",
    "href": "posts/2022-10-01_distill_to_quarto_part_1/index.html#create-the-missing-repo-and-push",
    "title": "Porting slinberg.net from Distill to Quarto",
    "section": "Create the missing repo and push",
    "text": "Create the missing repo and push\nThis is all going fine, but I haven’t actually created the GitHub repo yet, so now it’s time to do that. Once I’m happy with how it looks, I’ll need to update the publishing process to unhook the distill-blog project from my domain and replace it with this one, but for now I just want to be able to push the project.\nCreated the public project as “quarto-blog”, and now from the terminal:\ngit remote add origin git@github.com:stevelinberg/quarto-blog.git\ngit add .\ngit commit -m \"Initial commit\"\ngit push -u origin main"
  },
  {
    "objectID": "posts/2022-10-01_distill_to_quarto_part_1/index.html#migrating-a-post-from-the-distill-blog",
    "href": "posts/2022-10-01_distill_to_quarto_part_1/index.html#migrating-a-post-from-the-distill-blog",
    "title": "Porting slinberg.net from Distill to Quarto",
    "section": "Migrating a post from the Distill blog",
    "text": "Migrating a post from the Distill blog\nPosts are just directories, so we’ll copy the last old post from the distill project into the posts directory and see what we need to do to make it work.\ncp -r ../distill-blog/_posts/2022-04-02-quickie-for-importing-tasks-to-omnifocus-3 posts/\n\n\n\n\n\n\nImportant\n\n\n\nNote: if you’re a command-line wonk like me and not dragging-and-dropping in a UI, be careful not to include the trailing slash on the Distill post directory in the above command, or it will dump the contents of the old post’s directory into your posts directory, which is not what you want at all.\n\n\nPS. groovy warning box above! Quarto is built on Bootstrap 5 (at present), which means we get yummy candy like that for free:\n:::{.callout-important}\nNote: if you're...\n:::\nAnyway, now we’ve got the directory for a single Distill post copied into the Quarto posts directory, and we need to convert it so Quarto will render it. What do we need to do?\n\n1. Delete the extraneous files\nHere’s what we’ve got:\n\nThis will vary a bit by post content, but of these, the two we want are the .Rmd file, which is the original blog post content from Distill, and the supporting png image. The .html file can go, and the …_files directory (full of Javascript files) can go.\nThen we rename the .Rmd file to index.qmd, and we have this:\n\n\n\n2. Update the post’s YAML header\nThe old YAML header for the Distill blog looks like this:\n---\ntitle: \"Quickie for importing tasks to OmniFocus 3\"\ndescription: |\n  Copy and paste formatted text to mass-create tasks.\nauthor:\n  - name: Steve Linberg\n    url: https://slinberg.net\ndate: 2022-04-02\noutput:\n  distill::distill_article:\n    self_contained: false\npreview: https://www.omnigroup.com/assets/img/icons/omnifocus-web@2x.png\ncategories:\n  - omnifocus\n---\nSome of this will work in Quarto, some stuff has to be changed or removed. The current, very simple header for this post is:\ntitle: \"Porting slinberg.net from Distill to Quarto\"\ndate: \"2022-10-01\"\ncategories: [quarto, distill, blog]\nimage: quarto.png\nSo we can keep title and date in the original header. I don’t remember why the optional description attribute led with a vertical bar and a newline; the author tag can be deleted, as it will inherit the default values from posts/_metadata.yml; the output section can be deleted, the preview attribute can get quoted and renamed to image, and the categories tag gets pulled up to a single line, a square-bracketed list of categories. I’m adding “blog” as a category to blog posts, so far, since it will give another option for filtering blog posts as separate from static content on the site; not totally sure this will be needed, but going with it for the moment.\nThe updated YAML header now looks like this:\n---\ntitle: \"Quickie for importing tasks to OmniFocus 3\"\ndescription: \"Copy and paste formatted text to mass-create tasks.\"\ndate: \"2022-04-02\"\nimage: \"https://www.omnigroup.com/assets/img/icons/omnifocus-web@2x.png\"\ncategories: [blog, omnifocus]\n---\nThe description attribute, if present, will be used instead of a content preview on the listing page.\n\n\n3. Nuke the knitr setup block\n(For the life of me, I cannot remember, figure out, or google how to quote markdown within markdown, so to make this render I’m changing the backticks to single-quotes:)\n'''{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\n'''\nDelete that block, and any other extraneous knitr stuff unless you know why you need it.\nSave and render. For this post, we’re done; it’s in and looks good. The blog page now looks like this:\n\n\n\n4. Commit!\nDon’t forget: commit early, commit often:\ngit add posts/2022-04-02-quickie-for-importing-tasks-to-omnifocus-3/\ngit commit -m \"migrate old post\"\n\n\n5. Rinse and repeat\nNow, do the same process with the rest of the posts. I don’t have that many to move, so doing it manually isn’t too tedious, but it could be scripted for a lot of content if needed.\n\n\n\n\n\n\nNote\n\n\n\nNote: by not renaming the post directory, and by virtue of the fact that both Distill and Quarto use /posts as the top level for blog posts by default, the URLs for the posts won’t change when we switch. It’s rude to break your working URLs when migrating (even though the number of humans who correctly care about this is vanishingly small), so this is good news. If your posts directory will be called something else, or basically if anything changes, you should set up redirects for your old URLs so they point to the new ones."
  },
  {
    "objectID": "posts/2022-10-01_distill_to_quarto_part_1/index.html#optional-configure-for-github-pages",
    "href": "posts/2022-10-01_distill_to_quarto_part_1/index.html#optional-configure-for-github-pages",
    "title": "Porting slinberg.net from Distill to Quarto",
    "section": "(optional) Configure for Github Pages",
    "text": "(optional) Configure for Github Pages\nIf you’re hosting on Github pages, there are a couple of additional things to do:\n\n1. Set output to “docs”\nBy default, the site will render to the directory “_site”. Github Pages, where I’m presently hosting slinberg.net, publishes from the “docs” directory, so we need to tell Quarto to do that.\nIn _quarto.yml, add “output-dir: docs” to the project section, as shown:\nproject:\n  type: website\n  output-dir: docs\nRebuild the whole site from the terminal, in the root directory:\nquarto render\nDelete _site from your filesystem and repo, and add the “docs” directory:\ngit rm -rf _site\ngit commit -m \"Remove old _site directory\"\ngit add docs\ngit commit -m \"Add docs directory\"\n\n\n2. Create and commit a .nojekyll file\nGithub requires the presence of a file called .nojekyll to prevent rendering with Jekyll, Github’s site generation tool. Again, from your site’s root:\ntouch .nojekyll\ngit add .nojekyll\ngit commit -m \"Add .nojekyll file\"\n\n\n3. Create the CNAME file if you’re using a custom domain\nIf you are using a custom domain name (like slinberg.net), you must also create a file called CNAME in your site’s root, with the domain name in plain text followed by a carriage return. Of course, you also have to own the domain and have it pointed at Github correctly, a topic we’re not covering here. The domain name will not work correctly without this file."
  },
  {
    "objectID": "posts/2022-10-01_distill_to_quarto_part_1/index.html#other-conversion-notes",
    "href": "posts/2022-10-01_distill_to_quarto_part_1/index.html#other-conversion-notes",
    "title": "Porting slinberg.net from Distill to Quarto",
    "section": "Other conversion notes",
    "text": "Other conversion notes\nOther unordered odds and end from the porting process:\n\nHashpipe syntax\nOne of the nice things about Quarto is much more readable chunk options via the “hashpipe” syntax (# = “hash”, | = “pipe”, so #| = “hashpipe” 🙄). For example, we can turn this heavy thing:\n{r echo=FALSE, fig.cap=\"Figure from \n[The Visual Display of Quantitative Information](https://www.edwardtufte.com/tufte/books_vdqi),\np. 118\", out.width=400}\nknitr::include_graphics(\"badgraphic.png\")\nInto this more readable construct:\n{r}\n#| echo: FALSE\n#| fig-cap: Figure from [The Visual Display of Quantitative Information](https://www.edwardtufte.com/tufte/books_vdqi), p. 118\n#| out.with: 400\nknitr::include_graphics(\"badgraphic.png\")\n… in theory.\nIn practice, as of this writing, there’s a bug in Quarto that prevents this from working correctly in .qmd files, so… for the moment, we wait and keep it in the former style.\n\n\n.Rmd files still render\nI was a little surprised to see that my non-converted blog posts, copied straight over from the Distill blog and prior to the cleanup for Quarto, still rendered just fine. I guess that’s good, but I wasn’t expecting it. I’m still going to clean them up."
  },
  {
    "objectID": "posts/2022-10-01_distill_to_quarto_part_1/index.html#continue-to-part-2",
    "href": "posts/2022-10-01_distill_to_quarto_part_1/index.html#continue-to-part-2",
    "title": "Porting slinberg.net from Distill to Quarto",
    "section": "Continue to part 2!",
    "text": "Continue to part 2!\nMore notes and commentary on the process can be found in part 2 of this series."
  },
  {
    "objectID": "posts/2022-10-01_distill_to_quarto_part_1/index.html#appendix-a-problems",
    "href": "posts/2022-10-01_distill_to_quarto_part_1/index.html#appendix-a-problems",
    "title": "Porting slinberg.net from Distill to Quarto",
    "section": "Appendix A: problems",
    "text": "Appendix A: problems\nRather than interrupt the basic narrative above, let’s stack the problems we hit along the way down here.\n\nThat pesky first image\nOne of the basic concepts in blogs and blog listings is the all-important thumbnail image that is used on the listing page.\nThere are basically two ways to implement this in Quarto: either explicitly specify it with the image atrribute in the YAML section of the post’s index.qmd file:\nimage: \"image.jpg\"\nor, omit this attribute, and in that case, Quarto will use the first image it finds in the post body (if any). The difference is: if you only want to use the image on the listing page, but not have it appear in the post itself, put it in the YAML header; if you want it to be used on both, then omit the YAML header and put it inline.\nThe problem is that if you do the latter, and the image is right at the start of the post (a header image, in other words), it will not render correctly if it happens to be within the range of the preview text (which doesn’t appear to be visibly configurable in Quarto, but it’s more or less the first line or two); this is because it doesn’t convert the filename path to a global path. It does in the listing itself, which is odd. You can see the result here, from one of the sample posts; the original code is:\nThis is the first post in a Quarto blog. Welcome!\n\n![](thumbnail.jpg)\nBut it we reverse it to put the image above the text:\n![](thumbnail.jpg)\n\nThis is the first post in a Quarto blog. Welcome!\nWe get this on the listing page:\n\nThis is Quarto failing to adjust the image’s pathname in the listing, though it does render the image correctly to the right as part of the listing structure.\nThis can be worked around with the simple but ugly hack of fully specifying the image’s pathname, as in:\n![](/posts/post-with-code/thumbnail.jpg)\n\nThis is the first post in a Quarto blog. Welcome!\nbut\n\nGross, and\nYou probably don’t want an inline image from the body appearing in the listing preview anyway.\n\nHopefully this issue will get resolved at some point."
  },
  {
    "objectID": "posts/2022-12-06-chatgpt-sieve/index.html",
    "href": "posts/2022-12-06-chatgpt-sieve/index.html",
    "title": "ChatGPT fail: Sieve of Eratosthenes",
    "section": "",
    "text": "My flirtation with ChatGPT continues.\nA few days ago I had some fun asking it to compose emotive sonnets about data science. That was entertaining, and I maintain that anyone who isn’t recognizing this moment as a major inflection point for authoring content of any kind isn’t paying attention. And yes, this extends to writing code. Some people are (rightly) fearful that AI will be able to write better code than humans very soon, for approximately a nothingth of the cost, and that we might actually be there now. Stackoverflow has temporarily banned ChatGPT, though how they’re going to police that remains a bit of a mystery to me.\nThere may be some divine symmetry to AI systems obsoleting the human programmers that spawned them, but I don’t think we’re quite there yet. I asked ChatGPT to implement a Sieve of Eratosthenes in R, and this was its confident response:\n\n# Function to implement Sieve of Eratosthenes\nsieve <- function(n) {\n  # Create a boolean array \"prime[0..n]\" and\n  # initialize all entries it as true.\n  # A value in prime[i] will finally be\n  # false if i is Not a prime, else true.\n  prime <- rep(TRUE, n + 1)\n\n  # Set entries for 0 and 1 to false as\n  # they are not prime numbers\n  prime[1] <- FALSE\n  prime[2] <- TRUE\n\n  # Loop through the array and set the\n  # value of each element to false if\n  # it is not a prime number.\n  for (p in 2:n) {\n    if (prime[p]) {\n      # Update all multiples of p\n      for (i in 2 * p:n) {\n        prime[i] <- FALSE\n      }\n    }\n  }\n\n  # Return a vector of prime numbers\n  return(which(prime))\n}\n\nPretty impressive. Concise, clean, and commented (well, with a caps typo), with a paragraph of explanatory text below it describing how it works, it initially put a chill down my spine.\nUntil a few seconds later, when I noticed some obvious issues:\n  # Set entries for 0 and 1 to false as\n  # they are not prime numbers\n  prime[1] <- FALSE\n  prime[2] <- TRUE\nThere are at least 2 bugs here. The first is that R vectors are one-based, not zero-based, and the code uses the numbers being tested as primes as the index for the vector; the code is actually OK, but the comment is wrong. The second is even with the off-by-one index, the code doesn’t do what the comment describes (set two values to false). Also, the second line is redundant because the entire vector has already been initialized to TRUE. And finally, of course, when run, this code produces incorrect output:\n> sieve(100)\n [1]   2   3   5   7   9  11  13  15  17  19  21  23  25  27  29  31  33  35  37  39  41  43  45\n[24]  47  49  51  53  55  57  59  61  63  65  67  69  71  73  75  77  79  81  83  85  87  89  91\n[47]  93  95  97  99 101\nStarts off OK, but then 9 appears, and it goes downhill rapidly after that.\nIt’s easy enough to fix:\n\nstart the array with FALSE for 1, since 1 is not prime (logic error)\nuse the rounded square root of n for the upper boundary, not n (logic error)\nuse a seq function iterating by p, starting at p^2, to mark the composites; the R code for (i in 2 * p:n) doesn’t do that (language error).\n\n\nsieve <- function(n) {\n  # Create a boolean array \"prime[1..n]\" and\n  # initialize all entries it as true, except\n  # for prime[1] = FALSE.\n  # A value in prime[i] will finally be\n  # false if i is not a prime, else true.\n  # 1 is not a prime number; initialize it to FALSE.\n  prime <- c(FALSE, rep(TRUE, n - 1))\n  \n  # Loop through the array and set the\n  # value of each element to false if\n  # it is not a prime number.\n  for (p in 2:floor(sqrt(n))) {\n    if (prime[p]) {\n      # Update all multiples of p not exceeding n, starting at p^2\n      for (i in seq(p ^ 2, n, by = p)) {\n        prime[i] <- FALSE\n      }\n    }\n  }\n  \n  # Return a vector of prime numbers\n  return(which(prime))\n}\n\nThe point of this isn’t to flex on first-year computer-science R; the point is that the code is presented authoritatively by the AI (even with their general disclaimers that things may be wrong), and since a lot of what passes for programming these days seems to be blindly copying and pasting code fragments from Stackoverflow, this is another vector where that could happen.\nSo: it’s impressive that it got as close as it did; what worries me is how authoritatively it presented its buggy code. People are going to trust these systems, even if they shouldn’t, and since it’s likely to be used by people who aren’t experts, errors like those described above aren’t likely to be easily caught. I’m sure the quality will improve in time – these are the early days – but where is this all leading? The bot was so confident in its assertion that C is a dynamically-typed language the other day that it made me doubt myself after decades of C at the center of my brain’s programming-language orientation. I hadn’t studied it formally in school; had I been wrong about it all along?\nInterestingly, the python code the bot produced for the sieve was just fine (I just changed the limit for output comparison):\n\n# MAX_PRIMES = 100000  # maximum number of prime numbers to find\nMAX_PRIMES = 100  # maximum number of prime numbers to find\n\n# Sieve of Eratosthenes\n# Finds all prime numbers up to MAX_PRIMES using the Sieve of Eratosthenes algorithm\ndef sieve_of_eratosthenes():\n  primes = [True] * MAX_PRIMES  # list to store prime numbers\n\n  # 1 is not a prime number\n  primes[1] = False\n\n  # sieve algorithm\n  for i in range(2, int(MAX_PRIMES ** 0.5) + 1):\n    if primes[i]:\n      for j in range(i ** 2, MAX_PRIMES, i):\n        primes[j] = False\n\n  # print the prime numbers\n  for i in range(1, MAX_PRIMES):\n    if primes[i]:\n      print(i)\n\nsieve_of_eratosthenes()\n\n2\n3\n5\n7\n11\n13\n17\n19\n23\n29\n31\n37\n41\n43\n47\n53\n59\n61\n67\n71\n73\n79\n83\n89\n97\n\n\nExciting times. This technology is amazingly promising, and it’s going to change the world. But the flaws are as dangerous as the strengths are thrilling. Here we go.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{linberg2022,\n  author = {Steve Linberg},\n  title = {ChatGPT Fail: {Sieve} of {Eratosthenes}},\n  date = {2022-12-06},\n  url = {https://slinberg.net/posts/2022-12-06-chatgpt-sieve},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSteve Linberg. 2022. “ChatGPT Fail: Sieve of Eratosthenes.”\nDecember 6, 2022. https://slinberg.net/posts/2022-12-06-chatgpt-sieve."
  },
  {
    "objectID": "posts/2022-03-05-tufte-takes-no-prisoners/index.html",
    "href": "posts/2022-03-05-tufte-takes-no-prisoners/index.html",
    "title": "Tufte Takes No Prisoners",
    "section": "",
    "text": "The gloves come off in in “Chartjunk”, chapter 5 of Edward Tufte’s “The Visual Display of Quantitative Information”. Here I am, innocently beginning to think about a poster presentation I have coming up in a couple of months and to imagine design elements that will beautify the project and help make the information easier to understand, and then right in the first paragraph, Tufte drops this:\n\nGraphical decoration, which propers in technical publications as well as in commercial and media graphics, comes cheaper than the hard work required to produce intriguing numbers and secure evidence.\n\n…ouch.\nOK, let’s not overreact: he’s not saying “don’t strive for beauty” here. The main point is that visual clutter in charts and graphs, which can take many forms, at best pollutes the message and obfuscates the data, and at worst might be used to (attempt to) conceal a lack of data. That’s fair. He then goes on to show many examples of charts rendered almost unintelligible by over-use of fill patterns, made simple by modern computer-generated graphing systems, pointless use of three-dimensional effects that don’t represent data, and heavy gridlines that visually compete with data points.\nOn the subject of graph paper, he doesn’t hold back:\n\nMost ready-made graph paper comes with a darkly printed grid. The reverse (unprinted) side should be used, for then the lines show through faintly and do not clutter the data. If the paper is heavily gridded on both sides, throw it out.\n\nA little further on, he shows this monstrosity:\n\n\n\n\n\nFigure from The Visual Display of Quantitative Information, p. 118\n\n\n\n\n…introduced by this utter decimation:\n\nA series of weird three-dimensional displays appearing in the magazine American Education in the 1970s delighted connoisseurs of the graphically preposterous. Here five colors report, almost by happenstance, only five pieces of data (since the division within each year adds to 100 percent). This may well be the worst graphic ever to find its way into print.\n\nOn first read, I thought to myself: “I want to be a connoisseur of the graphically preposterous”. Then I thought it over a bit more and decided I wasn’t so sure.\nIn any event, I’m around the 2/3 point now in VDQI, enjoying many savagely entertaining examples of what not to do in the visual presentation of data. There are a few good examples scattered in as well, though with much lower overall frequency so far. Studying the wreckage of bad design can absolutely be educational, but with all due deference to the important work of learning from the mistakes of others, it is always easier to critique than to create. I am not “peeking ahead” out of sequence to see if he does this more in subsequent chapters; Saturday Mornings with Tufte™ are deliberately slow and meditative, with a cup of coffee and no electronic devices or distractions. So far this has been an informative and educational read, and I’m looking forward to the rest of it and then to moving on to “Beautiful Evidence”. But I’m still planning to try to create something beautiful in my upcoming poster, and hopefully I can find a way to do that that would also dodge the well-deserved barbs aimed at the many failed efforts shown in this book.\n\n(Endnote: I am a big fan of graph paper, and although most of my note-taking these days is on the ipad, I still have a big physical stash of my favorite, whitelines (non-referral link):\n\n\n\nFrom one of my whitelines notebooks\n\n\nIt uses a light gray tone with white gridlines instead of black, much easier on the eyes.)\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{linberg2022,\n  author = {Steve Linberg},\n  title = {Tufte {Takes} {No} {Prisoners}},\n  date = {2022-03-05},\n  url = {https://slinberg.net/posts/2022-03-05-tufte-takes-no-prisoners},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSteve Linberg. 2022. “Tufte Takes No Prisoners.” March 5,\n2022. https://slinberg.net/posts/2022-03-05-tufte-takes-no-prisoners."
  },
  {
    "objectID": "posts/2022-02-05-saturday-mornings-with-tufte/index.html",
    "href": "posts/2022-02-05-saturday-mornings-with-tufte/index.html",
    "title": "Saturday mornings with Tufte",
    "section": "",
    "text": "It takes a lot of sustained effort to build, maintain, and grow robust quantitative skills. Data science is hard, and it’s not enough to just have a wealth of formulas and statistical techniques committed to memory: data investigation requires instincts, which can only come as the hard-won results of a great deal of study, practice and inquiry.\nAs the world re-aligns itself around the tsunami of data that is already hitting us, though, we all know that quant-fu is only half the battle. Being able to manipulate and interpret data fluently is absolutely essential, but no less so is the ability to present and communicate it effectively. Data presentation skills are meaningless (and dangerous) if built atop a weak quantitative foundation, but all the quantitative skills in the world won’t save us if we can’t make the results of our work understandable.\n\n\n\nSaturday reading\n\n\nI’m taking a few hours every Saturday morning to read Edward Tufte’s “The Visual Display of Quantitative Information” and “Beautiful Evidence”. I’m not far enough in yet to articulate his vision or integrate his concepts into my own work, but it has the feeling of a correct path that I’ve learned to recognize from early forays in other fields. A lot of it is about decluttering, stripping away what is superfluous to leave only what is essential, which is one aspect of great art.\n\n\n\nSample page on time series\n\n\nOne of the things that I love about data science is that this principle is equally critical in both the quantitative and qualitative aspects of the field. Statistical and engineering skills help strip away unneeded or irrelevant data; information design skills help visually focus on the most important aspects of what remains, to deliver the message as clearly as possible.\n(And, right out of the gate on page 1 of VDQI, as the cool kids call it, is an idea for a short quantitative project that I’ll take up in the next post.)\n\n\n\nVDQI p1\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{linberg2022,\n  author = {Steve Linberg},\n  title = {Saturday Mornings with {Tufte}},\n  date = {2022-02-05},\n  url = {https://slinberg.net/posts/2022-02-05-saturday-mornings-with-tufte},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nSteve Linberg. 2022. “Saturday Mornings with Tufte.”\nFebruary 5, 2022. https://slinberg.net/posts/2022-02-05-saturday-mornings-with-tufte."
  },
  {
    "objectID": "steve.html",
    "href": "steve.html",
    "title": "About",
    "section": "",
    "text": "In summer 2022, I completed coursework for the Data Analytics and Computational Social Science (DACSS) Masters program at the University of Massachusetts, Amherst (formally graduating later this year). My present position is Senior Systems and Analytics Engineer at the Gandara Center in West Springfield, MA.\nI’ve worn many IT hats over the years - sysadmin, DBA, trainer, tech writer - but a day without code is a strange and sad day."
  },
  {
    "objectID": "steve.html#education",
    "href": "steve.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nMS, Data Analytics and Computational Social Science | 2022\nUniversity of Massachusetts, Amherst | Amherst, MA\nBA, Social Science | 1992\nHampshire College | Amherst, MA"
  },
  {
    "objectID": "steve.html#selected-experience",
    "href": "steve.html#selected-experience",
    "title": "About",
    "section": "Selected Experience",
    "text": "Selected Experience\nGandara Center, West Springfield, MA | Senior Systems and Analytics Engineer / CIO / IT Director | 2005 - Present\nSilicon Goblin Technologies, Amherst, MA | Owner / Programmer | 1999 - 2015\nRojo Networks, San Francisco, CA | Systems Administrator and Customer Service Lead | 2006-2008\nNational Center on Adult Literacy, University of Pennsylvania Graduate School of Education, Philadelphia, PA | Systems Programmer and Lead Web Developer | 1997-1999\nHolyoke Public Schools, Holyoke, MA | Literacy and GED Instructor | 1994-1997\nTurning Point Software, Boston, MA | Programmer | 1988-1990\nUnicom, Inc., Providence, RI | Systems Programmer and Technical Writer | 1983-1997"
  },
  {
    "objectID": "steve.html#interests",
    "href": "steve.html#interests",
    "title": "About",
    "section": "Interests",
    "text": "Interests\n\nData analytics, statistics, machine learning\nSoftware engineering\nPolitics, culture and the state of the world\nClassical drawing and painting, guitar, motorcycles, baking, gaming"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Steve Linberg",
    "section": "",
    "text": "I’m a software engineer, data analyst and systems/network admin, recently wrapping up my masters work in the Data Analytics and Computational Social Science (DACSS) program at the University of Massachusetts Amherst.\nThis site contains my blog with ruminations on computational social science, my own learning processes and discoveries, and related technical topics and snippets; selections of my academic work; contact information; and whatever other nuggets and tidbits pop up.\n\nData is the sword of the 21st century, those who wield it well, the Samurai\n– Jonathan Rosenberg\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nChatGPT writes buggy R in a quick test\n\n\n\n\n\n\nDec 6, 2022\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThe OpenAI system writes emotive sonnets about data science.\n\n\n\n\n\n\nDec 2, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nDecentralized, open-source social media? Yes please.\n\n\n\n\n\n\nNov 18, 2022\n\n\n2 min\n\n\n\n\n\n\nNo matching items\n\n\nSee the full blog for more.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYet more ways to look at the Florence data.\n\n\n\n\n\n\nMar 26, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamining additional centrality measures in the Florentine network\n\n\n\n\n\n\nFeb 16, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamining centrality measures in the Florentine network\n\n\n\n\n\n\nFeb 10, 2022\n\n\n7 min\n\n\n\n\n\n\nNo matching items\n\n\nSee the academics page for more."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Steve Linberg",
    "section": "Contact",
    "text": "Contact\n\n steve@slinberg.net (professional) or slinberg@umass.edu (academic)\n @stevelinberg@fosstodon.org\n @slinberg\n @stevelinberg (deprecated)"
  },
  {
    "objectID": "academics.html",
    "href": "academics.html",
    "title": "Academics",
    "section": "",
    "text": "This section contains a selection of my academic work, organized by class in the sidebar.\nBelow is a blog-style chronological list of the material.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 6: Clustering and Blockmodels in the Florentine Wedding dataset\n\n\n\n\n\n\n\nr\n\n\nsocial networks\n\n\n\n\nYet more ways to look at the Florence data.\n\n\n\n\n\n\nMar 26, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 5: Constraint and Brokerage in the Florentine Wedding dataset\n\n\n\n\n\n\n\nr\n\n\nsocial networks\n\n\n\n\nExamining additional centrality measures in the Florentine network\n\n\n\n\n\n\nFeb 16, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 4: Centrality Analysis of the Florentine Wedding dataset\n\n\n\n\n\n\n\nr\n\n\nsocial networks\n\n\n\n\nExamining centrality measures in the Florentine network\n\n\n\n\n\n\nFeb 10, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 3: The Florentine Wedding dataset\n\n\n\n\n\n\n\nr\n\n\nsocial networks\n\n\n\n\nA first-look network analysis\n\n\n\n\n\n\nFeb 9, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUseful R tidbits: Political Networks Analysis\n\n\n\n\n\n\n\nr\n\n\nsocial networks\n\n\n\n\nTasty R snacks that do useful things in Political Networks Analysis\n\n\n\n\n\n\nFeb 8, 2022\n\n\n20 min\n\n\n\n\n\n\n  \n\n\n\n\nBigger Concepts\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\nBigger concepts for MS797 coursework\n\n\n\n\n\n\nFeb 3, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n695M - Machine Learning\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2022\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nUseful R tidbits: Machine Learning\n\n\n\n\n\n\n\nr\n\n\nmachine learning\n\n\n\n\nTasty R snacks that do useful things in Machine Learning\n\n\n\n\n\n\nFeb 3, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlossary\n\n\n\n\n\n\n\nmachine learning\n\n\nglossary\n\n\nmath\n\n\n\n\nGlossary for 695M - Machine Learning\n\n\n\n\n\n\nFeb 3, 2022\n\n\n19 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n697E - Political Networks Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2022\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nGlossary\n\n\n\n\n\n\n\nnetworks\n\n\nglossary\n\n\n\n\nGlossary for 697E coursework\n\n\n\n\n\n\nJan 29, 2022\n\n\n10 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Blog\nUse the filter box below to filter posts by keyword or category.\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nChatGPT fail: Sieve of Eratosthenes\n\n\n\n\n\n\n\nAI\n\n\nR\n\n\nChatGPT\n\n\ndata science\n\n\n\n\nChatGPT writes buggy R in a quick test\n\n\n\n\n\n\nDec 6, 2022\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nAI Sonnets on Data Science\n\n\n\n\n\n\n\nAI\n\n\nChatGPT\n\n\ndata science\n\n\n\n\nThe OpenAI system writes emotive sonnets about data science.\n\n\n\n\n\n\nDec 2, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nMastodon\n\n\n\n\n\n\n\nR\n\n\nmastodon\n\n\nsocial media\n\n\ntwitter\n\n\nquarto\n\n\n\n\nDecentralized, open-source social media? Yes please.\n\n\n\n\n\n\nNov 18, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nHacked! (or not)\n\n\n\n\n\n\n\nmeta\n\n\nquarto\n\n\n\n\nHow a static site’s database was hacked\n\n\n\n\n\n\nOct 31, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nPorting to Quarto, part 2\n\n\n\n\n\n\n\nquarto\n\n\ndistill\n\n\n\n\nA few more notes on the Quarto transition\n\n\n\n\n\n\nOct 20, 2022\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nPorting slinberg.net from Distill to Quarto\n\n\n\n\n\n\n\nquarto\n\n\ndistill\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nQuickie for importing tasks to OmniFocus 3\n\n\n\n\n\n\n\nomnifocus\n\n\n\n\nCopy and paste formatted text to mass-create tasks.\n\n\n\n\n\n\nApr 2, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nTufte Takes No Prisoners\n\n\n\n\n\n\n\ntufte\n\n\ndesign\n\n\n\n\nEdward Tufte gets raw on chartjunk.\n\n\n\n\n\n\nMar 5, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nPlayfair and the “Flatten the Curve” Graphic\n\n\n\n\n\n\n\ntufte\n\n\ndesign\n\n\n\n\nThe importance of simplicity in statistical graphics\n\n\n\n\n\n\nFeb 19, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nSaturday mornings with Tufte\n\n\n\n\n\n\n\ntufte\n\n\ndesign\n\n\n\n\nThe other half of data science skillbuilding\n\n\n\n\n\n\nFeb 5, 2022\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPythagorean Theorem\n\n\n\n\n\n\n\nmath\n\n\n\n\nHow did I not know this\n\n\n\n\n\n\nFeb 2, 2022\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nDistill and Github Pages Setup\n\n\n\n\n\n\n\ndistill\n\n\ngithub pages\n\n\ngit\n\n\nrstudio\n\n\n\n\nA walkthrough of setting up a new Distill site on GitHub Pages\n\n\n\n\n\n\nJan 29, 2022\n\n\n10 min\n\n\n\n\n\n\nNo matching items"
  }
]